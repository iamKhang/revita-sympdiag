{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"45b5cbf9b3ea496cb9205d85dc59e755":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bcdfb1fb3c41446299f352c795f2a0b8","IPY_MODEL_77da4b97330f42a9be6343b1dd5b4639","IPY_MODEL_8c226275f41c43c0a46d3bd71b438820"],"layout":"IPY_MODEL_7d5164d3f98947aa902efc1164ce3485"}},"bcdfb1fb3c41446299f352c795f2a0b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6ad059c2e854c1c9de02a9c0af271a6","placeholder":"​","style":"IPY_MODEL_c18281271c344266bd23aa4508605857","value":"config.json: 100%"}},"77da4b97330f42a9be6343b1dd5b4639":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4577de5f103947779b40adb588696436","max":313,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee45d5b3ea454ae3a9897ac7fc37a9d8","value":313}},"8c226275f41c43c0a46d3bd71b438820":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7ecb1dfd01d4421ae2e7d293e855c11","placeholder":"​","style":"IPY_MODEL_227f8af0acff4fdabf0de1738a05b161","value":" 313/313 [00:00&lt;00:00, 35.0kB/s]"}},"7d5164d3f98947aa902efc1164ce3485":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ad059c2e854c1c9de02a9c0af271a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c18281271c344266bd23aa4508605857":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4577de5f103947779b40adb588696436":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee45d5b3ea454ae3a9897ac7fc37a9d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7ecb1dfd01d4421ae2e7d293e855c11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227f8af0acff4fdabf0de1738a05b161":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"045257ffc7ba469588d3a7ca92bc5811":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a78dbfbbcc124cee94ff2dfa7ac6f178","IPY_MODEL_a323559a9d7f405c946678034d654596","IPY_MODEL_93ae2045893346118538101bc63ff078"],"layout":"IPY_MODEL_e0c867b732fd4c9cacfd24f46f3e176a"}},"a78dbfbbcc124cee94ff2dfa7ac6f178":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3674e24c30de4f849242e55049c037cd","placeholder":"​","style":"IPY_MODEL_d752ccf3e2864eefa175b5a1732c633f","value":"vocab.txt: "}},"a323559a9d7f405c946678034d654596":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f57038bd4134166b3f9928934af8be9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_396d5363aa564554a66803cf9b18b249","value":1}},"93ae2045893346118538101bc63ff078":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5389a1785d794cb3ba53d52b52f3214d","placeholder":"​","style":"IPY_MODEL_ed8d26bc348c4eb1a57ad23321b1a735","value":" 213k/? [00:00&lt;00:00, 7.42MB/s]"}},"e0c867b732fd4c9cacfd24f46f3e176a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3674e24c30de4f849242e55049c037cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d752ccf3e2864eefa175b5a1732c633f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f57038bd4134166b3f9928934af8be9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"396d5363aa564554a66803cf9b18b249":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5389a1785d794cb3ba53d52b52f3214d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed8d26bc348c4eb1a57ad23321b1a735":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d0c6e6764b647548fdb62dec0b82f83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e3a632fe5864a2d98fffd7eb939a94d","IPY_MODEL_ef620a8400534a81ade2734879ebdcb2","IPY_MODEL_567c8ab5004f481190a601dd557b1b52"],"layout":"IPY_MODEL_64400f90e48843ac92227255c2fe642a"}},"2e3a632fe5864a2d98fffd7eb939a94d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5cb4762af3d47e0b4f53bf5cb3ee4f1","placeholder":"​","style":"IPY_MODEL_ae08b76691be45979e5f1dbf1de3a1f2","value":"pytorch_model.bin: 100%"}},"ef620a8400534a81ade2734879ebdcb2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a860f8d20b40cf93d4414df9ab14f0","max":435780550,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d11c26e686e2432b82ce33ab9e11e97b","value":435780550}},"567c8ab5004f481190a601dd557b1b52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e11238d5b8234dc9bbe20382251087cd","placeholder":"​","style":"IPY_MODEL_ef9cb53cfb524bab914f3980d34ca726","value":" 436M/436M [00:06&lt;00:00, 70.7MB/s]"}},"64400f90e48843ac92227255c2fe642a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5cb4762af3d47e0b4f53bf5cb3ee4f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae08b76691be45979e5f1dbf1de3a1f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a860f8d20b40cf93d4414df9ab14f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d11c26e686e2432b82ce33ab9e11e97b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e11238d5b8234dc9bbe20382251087cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef9cb53cfb524bab914f3980d34ca726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13336031,"sourceType":"datasetVersion","datasetId":8319146},{"sourceId":265038488,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle config (KHÔNG dò đường, dùng đúng tên dataset)\nfrom pathlib import Path\nimport pandas as pd\n\n# ---- Đặt đúng tên dataset như bạn đã thêm vào notebook ----\nBASE_INPUT = Path(\"/kaggle/input/mimic-iv-proc-revita-2025-09-222\")   # <-- tên đúng như bạn nói\nPROC = BASE_INPUT                                     # file unified ở GỐC dataset\n\n# ---- File unified ----\nUNIFIED_PQT = PROC / \"train_unified.parquet\"\nUNIFIED_CSV = PROC / \"train_unified.csv\"\n\n# ==== Tham số bạn đang dùng ====\nMIN_LABEL_FREQ = 100\nMAX_LABELS = 3000\nLIMIT_TRAIN = 200000\n\n# Vector hoá\nN_FEATURES = 2**18\nUSE_CHAR_NGRAMS = False\nNGRAM_WORD = (1, 2)\nNGRAM_CHAR = (3, 5)\nMAX_TOKENS_PER_DOC = 4000\n\n# Train loop\nBATCH_SIZE = 512\nEPOCHS = 1\nLOG_EVERY = 20\nSEED = 42\n\n# Checkpoint/model (ghi được ở Kaggle)\nCKPT_PATH = Path(\"/kaggle/working\") / \"ovr_lazy_sgd.joblib\"\n\n# ---- Load unified ----\nif UNIFIED_PQT.exists():\n    df = pd.read_parquet(UNIFIED_PQT)\nelif UNIFIED_CSV.exists():\n    df = pd.read_csv(UNIFIED_CSV)\nelse:\n    raise FileNotFoundError(\"Không thấy train_unified.{parquet|csv} ở gốc dataset 'mimic iv dataset'.\")\n\nprint({\n    \"dataset_dir\": str(BASE_INPUT),\n    \"CKPT_PATH\": str(CKPT_PATH),\n    \"Loaded\": df.shape,\n    \"MIN_LABEL_FREQ\": MIN_LABEL_FREQ,\n    \"MAX_LABELS\": MAX_LABELS,\n    \"N_FEATURES\": N_FEATURES,\n    \"BATCH_SIZE\": BATCH_SIZE,\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:55:33.894643Z","iopub.execute_input":"2025-10-11T13:55:33.894862Z","iopub.status.idle":"2025-10-11T13:55:55.604628Z","shell.execute_reply.started":"2025-10-11T13:55:33.894844Z","shell.execute_reply":"2025-10-11T13:55:55.603942Z"}},"outputs":[{"name":"stdout","text":"{'dataset_dir': '/kaggle/input/mimic-iv-proc-revita-2025-09-222', 'CKPT_PATH': '/kaggle/working/ovr_lazy_sgd.joblib', 'Loaded': (331062, 6), 'MIN_LABEL_FREQ': 100, 'MAX_LABELS': 3000, 'N_FEATURES': 262144, 'BATCH_SIZE': 512}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ƯU TIÊN tái dùng file tần suất đã xuất sẵn; nếu không có thì tính từ df\nimport pandas as pd\n\nKEEP_LABELS = None\nfreq_csv = (PROC/\"top_icd_coverage.csv\")\nif not freq_csv.exists():\n    freq_csv = (PROC/\"icd_hadm_freq.csv\")\n\nif freq_csv.exists():\n    freq = pd.read_csv(freq_csv)\n    # Chuẩn hoá tên cột\n    if \"icd_full\" not in freq.columns:\n        if freq.columns.tolist() == [\"index\",\"hadm_freq\"]:\n            freq = freq.rename(columns={\"index\":\"icd_full\"})\n        else:\n            freq.columns = [\"icd_full\",\"hadm_freq\"]\n    keep_df = (freq[freq[\"hadm_freq\"] >= MIN_LABEL_FREQ]\n               .sort_values(\"hadm_freq\", ascending=False)\n               .head(MAX_LABELS))\n    KEEP_LABELS = set(keep_df[\"icd_full\"].tolist())\n    print(f\"Reuse nhãn từ {freq_csv.name}: {len(KEEP_LABELS)} labels\")\nelse:\n    # fallback: lấy từ chính df unified\n    from collections import Counter\n    codes = df[\"icd_codes\"].str.split(\";\")\n    cnt = Counter(c for row in codes for c in row)\n    keep = [c for c,n in cnt.items() if n >= MIN_LABEL_FREQ]\n    keep = sorted(keep, key=lambda c: cnt[c], reverse=True)[:MAX_LABELS]\n    KEEP_LABELS = set(keep)\n    print(f\"Tính nhãn từ unified: {len(KEEP_LABELS)} labels\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:56:12.766761Z","iopub.execute_input":"2025-10-11T13:56:12.767030Z","iopub.status.idle":"2025-10-11T13:56:12.857152Z","shell.execute_reply.started":"2025-10-11T13:56:12.767008Z","shell.execute_reply":"2025-10-11T13:56:12.856517Z"}},"outputs":[{"name":"stdout","text":"Reuse nhãn từ top_icd_coverage.csv: 3000 labels\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import GroupShuffleSplit\n\ncodes = df[\"icd_codes\"].str.split(\";\")\nmask = codes.map(lambda L: any(c in KEEP_LABELS for c in L))\ndf = df.loc[mask].copy()\ndf[\"labels\"] = codes.map(lambda L: [c for c in L if c in KEEP_LABELS])\n\n# thử nhanh (nếu muốn)\nif LIMIT_TRAIN is not None and len(df) > LIMIT_TRAIN:\n    df = df.sample(LIMIT_TRAIN, random_state=SEED).reset_index(drop=True)\n\n# split theo subject_id (không rò rỉ bệnh nhân)\ngss1 = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=SEED)\nidx_tr, idx_te = next(gss1.split(df, groups=df[\"subject_id\"]))\ntrain_val = df.iloc[idx_tr].reset_index(drop=True)\ntest = df.iloc[idx_te].reset_index(drop=True)\n\ngss2 = GroupShuffleSplit(n_splits=1, test_size=0.15/(1-0.15), random_state=SEED)\nidx_tr2, idx_va = next(gss2.split(train_val, groups=train_val[\"subject_id\"]))\ntrain = train_val.iloc[idx_tr2].reset_index(drop=True)\nval   = train_val.iloc[idx_va].reset_index(drop=True)\n\nprint(\"Split sizes:\", len(train), len(val), len(test))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:56:16.623226Z","iopub.execute_input":"2025-10-11T13:56:16.624095Z","iopub.status.idle":"2025-10-11T13:56:20.290586Z","shell.execute_reply.started":"2025-10-11T13:56:16.624050Z","shell.execute_reply":"2025-10-11T13:56:20.289732Z"}},"outputs":[{"name":"stdout","text":"Split sizes: 69 16 15\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom scipy.sparse import hstack\n\ndef truncate_tokens(text, mx=MAX_TOKENS_PER_DOC):\n    return \" \".join(str(text).split()[:mx])\n\nhv_word = HashingVectorizer(\n    n_features=N_FEATURES,\n    ngram_range=NGRAM_WORD,\n    analyzer=\"word\",\n    alternate_sign=False,\n    norm=\"l2\",\n    dtype=np.float32,\n    preprocessor=lambda t: truncate_tokens(t, MAX_TOKENS_PER_DOC),\n)\n\nhv_char = None\nif USE_CHAR_NGRAMS:\n    hv_char = HashingVectorizer(\n        n_features=N_FEATURES//2,\n        ngram_range=NGRAM_CHAR,\n        analyzer=\"char\",\n        alternate_sign=False,\n        norm=\"l2\",\n        dtype=np.float32,\n        preprocessor=lambda t: truncate_tokens(t, MAX_TOKENS_PER_DOC),\n    )\n\ndef X_from_text(series):\n    Xw = hv_word.transform(series)\n    if hv_char is None:\n        return Xw.tocsr()\n    Xc = hv_char.transform(series)\n    return hstack([Xw, Xc], format=\"csr\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:56:40.878564Z","iopub.execute_input":"2025-10-11T13:56:40.879212Z","iopub.status.idle":"2025-10-11T13:56:40.885789Z","shell.execute_reply.started":"2025-10-11T13:56:40.879187Z","shell.execute_reply":"2025-10-11T13:56:40.884893Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\n\nmlb = MultiLabelBinarizer()\n_ = mlb.fit(train[\"labels\"])   # chỉ để cố định trật tự nhãn\n\ndef eval_f1k_pk_streaming(model, mlb, df_eval, K=5, batch_size=512):\n    tp=fp=fn=correct_k_total=total_preds=0\n    n=len(df_eval)\n    for i in range(0, n, batch_size):\n        j=min(i+batch_size, n)\n        Xb = X_from_text(df_eval[\"text_clean\"].iloc[i:j])\n        Yb = mlb.transform(df_eval[\"labels\"].iloc[i:j]).astype(np.int8)\n        Pb = model.predict_proba(Xb)\n        topk_idx = np.argsort(-Pb, axis=1)[:, :K]\n        pred_b = np.zeros_like(Pb, dtype=np.int8)\n        rows = np.arange(Pb.shape[0])[:, None]\n        pred_b[rows, topk_idx] = 1\n        tp += int((pred_b & Yb).sum())\n        fp += int((pred_b & (1 - Yb)).sum())\n        fn += int(((1 - pred_b) & Yb).sum())\n        correct_k_total += int((pred_b & Yb).sum())\n        total_preds += pred_b.shape[0] * K\n        del Xb, Yb, Pb, pred_b\n    prec = tp / (tp + fp + 1e-12)\n    rec  = tp / (tp + fn + 1e-12)\n    f1k  = 2 * prec * rec / (prec + rec + 1e-12)\n    pk   = correct_k_total / (total_preds + 1e-12)\n    return {\"f1@{}\".format(K): f1k, \"p@{}\".format(K): pk}\n\n# đánh giá chỉ trên các nhãn đã được huấn luyện (phòng case LIMIT_TRAIN nhỏ)\ndef eval_on_trained_labels(model, mlb, df_eval, K=5, batch_size=512):\n    trained_idx = sorted(getattr(model, \"trained_label_indices\", range(len(mlb.classes_))))\n    trained_labels = [mlb.classes_[i] for i in trained_idx]\n    eval_mlb = MultiLabelBinarizer(classes=trained_labels)\n    eval_mlb.fit([trained_labels])\n    tp=fp=fn=correct_k_total=total_preds=0\n    n=len(df_eval)\n    for i in range(0, n, batch_size):\n        j=min(i+batch_size, n)\n        Xb = X_from_text(df_eval[\"text_clean\"].iloc[i:j])\n        Yb = eval_mlb.transform(df_eval[\"labels\"].iloc[i:j]).astype(np.int8)\n        Pb_full = model.predict_proba(Xb)\n        Pb = Pb_full[:, trained_idx]\n        topk_idx = np.argsort(-Pb, axis=1)[:, :K]\n        pred_b = np.zeros_like(Pb, dtype=np.int8)\n        rows = np.arange(Pb.shape[0])[:, None]\n        pred_b[rows, topk_idx] = 1\n        tp += int((pred_b & Yb).sum())\n        fp += int((pred_b & (1 - Yb)).sum())\n        fn += int(((1 - pred_b) & Yb).sum())\n        correct_k_total += int((pred_b & Yb).sum())\n        total_preds += pred_b.shape[0] * K\n        del Xb, Yb, Pb_full, Pb, pred_b\n    prec = tp / (tp + fp + 1e-12)\n    rec  = tp / (tp + fn + 1e-12)\n    f1k  = 2 * prec * rec / (prec + rec + 1e-12)\n    pk   = correct_k_total / (total_preds + 1e-12)\n    return {\"f1@{}\".format(K): f1k, \"p@{}\".format(K): pk}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:57:03.074712Z","iopub.execute_input":"2025-10-11T13:57:03.074995Z","iopub.status.idle":"2025-10-11T13:57:03.087138Z","shell.execute_reply.started":"2025-10-11T13:57:03.074974Z","shell.execute_reply":"2025-10-11T13:57:03.086271Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import numpy as np, time, joblib, gc\nfrom sklearn.linear_model import SGDClassifier\n\nclass LazyOVR:\n    \"\"\"\n    - Mỗi nhãn là 1 SGDClassifier nhị phân, chỉ tạo khi nhãn xuất hiện trong batch.\n    - predict_proba: chỉ tính cho head đã train; nhãn chưa train -> 0.0.\n    \"\"\"\n    def __init__(self, n_labels, alpha=1e-5, random_state=42):\n        self.n_labels = n_labels\n        self.alpha = alpha\n        self.random_state = random_state\n        self.heads = {}           # k -> SGDClassifier\n        self._init_done = set()   # k đã init\n        self.trained_label_indices = set()\n\n    def _get_head(self, k):\n        if k not in self.heads:\n            self.heads[k] = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=self.alpha,\n                                          learning_rate=\"optimal\", max_iter=1, tol=None,\n                                          random_state=self.random_state)\n        return self.heads[k]\n\n    def partial_fit(self, X, Y):\n        # Y shape: (n_samples, n_labels)\n        active = np.where(Y.sum(axis=0) > 0)[0]\n        if active.size == 0:\n            return self\n        classes = np.array([0,1], dtype=np.int8)\n        for k in active:\n            yk = Y[:, k].astype(np.int8)\n            head = self._get_head(k)\n            if k not in self._init_done:\n                head.partial_fit(X, yk, classes=classes)\n                self._init_done.add(k)\n            else:\n                head.partial_fit(X, yk)\n            self.trained_label_indices.add(k)\n        return self\n\n    def predict_proba(self, X):\n        n = X.shape[0]\n        P = np.zeros((n, self.n_labels), dtype=np.float32)\n        for k, head in self.heads.items():\n            P[:, k] = head.predict_proba(X)[:, 1]\n        return P\n\n# ===== Train loop =====\nrng = np.random.default_rng(SEED)\ndef batches_idx(n, bs):\n    idx = np.arange(n); rng.shuffle(idx)\n    for i in range(0, n, bs):\n        yield idx[i:i+bs]\n\nn_labels = len(mlb.classes_)\nmodel = LazyOVR(n_labels=n_labels, alpha=1e-5, random_state=SEED)\n\nstart = time.time(); seen = 0\nfor ep in range(EPOCHS):\n    for bi, idx in enumerate(batches_idx(len(train), BATCH_SIZE), 1):\n        X_b = X_from_text(train.loc[idx, \"text_clean\"])\n        Y_b = mlb.transform(train.loc[idx, \"labels\"]).astype(np.int8)\n\n        model.partial_fit(X_b, Y_b)\n        seen += len(idx)\n\n        del X_b, Y_b; gc.collect()\n\n        if bi % LOG_EVERY == 0:\n            m = eval_on_trained_labels(model, mlb, val, K=5, batch_size=512)\n            print(f\"[ep {ep+1} | batch {bi}] seen={seen}  F1@5={m['f1@5']:.4f}  P@5={m['p@5']:.4f}  heads={len(model.heads)}  t={time.time()-start:.1f}s\")\n\nprint(\"Done in {:.1f}s\".format(time.time()-start))\n\njoblib.dump({\"heads\": model.heads, \"mlb\": mlb,\n             \"cfg\": {\"N_FEATURES\": N_FEATURES, \"NGRAM_WORD\": NGRAM_WORD,\n                     \"USE_CHAR_NGRAMS\": USE_CHAR_NGRAMS,\n                     \"MAX_TOKENS_PER_DOC\": MAX_TOKENS_PER_DOC}},\n            CKPT_PATH)\nprint(\"Saved:\", CKPT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:57:09.624842Z","iopub.execute_input":"2025-10-11T13:57:09.625359Z","iopub.status.idle":"2025-10-11T13:57:16.115423Z","shell.execute_reply.started":"2025-10-11T13:57:09.625328Z","shell.execute_reply":"2025-10-11T13:57:16.114611Z"}},"outputs":[{"name":"stdout","text":"Done in 5.5s\nSaved: /kaggle/working/ovr_lazy_sgd.joblib\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import numpy as np, time, joblib, gc\nfrom sklearn.linear_model import SGDClassifier\n\nclass LazyOVR:\n    \"\"\"\n    - Mỗi nhãn là 1 SGDClassifier nhị phân, chỉ tạo khi nhãn xuất hiện trong batch.\n    - predict_proba: chỉ tính cho head đã train; nhãn chưa train -> 0.0.\n    \"\"\"\n    def __init__(self, n_labels, alpha=1e-5, random_state=42):\n        self.n_labels = n_labels\n        self.alpha = alpha\n        self.random_state = random_state\n        self.heads = {}           # k -> SGDClassifier\n        self._init_done = set()   # k đã init\n        self.trained_label_indices = set()\n\n    def _get_head(self, k):\n        if k not in self.heads:\n            self.heads[k] = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=self.alpha,\n                                          learning_rate=\"optimal\", max_iter=1, tol=None,\n                                          random_state=self.random_state)\n        return self.heads[k]\n\n    def partial_fit(self, X, Y):\n        # Y shape: (n_samples, n_labels)\n        active = np.where(Y.sum(axis=0) > 0)[0]\n        if active.size == 0:\n            return self\n        classes = np.array([0,1], dtype=np.int8)\n        for k in active:\n            yk = Y[:, k].astype(np.int8)\n            head = self._get_head(k)\n            if k not in self._init_done:\n                head.partial_fit(X, yk, classes=classes)\n                self._init_done.add(k)\n            else:\n                head.partial_fit(X, yk)\n            self.trained_label_indices.add(k)\n        return self\n\n    def predict_proba(self, X):\n        n = X.shape[0]\n        P = np.zeros((n, self.n_labels), dtype=np.float32)\n        for k, head in self.heads.items():\n            P[:, k] = head.predict_proba(X)[:, 1]\n        return P\n\n# ===== Train loop =====\nrng = np.random.default_rng(SEED)\ndef batches_idx(n, bs):\n    idx = np.arange(n); rng.shuffle(idx)\n    for i in range(0, n, bs):\n        yield idx[i:i+bs]\n\nn_labels = len(mlb.classes_)\nmodel = LazyOVR(n_labels=n_labels, alpha=1e-5, random_state=SEED)\n\nstart = time.time(); seen = 0\nfor ep in range(EPOCHS):\n    for bi, idx in enumerate(batches_idx(len(train), BATCH_SIZE), 1):\n        X_b = X_from_text(train.loc[idx, \"text_clean\"])\n        Y_b = mlb.transform(train.loc[idx, \"labels\"]).astype(np.int8)\n\n        model.partial_fit(X_b, Y_b)\n        seen += len(idx)\n\n        del X_b, Y_b; gc.collect()\n\n        if bi % LOG_EVERY == 0:\n            m = eval_on_trained_labels(model, mlb, val, K=5, batch_size=512)\n            print(f\"[ep {ep+1} | batch {bi}] seen={seen}  F1@5={m['f1@5']:.4f}  P@5={m['p@5']:.4f}  heads={len(model.heads)}  t={time.time()-start:.1f}s\")\n\nprint(\"Done in {:.1f}s\".format(time.time()-start))\n\njoblib.dump({\"heads\": model.heads, \"mlb\": mlb,\n             \"cfg\": {\"N_FEATURES\": N_FEATURES, \"NGRAM_WORD\": NGRAM_WORD,\n                     \"USE_CHAR_NGRAMS\": USE_CHAR_NGRAMS,\n                     \"MAX_TOKENS_PER_DOC\": MAX_TOKENS_PER_DOC}},\n            CKPT_PATH)\nprint(\"Saved:\", CKPT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:57:23.830753Z","iopub.execute_input":"2025-10-11T13:57:23.831325Z","iopub.status.idle":"2025-10-11T13:57:32.011404Z","shell.execute_reply.started":"2025-10-11T13:57:23.831282Z","shell.execute_reply":"2025-10-11T13:57:32.010594Z"}},"outputs":[{"name":"stdout","text":"Done in 6.1s\nSaved: /kaggle/working/ovr_lazy_sgd.joblib\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np, joblib\n\ndef predict_topk(texts, K=5):\n    Xq = X_from_text(pd.Series(texts))\n    P  = model.predict_proba(Xq)\n    codes = mlb.classes_\n    out = []\n    for i in range(len(texts)):\n        idx = np.argsort(-P[i])[:K]\n        out.append([(codes[j], float(P[i,j])) for j in idx])\n    return out\n\ndemo = [\n    \"Service: MEDICINE\\nHistory: chest pain, HTN, DM, hyperlipidemia...\",\n    \"Service: SURGERY\\nPost-op day #2, fever, wound infection, antibiotics...\"\n]\nfor i, preds in enumerate(predict_topk(demo, K=5), 1):\n    print(f\"\\nCase {i}:\")\n    for code, prob in preds:\n        print(f\"  {code}: {prob:.3f}\")\n\n# Lưu lại lần nữa vào working để đảm bảo artifact xuất hiện\njoblib.dump({\"heads\": model.heads, \"mlb\": mlb}, CKPT_PATH)\nprint(\"Model @\", CKPT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:57:39.707908Z","iopub.execute_input":"2025-10-11T13:57:39.708511Z","iopub.status.idle":"2025-10-11T13:57:41.687409Z","shell.execute_reply.started":"2025-10-11T13:57:39.708490Z","shell.execute_reply":"2025-10-11T13:57:41.686360Z"}},"outputs":[{"name":"stdout","text":"\nCase 1:\n  9-4019: 0.845\n  9-42731: 0.745\n  9-42789: 0.693\n  9-311: 0.671\n  9-27651: 0.625\n\nCase 2:\n  9-04111: 0.809\n  9-99592: 0.790\n  9-78552: 0.790\n  9-0389: 0.790\n  9-V1254: 0.763\nModel @ /kaggle/working/ovr_lazy_sgd.joblib\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Xuất top-5 dự đoán cho một batch test nhỏ\nout = []\nK = 5\nfor i in range(min(50, len(test))):\n    s = test.iloc[i]\n    top5 = predict_topk([s[\"text_clean\"]], K=K)[0]\n    out.append({\n        \"subject_id\": s[\"subject_id\"],\n        \"hadm_id\": s[\"hadm_id\"],\n        \"gold\": \";\".join(s[\"labels\"]),\n        \"pred_top5\": \";\".join([f\"{c}:{p:.3f}\" for c,p in top5])\n    })\npd.DataFrame(out).to_csv(\"/kaggle/working/preds_sample.csv\", index=False)\nprint(\"Saved /kaggle/working/preds_sample.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T13:58:50.193379Z","iopub.execute_input":"2025-10-11T13:58:50.194077Z","iopub.status.idle":"2025-10-11T13:58:51.010181Z","shell.execute_reply.started":"2025-10-11T13:58:50.194056Z","shell.execute_reply":"2025-10-11T13:58:51.009538Z"}},"outputs":[{"name":"stdout","text":"Saved /kaggle/working/preds_sample.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ĐÁNH GIÁ NHANH: Top-K có trúng bao nhiêu mã thật trên 100 ca?\nimport numpy as np, pandas as pd\nfrom pathlib import Path\n\nK = 5                     # Top-K cần kiểm tra\nN_EVAL = 100              # số ca để đánh giá nhanh\nBATCH = 256               # batch suy luận để tiết kiệm RAM\nOUT_CSV = Path(\"/kaggle/working/eval_topk_sample.csv\")\n\n# lấy 100 ca ngẫu nhiên (hoặc ít hơn nếu test nhỏ)\neval_df = test.sample(min(N_EVAL, len(test)), random_state=SEED).reset_index(drop=True)\n\ncodes_all = mlb.classes_\nhits_per_case = []\nrows_out = []\n\nfor i in range(0, len(eval_df), BATCH):\n    j = min(i + BATCH, len(eval_df))\n    Xb = X_from_text(eval_df.loc[i:j-1, \"text_clean\"])\n    Pb = model.predict_proba(Xb)                 # (batch, n_labels)\n    topk_idx = np.argsort(-Pb, axis=1)[:, :K]    # chỉ số nhãn top-K\n    for r in range(j - i):\n        gold = set(eval_df.at[i+r, \"labels\"])    # nhãn thật (list -> set)\n        pred_idx = topk_idx[r].tolist()\n        pred_codes = [codes_all[t] for t in pred_idx]\n        # đếm số mã trùng giữa Top-K và gold\n        hit = len(gold.intersection(pred_codes))\n        hits_per_case.append(hit)\n        rows_out.append({\n            \"subject_id\": eval_df.at[i+r, \"subject_id\"],\n            \"hadm_id\": eval_df.at[i+r, \"hadm_id\"],\n            \"hits@{}\".format(K): hit,\n            \"gold_codes\": \";\".join(sorted(gold)),\n            \"pred_top{}\".format(K): \";\".join(pred_codes),\n            # (tuỳ chọn) kèm xác suất cho dễ soi\n            \"pred_top{}_probs\".format(K): \";\".join([f\"{float(Pb[r, t]):.3f}\" for t in pred_idx])\n        })\n\n# TỔNG KẾT\nhits_arr = np.array(hits_per_case)\nhit_rate = (hits_arr > 0).mean()          # % ca có ít nhất 1 mã đúng trong Top-K\navg_hits = hits_arr.mean()                 # trung bình số mã đúng trong Top-K\nprint({\n    \"K\": K,\n    \"n_cases\": len(eval_df),\n    \"hit_rate@K (>=1 đúng)\": round(float(hit_rate), 4),\n    \"avg_hits@K (trung bình số mã đúng)\": round(float(avg_hits), 4),\n    \"cases_0hit\": int((hits_arr==0).sum()),\n    \"cases_1+\": int((hits_arr>0).sum()),\n})\n\n# LƯU chi tiết để xem ngoài\ndf_out = pd.DataFrame(rows_out)\ndf_out.to_csv(OUT_CSV, index=False)\nprint(\"Saved:\", OUT_CSV)\n\n# IN 5 ví dụ: 3 ca có trúng & 2 ca trượt để bạn quan sát\nok_idx = np.where(hits_arr>0)[0][:3].tolist()\nko_idx = np.where(hits_arr==0)[0][:2].tolist()\nshow_idx = ok_idx + ko_idx\nprint(\"\\n=== Ví dụ nhanh ===\")\nfor idx in show_idx:\n    row = df_out.iloc[idx]\n    print(f\"- hadm_id={row['hadm_id']}  hits@{K}={row[f'hits@{K}']}\")\n    print(f\"  gold: {row['gold_codes']}\")\n    print(f\"  pred: {row[f'pred_top{K}']}\")\n    print(f\"  prob: {row[f'pred_top{K}_probs']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:03:04.989790Z","iopub.execute_input":"2025-10-11T14:03:04.990535Z","iopub.status.idle":"2025-10-11T14:03:05.189735Z","shell.execute_reply.started":"2025-10-11T14:03:04.990514Z","shell.execute_reply":"2025-10-11T14:03:05.188946Z"}},"outputs":[{"name":"stdout","text":"{'K': 5, 'n_cases': 15, 'hit_rate@K (>=1 đúng)': 0.2667, 'avg_hits@K (trung bình số mã đúng)': 0.6, 'cases_0hit': 11, 'cases_1+': 4}\nSaved: /kaggle/working/eval_topk_sample.csv\n\n=== Ví dụ nhanh ===\n- hadm_id=24345926  hits@5=2\n  gold: 10-C787;10-E039;10-E8339;10-E876;10-F329;10-F419;10-G893;10-K219;10-K3184;10-M1990;10-R110;10-Z170;10-Z7982;10-Z853;10-Z8673;10-Z934\n  pred: 10-Z7902;10-I480;10-E8342;10-K219;10-E8339\n  prob: 1.000;0.999;0.999;0.996;0.995\n- hadm_id=23709687  hits@5=4\n  gold: 10-B3781;10-D62;10-E039;10-E440;10-E785;10-E8342;10-E8351;10-F17210;10-G43909;10-G4700;10-I10;10-I2510;10-I714;10-I739;10-J449;10-K219;10-K3189;10-K6389;10-K922;10-L89150;10-M62838;10-N179;10-N390;10-R911;10-Z6821;10-Z7902\n  pred: 10-Z7902;10-K219;10-N179;10-I480;10-E8342\n  prob: 1.000;0.999;0.997;0.995;0.994\n- hadm_id=25140310  hits@5=2\n  gold: 10-G4733;10-I10;10-I350;10-I480;10-J398;10-J40;10-K219;10-M160;10-M170;10-M479;10-M8580;10-R911;10-Z85828\n  pred: 10-E8342;10-I480;10-Z7902;10-K219;10-N179\n  prob: 0.999;0.999;0.999;0.998;0.996\n- hadm_id=24664773  hits@5=0\n  gold: 9-07054;9-30500;9-30520;9-311;9-7210;9-V600\n  pred: 10-Z7902;10-E8342;10-I480;10-N179;10-K219\n  prob: 0.999;0.991;0.981;0.978;0.971\n- hadm_id=23620621  hits@5=0\n  gold: 9-2449;9-2536;9-4019;9-4240;9-4280;9-53081;9-8082;9-E8859;9-V1005;9-V1551;9-V1582;9-V1588\n  pred: 10-K219;10-I482;10-B9620;10-N3000;10-K220\n  prob: 0.981;0.973;0.962;0.962;0.962\n","output_type":"stream"}],"execution_count":13}]}