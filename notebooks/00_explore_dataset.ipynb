{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4056183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ƒêang x·ª≠ l√Ω ICD...\n",
      "‚úÖ L∆∞u 50 m√£ ICD ‚Üí ../data/mimiciv_subset/icd.csv (28,562 m√£, t·ªïng 6,364,488 l∆∞·ª£t)\n",
      "üõ†Ô∏è  ƒêang x·ª≠ l√Ω th·ªß thu·∫≠t...\n",
      "‚úÖ L∆∞u 50 th·ªß thu·∫≠t ‚Üí ../data/mimiciv_subset/procedures.csv (14,911 m√£, t·ªïng 859,655 l∆∞·ª£t)\n",
      "üß™ ƒêang x·ª≠ l√Ω x√©t nghi·ªám...\n",
      "‚úÖ L∆∞u 50 x√©t nghi·ªám ‚Üí ../data/mimiciv_subset/labs.csv (896 m√£, t·ªïng 84,605,867 l∆∞·ª£t)\n",
      "\n",
      "=== T·ªîNG K·∫æT ===\n",
      "- T·ªïng m√£ ICD: 28,562\n",
      "- T·ªïng m√£ th·ªß thu·∫≠t: 14,911\n",
      "- T·ªïng m√£ x√©t nghi·ªám: 896\n",
      "- T·ªïng l∆∞·ª£t ICD ghi nh·∫≠n: 6,364,488\n",
      "- T·ªïng l∆∞·ª£t th·ªß thu·∫≠t ghi nh·∫≠n: 859,655\n",
      "- T·ªïng l∆∞·ª£t x√©t nghi·ªám ghi nh·∫≠n: 84,605,867\n",
      "üìÇ C√°c file ƒë√£ l∆∞u trong /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "TOP_N = 50  # ‚¨ÖÔ∏è ch·ªânh ·ªü ƒë√¢y n·∫øu mu·ªën ƒë·ªïi s·ªë l∆∞·ª£ng m√£ l·∫•y\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "PROC_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# H√ÄM: L·∫§Y & L∆ØU TOP M√É PH·ªî BI·∫æN\n",
    "# ==========================\n",
    "def export_top_codes(HOSP_DIR, PROC_DIR, TOP_N=50):\n",
    "    # --- 1Ô∏è‚É£ ICD (diagnoses_icd) ---\n",
    "    diag_path = HOSP_DIR / \"diagnoses_icd.csv.gz\"\n",
    "    print(\"üìñ ƒêang x·ª≠ l√Ω ICD...\")\n",
    "    diag_cnt = Counter()\n",
    "    for chunk in pd.read_csv(diag_path, usecols=[\"hadm_id\", \"icd_code\"], compression=\"gzip\", chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", \"icd_code\"])\n",
    "        diag_cnt.update(chunk[\"icd_code\"].astype(str))\n",
    "    diag_df = pd.DataFrame(diag_cnt.items(), columns=[\"icd_code\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "    diag_top = diag_df.head(TOP_N)\n",
    "    out_icd = PROC_DIR / \"icd.csv\"\n",
    "    diag_top.to_csv(out_icd, index=False)\n",
    "    print(f\"‚úÖ L∆∞u {TOP_N} m√£ ICD ‚Üí {out_icd} ({len(diag_df):,} m√£, t·ªïng {diag_df['count'].sum():,} l∆∞·ª£t)\")\n",
    "\n",
    "    # --- 2Ô∏è‚É£ Procedures (procedures_icd) ---\n",
    "    proc_path = HOSP_DIR / \"procedures_icd.csv.gz\"\n",
    "    print(\"üõ†Ô∏è  ƒêang x·ª≠ l√Ω th·ªß thu·∫≠t...\")\n",
    "    proc_cnt = Counter()\n",
    "    for chunk in pd.read_csv(proc_path, usecols=[\"hadm_id\", \"icd_code\"], compression=\"gzip\", chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", \"icd_code\"])\n",
    "        proc_cnt.update(chunk[\"icd_code\"].astype(str))\n",
    "    proc_df = pd.DataFrame(proc_cnt.items(), columns=[\"proc_code\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "    proc_top = proc_df.head(TOP_N)\n",
    "    out_proc = PROC_DIR / \"procedures.csv\"\n",
    "    proc_top.to_csv(out_proc, index=False)\n",
    "    print(f\"‚úÖ L∆∞u {TOP_N} th·ªß thu·∫≠t ‚Üí {out_proc} ({len(proc_df):,} m√£, t·ªïng {proc_df['count'].sum():,} l∆∞·ª£t)\")\n",
    "\n",
    "    # --- 3Ô∏è‚É£ Labs (labevents + d_labitems) ---\n",
    "    lab_path = HOSP_DIR / \"labevents.csv.gz\"\n",
    "    dlab_path = HOSP_DIR / \"d_labitems.csv.gz\"\n",
    "    print(\"üß™ ƒêang x·ª≠ l√Ω x√©t nghi·ªám...\")\n",
    "    lab_cnt = Counter()\n",
    "    for chunk in pd.read_csv(lab_path, usecols=[\"hadm_id\", \"itemid\"], compression=\"gzip\", chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", \"itemid\"])\n",
    "        lab_cnt.update(chunk[\"itemid\"].astype(int))\n",
    "    lab_df = pd.DataFrame(lab_cnt.items(), columns=[\"itemid\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "\n",
    "    # G·∫Øn label t·ª´ d_labitems\n",
    "    dlab = pd.read_csv(dlab_path, usecols=[\"itemid\", \"label\"], compression=\"gzip\")\n",
    "    lab_df = lab_df.merge(dlab, on=\"itemid\", how=\"left\")\n",
    "    lab_top = lab_df.head(TOP_N)\n",
    "    out_lab = PROC_DIR / \"labs.csv\"\n",
    "    lab_top.to_csv(out_lab, index=False)\n",
    "    print(f\"‚úÖ L∆∞u {TOP_N} x√©t nghi·ªám ‚Üí {out_lab} ({len(lab_df):,} m√£, t·ªïng {lab_df['count'].sum():,} l∆∞·ª£t)\")\n",
    "\n",
    "    # --- T·ªïng k·∫øt ---\n",
    "    print(\"\\n=== T·ªîNG K·∫æT ===\")\n",
    "    print(f\"- T·ªïng m√£ ICD: {len(diag_df):,}\")\n",
    "    print(f\"- T·ªïng m√£ th·ªß thu·∫≠t: {len(proc_df):,}\")\n",
    "    print(f\"- T·ªïng m√£ x√©t nghi·ªám: {len(lab_df):,}\")\n",
    "    print(f\"- T·ªïng l∆∞·ª£t ICD ghi nh·∫≠n: {diag_df['count'].sum():,}\")\n",
    "    print(f\"- T·ªïng l∆∞·ª£t th·ªß thu·∫≠t ghi nh·∫≠n: {proc_df['count'].sum():,}\")\n",
    "    print(f\"- T·ªïng l∆∞·ª£t x√©t nghi·ªám ghi nh·∫≠n: {lab_df['count'].sum():,}\")\n",
    "    print(f\"üìÇ C√°c file ƒë√£ l∆∞u trong {PROC_DIR.resolve()}\")\n",
    "\n",
    "# ==========================\n",
    "# G·ªåI H√ÄM\n",
    "# ==========================\n",
    "export_top_codes(HOSP_DIR, PROC_DIR, TOP_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a30ee0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒê·ªçc vocab top50: ICD=50, PROC=50, LAB=50\n",
      "üìñ ƒêang l·ªçc hadm theo top ICD...\n",
      "‚Üí 454,431 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ICD trong top 50\n",
      "üõ†Ô∏è  ƒêang l·ªçc hadm theo top th·ªß thu·∫≠t...\n",
      "‚Üí 145,381 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ th·ªß thu·∫≠t trong top 50\n",
      "üß™ ƒêang l·ªçc hadm theo top x√©t nghi·ªám...\n",
      "‚Üí 441,727 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ x√©t nghi·ªám trong top 50\n",
      "ü©∫ ƒêang l·ªçc hadm c√≥ ghi ch√∫ xu·∫•t vi·ªán...\n",
      "‚Üí 331,793 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ note xu·∫•t vi·ªán\n",
      "\n",
      "‚úÖ T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ƒë·ªß 4 y·∫øu t·ªë (ICD + PROC + LAB + NOTE): 87,951\n",
      "üìò ƒêang l·ªçc b·∫£ng admissions...\n",
      "üìÇ ƒê√£ l∆∞u subset admissions v·ªõi 87,951 l∆∞·ª£t nh·∫≠p vi·ªán ‚Üí /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset/admissions.csv\n",
      "Bao g·ªìm c√°c c·ªôt: hadm_id, subject_id, admittime, dischtime, deathtime\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROC_DIR = SUBSET_DIR\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ ƒê·ªçc top 50 m√£ ph·ªï bi·∫øn\n",
    "# ==========================\n",
    "top_icd = pd.read_csv(PROC_DIR / \"icd.csv\")[\"icd_code\"].astype(str).tolist()\n",
    "top_proc = pd.read_csv(PROC_DIR / \"procedures.csv\")[\"proc_code\"].astype(str).tolist()\n",
    "top_lab = pd.read_csv(PROC_DIR / \"labs.csv\")[\"itemid\"].astype(int).tolist()\n",
    "\n",
    "print(f\"üîç ƒê·ªçc vocab top50: ICD={len(top_icd)}, PROC={len(top_proc)}, LAB={len(top_lab)}\")\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ H√†m ti·ªán √≠ch: l·∫•y danh s√°ch hadm_id theo m√£\n",
    "# ==========================\n",
    "def get_hadm_with_codes(path, col_code, top_list, dtype=\"str\"):\n",
    "    hadm_set = set()\n",
    "    for chunk in pd.read_csv(path, usecols=[\"hadm_id\", col_code], chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", col_code])\n",
    "        if dtype == \"str\":\n",
    "            chunk[col_code] = chunk[col_code].astype(str)\n",
    "        else:\n",
    "            chunk[col_code] = chunk[col_code].astype(int)\n",
    "        matched = chunk.loc[chunk[col_code].isin(top_list), \"hadm_id\"].astype(int)\n",
    "        hadm_set.update(matched.tolist())\n",
    "    return hadm_set\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ L·ªçc theo t·ª´ng lo·∫°i\n",
    "# ==========================\n",
    "print(\"üìñ ƒêang l·ªçc hadm theo top ICD...\")\n",
    "hadm_icd = get_hadm_with_codes(HOSP_DIR / \"diagnoses_icd.csv.gz\", \"icd_code\", top_icd, \"str\")\n",
    "print(f\"‚Üí {len(hadm_icd):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ICD trong top 50\")\n",
    "\n",
    "print(\"üõ†Ô∏è  ƒêang l·ªçc hadm theo top th·ªß thu·∫≠t...\")\n",
    "hadm_proc = get_hadm_with_codes(HOSP_DIR / \"procedures_icd.csv.gz\", \"icd_code\", top_proc, \"str\")\n",
    "print(f\"‚Üí {len(hadm_proc):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ th·ªß thu·∫≠t trong top 50\")\n",
    "\n",
    "print(\"üß™ ƒêang l·ªçc hadm theo top x√©t nghi·ªám...\")\n",
    "hadm_lab = get_hadm_with_codes(HOSP_DIR / \"labevents.csv.gz\", \"itemid\", top_lab, \"int\")\n",
    "print(f\"‚Üí {len(hadm_lab):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ x√©t nghi·ªám trong top 50\")\n",
    "\n",
    "print(\"ü©∫ ƒêang l·ªçc hadm c√≥ ghi ch√∫ xu·∫•t vi·ªán...\")\n",
    "hadm_note = set()\n",
    "for chunk in pd.read_csv(NOTE_DIR / \"discharge.csv.gz\", usecols=[\"hadm_id\", \"text\"], chunksize=200_000):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"])\n",
    "    hadm_note.update(chunk[\"hadm_id\"].astype(int).tolist())\n",
    "print(f\"‚Üí {len(hadm_note):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ note xu·∫•t vi·ªán\")\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ Giao 4 t·∫≠p\n",
    "# ==========================\n",
    "hadm_all = hadm_icd & hadm_proc & hadm_lab & hadm_note\n",
    "print(f\"\\n‚úÖ T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ƒë·ªß 4 y·∫øu t·ªë (ICD + PROC + LAB + NOTE): {len(hadm_all):,}\")\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ L·ªçc b·∫£ng admissions v√† l∆∞u ra CSV th∆∞·ªùng\n",
    "# ==========================\n",
    "adm_path = HOSP_DIR / \"admissions.csv.gz\"\n",
    "print(\"üìò ƒêang l·ªçc b·∫£ng admissions...\")\n",
    "\n",
    "admissions = pd.read_csv(\n",
    "    adm_path,\n",
    "    usecols=[\"hadm_id\", \"subject_id\", \"admittime\", \"dischtime\", \"deathtime\"],\n",
    "    compression=\"gzip\",\n",
    "    parse_dates=[\"admittime\", \"dischtime\", \"deathtime\"],\n",
    ")\n",
    "\n",
    "admissions = admissions[admissions[\"hadm_id\"].isin(hadm_all)].copy()\n",
    "admissions = admissions.sort_values(\"admittime\")\n",
    "\n",
    "out_path = SUBSET_DIR / \"admissions.csv\"\n",
    "admissions.to_csv(out_path, index=False)  # üíæ kh√¥ng n√©n gzip\n",
    "\n",
    "print(f\"üìÇ ƒê√£ l∆∞u subset admissions v·ªõi {len(admissions):,} l∆∞·ª£t nh·∫≠p vi·ªán ‚Üí {out_path.resolve()}\")\n",
    "print(\"Bao g·ªìm c√°c c·ªôt: hadm_id, subject_id, admittime, dischtime, deathtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0043762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç T·ªïng s·ªë b·ªánh nh√¢n c·∫ßn gi·ªØ: 52,610\n",
      "‚úÖ ƒê√£ l·ªçc 52,610 b·ªánh nh√¢n trong t·∫≠p admissions subset\n",
      "üìÇ ƒê√£ l∆∞u file r√∫t g·ªçn ‚Üí /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset/patients.csv\n",
      "Bao g·ªìm c√°c c·ªôt: subject_id, gender, anchor_age, anchor_year\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ ƒê·ªçc danh s√°ch b·ªánh nh√¢n trong subset admissions\n",
    "# ==========================\n",
    "adm_subset_path = SUBSET_DIR / \"admissions.csv\"\n",
    "if not adm_subset_path.exists():\n",
    "    adm_subset_path = SUBSET_DIR / \"admissions.csv.gz\"\n",
    "\n",
    "admissions = pd.read_csv(adm_subset_path, usecols=[\"hadm_id\", \"subject_id\"])\n",
    "subject_ids = set(admissions[\"subject_id\"].dropna().astype(int).tolist())\n",
    "\n",
    "print(f\"üîç T·ªïng s·ªë b·ªánh nh√¢n c·∫ßn gi·ªØ: {len(subject_ids):,}\")\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ ƒê·ªçc v√† l·ªçc b·∫£ng patients\n",
    "# ==========================\n",
    "patients_path = HOSP_DIR / \"patients.csv.gz\"\n",
    "usecols = [\"subject_id\", \"gender\", \"anchor_age\", \"anchor_year\"]\n",
    "\n",
    "patients = pd.read_csv(patients_path, usecols=usecols, compression=\"gzip\")\n",
    "patients = patients[patients[\"subject_id\"].isin(subject_ids)].copy()\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l·ªçc {len(patients):,} b·ªánh nh√¢n trong t·∫≠p admissions subset\")\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Chu·∫©n ho√° gi·ªõi t√≠nh v√† l∆∞u CSV (kh√¥ng n√©n)\n",
    "# ==========================\n",
    "patients[\"gender\"] = patients[\"gender\"].astype(str).str.upper().str[0]  # chu·∫©n h√≥a 'M', 'F', 'U'\n",
    "\n",
    "out_path = SUBSET_DIR / \"patients.csv\"\n",
    "patients.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"üìÇ ƒê√£ l∆∞u file r√∫t g·ªçn ‚Üí {out_path.resolve()}\")\n",
    "print(\"Bao g·ªìm c√°c c·ªôt: subject_id, gender, anchor_age, anchor_year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe031fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c·∫ßn gi·ªØ: 87,951\n",
      "ü©∫ ƒêang ƒë·ªçc v√† l√†m s·∫°ch ghi ch√∫ xu·∫•t vi·ªán...\n",
      "‚úÖ ƒê√£ l·ªçc & l√†m s·∫°ch 87,951 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ghi ch√∫ xu·∫•t vi·ªán\n",
      "üìÇ ƒê√£ l∆∞u file ghi ch√∫ xu·∫•t vi·ªán ‚Üí /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset/discharge.csv\n",
      "Bao g·ªìm c√°c c·ªôt: hadm_id, charttime, text (ƒë√£ b·ªè ph·∫ßn ƒë·∫ßu nh∆∞ng v·∫´n gi·ªØ 'Service:').\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc danh s√°ch hadm_id subset\n",
    "# ==========================\n",
    "admissions = pd.read_csv(SUBSET_DIR / \"admissions.csv\", usecols=[\"hadm_id\"])\n",
    "hadm_ids = set(admissions[\"hadm_id\"].astype(int).tolist())\n",
    "print(f\"üîç T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c·∫ßn gi·ªØ: {len(hadm_ids):,}\")\n",
    "\n",
    "# ==========================\n",
    "# H√†m l√†m s·∫°ch ghi ch√∫ (c·∫Øt ƒë·∫øn *tr∆∞·ªõc* 'Service:')\n",
    "# ==========================\n",
    "def clean_discharge_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "\n",
    "    # üö© T√¨m \"Service:\" v√† c·∫Øt b·ªè m·ªçi th·ª© tr∆∞·ªõc n√≥ (nh∆∞ng v·∫´n gi·ªØ 'Service:')\n",
    "    idx = s.find(\"Service:\")\n",
    "    if idx != -1:\n",
    "        s = s[idx:]  # gi·ªØ nguy√™n t·ª´ 'Service:' tr·ªü ƒëi\n",
    "\n",
    "    # L√†m s·∫°ch k√Ω t·ª± ƒë·∫∑c bi·ªát / d√≤ng trang tr√≠\n",
    "    s = re.sub(r\"[-_=]{3,}\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh file qu√° l·ªõn\n",
    "    return s[:4000]\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc & l·ªçc discharge note\n",
    "# ==========================\n",
    "discharge_path = NOTE_DIR / \"discharge.csv.gz\"\n",
    "rows = []\n",
    "\n",
    "print(\"ü©∫ ƒêang ƒë·ªçc v√† l√†m s·∫°ch ghi ch√∫ xu·∫•t vi·ªán...\")\n",
    "for chunk in pd.read_csv(discharge_path, usecols=[\"hadm_id\", \"charttime\", \"text\"], compression=\"gzip\", chunksize=200_000):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"]).copy()\n",
    "    chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(int)\n",
    "    chunk = chunk[chunk[\"hadm_id\"].isin(hadm_ids)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"], errors=\"coerce\")\n",
    "    chunk[\"text\"] = chunk[\"text\"].map(clean_discharge_text)\n",
    "    rows.append(chunk[[\"hadm_id\", \"charttime\", \"text\"]])\n",
    "\n",
    "# ==========================\n",
    "# G·ªôp & ch·ªçn note m·ªõi nh·∫•t\n",
    "# ==========================\n",
    "if rows:\n",
    "    discharge_df = pd.concat(rows, ignore_index=True)\n",
    "    discharge_df = (\n",
    "        discharge_df.sort_values([\"hadm_id\", \"charttime\"], ascending=[True, False])\n",
    "        .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "    )\n",
    "else:\n",
    "    discharge_df = pd.DataFrame(columns=[\"hadm_id\", \"charttime\", \"text\"])\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l·ªçc & l√†m s·∫°ch {len(discharge_df):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ghi ch√∫ xu·∫•t vi·ªán\")\n",
    "\n",
    "# ==========================\n",
    "# L∆∞u CSV\n",
    "# ==========================\n",
    "out_path = SUBSET_DIR / \"discharge.csv\"\n",
    "discharge_df.to_csv(out_path, index=False)\n",
    "print(f\"üìÇ ƒê√£ l∆∞u file ghi ch√∫ xu·∫•t vi·ªán ‚Üí {out_path.resolve()}\")\n",
    "print(\"Bao g·ªìm c√°c c·ªôt: hadm_id, charttime, text (ƒë√£ b·ªè ph·∫ßn ƒë·∫ßu nh∆∞ng v·∫´n gi·ªØ 'Service:').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eda0089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò X·ª≠ l√Ω icd.csv ‚Üí d_icd_diagnoses.csv\n",
      "‚úÖ L∆∞u 50 d√≤ng ‚Üí ../data/mimiciv_subset/d_icd_diagnoses.csv\n",
      "\n",
      "üìò X·ª≠ l√Ω procedures.csv ‚Üí d_icd_procedures.csv\n",
      "‚úÖ L∆∞u 50 d√≤ng ‚Üí ../data/mimiciv_subset/d_icd_procedures.csv\n",
      "\n",
      "=== T·ªîNG K·∫æT ===\n",
      "- d_icd_diagnoses.csv: 50 d√≤ng\n",
      "- d_icd_procedures.csv: 50 d√≤ng\n",
      "üìÇ ƒê√£ l∆∞u v√†o: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ H√†m ti·ªán √≠ch\n",
    "# ==========================\n",
    "def export_filtered_dict(top_file, dict_file, out_name, code_col=\"icd_code\"):\n",
    "    print(f\"\\nüìò X·ª≠ l√Ω {top_file.name} ‚Üí {out_name}\")\n",
    "    \n",
    "    # ƒê·ªçc danh s√°ch top\n",
    "    top_df = pd.read_csv(top_file)\n",
    "    top_codes = top_df[top_df.columns[0]].astype(str).tolist()\n",
    "    \n",
    "    # ƒê·ªçc dictionary g·ªëc\n",
    "    dict_df = pd.read_csv(dict_file, compression=\"gzip\")\n",
    "    dict_df[code_col] = dict_df[code_col].astype(str)\n",
    "    \n",
    "    # L·ªçc theo top\n",
    "    filtered = dict_df[dict_df[code_col].isin(top_codes)].copy()\n",
    "    \n",
    "    # Merge th√™m c·ªôt count ƒë·ªÉ ti·ªán tra c·ª©u\n",
    "    filtered = filtered.merge(top_df, left_on=code_col, right_on=top_df.columns[0], how=\"left\")\n",
    "    filtered = filtered.sort_values(\"count\", ascending=False)\n",
    "    \n",
    "    # L∆∞u file v·ªõi t√™n g·ªëc\n",
    "    out_path = SUBSET_DIR / out_name\n",
    "    filtered.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ L∆∞u {len(filtered):,} d√≤ng ‚Üí {out_path}\")\n",
    "    return filtered\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ ICD Diagnoses\n",
    "# ==========================\n",
    "diag_filtered = export_filtered_dict(\n",
    "    top_file = SUBSET_DIR / \"icd.csv\",\n",
    "    dict_file = HOSP_DIR / \"d_icd_diagnoses.csv.gz\",\n",
    "    out_name  = \"d_icd_diagnoses.csv\",\n",
    "    code_col  = \"icd_code\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ ICD Procedures\n",
    "# ==========================\n",
    "proc_filtered = export_filtered_dict(\n",
    "    top_file = SUBSET_DIR / \"procedures.csv\",\n",
    "    dict_file = HOSP_DIR / \"d_icd_procedures.csv.gz\",\n",
    "    out_name  = \"d_icd_procedures.csv\",\n",
    "    code_col  = \"icd_code\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ T·ªïng k·∫øt\n",
    "# ==========================\n",
    "print(\"\\n=== T·ªîNG K·∫æT ===\")\n",
    "print(f\"- d_icd_diagnoses.csv: {len(diag_filtered):,} d√≤ng\")\n",
    "print(f\"- d_icd_procedures.csv: {len(proc_filtered):,} d√≤ng\")\n",
    "print(f\"üìÇ ƒê√£ l∆∞u v√†o: {SUBSET_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
