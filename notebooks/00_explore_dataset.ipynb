{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4056183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ƒêang x·ª≠ l√Ω ICD...\n",
      "‚úÖ L∆∞u 50 m√£ ICD ‚Üí ../data/mimiciv_subset/icd.csv (28,562 m√£, t·ªïng 6,364,488 l∆∞·ª£t)\n",
      "üõ†Ô∏è  ƒêang x·ª≠ l√Ω th·ªß thu·∫≠t...\n",
      "‚úÖ L∆∞u 50 th·ªß thu·∫≠t ‚Üí ../data/mimiciv_subset/procedures.csv (14,911 m√£, t·ªïng 859,655 l∆∞·ª£t)\n",
      "üß™ ƒêang x·ª≠ l√Ω x√©t nghi·ªám...\n",
      "‚úÖ L∆∞u 50 x√©t nghi·ªám ‚Üí ../data/mimiciv_subset/labs.csv (896 m√£, t·ªïng 84,605,867 l∆∞·ª£t)\n",
      "\n",
      "=== T·ªîNG K·∫æT ===\n",
      "- T·ªïng m√£ ICD: 28,562\n",
      "- T·ªïng m√£ th·ªß thu·∫≠t: 14,911\n",
      "- T·ªïng m√£ x√©t nghi·ªám: 896\n",
      "- T·ªïng l∆∞·ª£t ICD ghi nh·∫≠n: 6,364,488\n",
      "- T·ªïng l∆∞·ª£t th·ªß thu·∫≠t ghi nh·∫≠n: 859,655\n",
      "- T·ªïng l∆∞·ª£t x√©t nghi·ªám ghi nh·∫≠n: 84,605,867\n",
      "üìÇ C√°c file ƒë√£ l∆∞u trong /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "TOP_N = 50  # ‚¨ÖÔ∏è ch·ªânh ·ªü ƒë√¢y n·∫øu mu·ªën ƒë·ªïi s·ªë l∆∞·ª£ng m√£ l·∫•y\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "PROC_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# H√ÄM: L·∫§Y & L∆ØU TOP M√É PH·ªî BI·∫æN\n",
    "# ==========================\n",
    "def export_top_codes(HOSP_DIR, PROC_DIR, TOP_N=50):\n",
    "    # --- 1Ô∏è‚É£ ICD (diagnoses_icd) ---\n",
    "    diag_path = HOSP_DIR / \"diagnoses_icd.csv.gz\"\n",
    "    print(\"üìñ ƒêang x·ª≠ l√Ω ICD...\")\n",
    "    diag_cnt = Counter()\n",
    "    for chunk in pd.read_csv(diag_path, usecols=[\"hadm_id\", \"icd_code\"], compression=\"gzip\", chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", \"icd_code\"])\n",
    "        diag_cnt.update(chunk[\"icd_code\"].astype(str))\n",
    "    diag_df = pd.DataFrame(diag_cnt.items(), columns=[\"icd_code\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "    diag_top = diag_df.head(TOP_N)\n",
    "    out_icd = PROC_DIR / \"icd.csv\"\n",
    "    diag_top.to_csv(out_icd, index=False)\n",
    "    print(f\"‚úÖ L∆∞u {TOP_N} m√£ ICD ‚Üí {out_icd} ({len(diag_df):,} m√£, t·ªïng {diag_df['count'].sum():,} l∆∞·ª£t)\")\n",
    "\n",
    "    # --- 2Ô∏è‚É£ Procedures (procedures_icd) ---\n",
    "    proc_path = HOSP_DIR / \"procedures_icd.csv.gz\"\n",
    "    print(\"üõ†Ô∏è  ƒêang x·ª≠ l√Ω th·ªß thu·∫≠t...\")\n",
    "    proc_cnt = Counter()\n",
    "    for chunk in pd.read_csv(proc_path, usecols=[\"hadm_id\", \"icd_code\"], compression=\"gzip\", chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", \"icd_code\"])\n",
    "        proc_cnt.update(chunk[\"icd_code\"].astype(str))\n",
    "    proc_df = pd.DataFrame(proc_cnt.items(), columns=[\"proc_code\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "    proc_top = proc_df.head(TOP_N)\n",
    "    out_proc = PROC_DIR / \"procedures.csv\"\n",
    "    proc_top.to_csv(out_proc, index=False)\n",
    "    print(f\"‚úÖ L∆∞u {TOP_N} th·ªß thu·∫≠t ‚Üí {out_proc} ({len(proc_df):,} m√£, t·ªïng {proc_df['count'].sum():,} l∆∞·ª£t)\")\n",
    "\n",
    "    # --- 3Ô∏è‚É£ Labs (labevents + d_labitems) ---\n",
    "    lab_path = HOSP_DIR / \"labevents.csv.gz\"\n",
    "    dlab_path = HOSP_DIR / \"d_labitems.csv.gz\"\n",
    "    print(\"üß™ ƒêang x·ª≠ l√Ω x√©t nghi·ªám...\")\n",
    "    lab_cnt = Counter()\n",
    "    for chunk in pd.read_csv(lab_path, usecols=[\"hadm_id\", \"itemid\"], compression=\"gzip\", chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", \"itemid\"])\n",
    "        lab_cnt.update(chunk[\"itemid\"].astype(int))\n",
    "    lab_df = pd.DataFrame(lab_cnt.items(), columns=[\"itemid\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "\n",
    "    # G·∫Øn label t·ª´ d_labitems\n",
    "    dlab = pd.read_csv(dlab_path, usecols=[\"itemid\", \"label\"], compression=\"gzip\")\n",
    "    lab_df = lab_df.merge(dlab, on=\"itemid\", how=\"left\")\n",
    "    lab_top = lab_df.head(TOP_N)\n",
    "    out_lab = PROC_DIR / \"labs.csv\"\n",
    "    lab_top.to_csv(out_lab, index=False)\n",
    "    print(f\"‚úÖ L∆∞u {TOP_N} x√©t nghi·ªám ‚Üí {out_lab} ({len(lab_df):,} m√£, t·ªïng {lab_df['count'].sum():,} l∆∞·ª£t)\")\n",
    "\n",
    "    # --- T·ªïng k·∫øt ---\n",
    "    print(\"\\n=== T·ªîNG K·∫æT ===\")\n",
    "    print(f\"- T·ªïng m√£ ICD: {len(diag_df):,}\")\n",
    "    print(f\"- T·ªïng m√£ th·ªß thu·∫≠t: {len(proc_df):,}\")\n",
    "    print(f\"- T·ªïng m√£ x√©t nghi·ªám: {len(lab_df):,}\")\n",
    "    print(f\"- T·ªïng l∆∞·ª£t ICD ghi nh·∫≠n: {diag_df['count'].sum():,}\")\n",
    "    print(f\"- T·ªïng l∆∞·ª£t th·ªß thu·∫≠t ghi nh·∫≠n: {proc_df['count'].sum():,}\")\n",
    "    print(f\"- T·ªïng l∆∞·ª£t x√©t nghi·ªám ghi nh·∫≠n: {lab_df['count'].sum():,}\")\n",
    "    print(f\"üìÇ C√°c file ƒë√£ l∆∞u trong {PROC_DIR.resolve()}\")\n",
    "\n",
    "# ==========================\n",
    "# G·ªåI H√ÄM\n",
    "# ==========================\n",
    "export_top_codes(HOSP_DIR, PROC_DIR, TOP_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30ee0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒê·ªçc vocab top50: ICD=50, PROC=50, LAB=50\n",
      "üìñ ƒêang l·ªçc hadm theo top ICD...\n",
      "‚Üí 454,431 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ICD trong top 50\n",
      "üõ†Ô∏è  ƒêang l·ªçc hadm theo top th·ªß thu·∫≠t...\n",
      "‚Üí 145,381 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ th·ªß thu·∫≠t trong top 50\n",
      "üß™ ƒêang l·ªçc hadm theo top x√©t nghi·ªám...\n",
      "‚Üí 441,727 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ x√©t nghi·ªám trong top 50\n",
      "ü©∫ ƒêang l·ªçc hadm c√≥ ghi ch√∫ xu·∫•t vi·ªán...\n",
      "‚Üí 331,793 l∆∞·ª£t nh·∫≠p vi·ªán c√≥ note xu·∫•t vi·ªán\n",
      "\n",
      "‚úÖ T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ƒë·ªß 4 y·∫øu t·ªë (ICD + PROC + LAB + NOTE): 87,951\n",
      "üìò ƒêang l·ªçc b·∫£ng admissions...\n",
      "üìÇ ƒê√£ l∆∞u subset admissions v·ªõi 87,951 l∆∞·ª£t nh·∫≠p vi·ªán ‚Üí /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset/admissions.csv\n",
      "Bao g·ªìm c√°c c·ªôt: hadm_id, subject_id, admittime, dischtime, deathtime\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROC_DIR = SUBSET_DIR\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ ƒê·ªçc top 50 m√£ ph·ªï bi·∫øn\n",
    "# ==========================\n",
    "top_icd = pd.read_csv(PROC_DIR / \"icd.csv\")[\"icd_code\"].astype(str).tolist()\n",
    "top_proc = pd.read_csv(PROC_DIR / \"procedures.csv\")[\"proc_code\"].astype(str).tolist()\n",
    "top_lab = pd.read_csv(PROC_DIR / \"labs.csv\")[\"itemid\"].astype(int).tolist()\n",
    "\n",
    "print(f\"üîç ƒê·ªçc vocab top50: ICD={len(top_icd)}, PROC={len(top_proc)}, LAB={len(top_lab)}\")\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ H√†m ti·ªán √≠ch: l·∫•y danh s√°ch hadm_id theo m√£\n",
    "# ==========================\n",
    "def get_hadm_with_codes(path, col_code, top_list, dtype=\"str\"):\n",
    "    hadm_set = set()\n",
    "    for chunk in pd.read_csv(path, usecols=[\"hadm_id\", col_code], chunksize=200_000):\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", col_code])\n",
    "        if dtype == \"str\":\n",
    "            chunk[col_code] = chunk[col_code].astype(str)\n",
    "        else:\n",
    "            chunk[col_code] = chunk[col_code].astype(int)\n",
    "        matched = chunk.loc[chunk[col_code].isin(top_list), \"hadm_id\"].astype(int)\n",
    "        hadm_set.update(matched.tolist())\n",
    "    return hadm_set\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ L·ªçc theo t·ª´ng lo·∫°i\n",
    "# ==========================\n",
    "print(\"üìñ ƒêang l·ªçc hadm theo top ICD...\")\n",
    "hadm_icd = get_hadm_with_codes(HOSP_DIR / \"diagnoses_icd.csv.gz\", \"icd_code\", top_icd, \"str\")\n",
    "print(f\"‚Üí {len(hadm_icd):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ICD trong top 50\")\n",
    "\n",
    "print(\"üõ†Ô∏è  ƒêang l·ªçc hadm theo top th·ªß thu·∫≠t...\")\n",
    "hadm_proc = get_hadm_with_codes(HOSP_DIR / \"procedures_icd.csv.gz\", \"icd_code\", top_proc, \"str\")\n",
    "print(f\"‚Üí {len(hadm_proc):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ th·ªß thu·∫≠t trong top 50\")\n",
    "\n",
    "print(\"üß™ ƒêang l·ªçc hadm theo top x√©t nghi·ªám...\")\n",
    "hadm_lab = get_hadm_with_codes(HOSP_DIR / \"labevents.csv.gz\", \"itemid\", top_lab, \"int\")\n",
    "print(f\"‚Üí {len(hadm_lab):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ x√©t nghi·ªám trong top 50\")\n",
    "\n",
    "print(\"ü©∫ ƒêang l·ªçc hadm c√≥ ghi ch√∫ xu·∫•t vi·ªán...\")\n",
    "hadm_note = set()\n",
    "for chunk in pd.read_csv(NOTE_DIR / \"discharge.csv.gz\", usecols=[\"hadm_id\", \"text\"], chunksize=200_000):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"])\n",
    "    hadm_note.update(chunk[\"hadm_id\"].astype(int).tolist())\n",
    "print(f\"‚Üí {len(hadm_note):,} l∆∞·ª£t nh·∫≠p vi·ªán c√≥ note xu·∫•t vi·ªán\")\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ Giao 4 t·∫≠p\n",
    "# ==========================\n",
    "hadm_all = hadm_icd & hadm_proc & hadm_lab & hadm_note\n",
    "print(f\"\\n‚úÖ T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c√≥ ƒë·ªß 4 y·∫øu t·ªë (ICD + PROC + LAB + NOTE): {len(hadm_all):,}\")\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ L·ªçc b·∫£ng admissions v√† l∆∞u ra CSV th∆∞·ªùng\n",
    "# ==========================\n",
    "adm_path = HOSP_DIR / \"admissions.csv.gz\"\n",
    "print(\"üìò ƒêang l·ªçc b·∫£ng admissions...\")\n",
    "\n",
    "admissions = pd.read_csv(\n",
    "    adm_path,\n",
    "    usecols=[\"hadm_id\", \"subject_id\", \"admittime\", \"dischtime\", \"deathtime\"],\n",
    "    compression=\"gzip\",\n",
    "    parse_dates=[\"admittime\", \"dischtime\", \"deathtime\"],\n",
    ")\n",
    "\n",
    "admissions = admissions[admissions[\"hadm_id\"].isin(hadm_all)].copy()\n",
    "admissions = admissions.sort_values(\"admittime\")\n",
    "\n",
    "out_path = SUBSET_DIR / \"admissions.csv\"\n",
    "admissions.to_csv(out_path, index=False)  # üíæ kh√¥ng n√©n gzip\n",
    "\n",
    "print(f\"üìÇ ƒê√£ l∆∞u subset admissions v·ªõi {len(admissions):,} l∆∞·ª£t nh·∫≠p vi·ªán ‚Üí {out_path.resolve()}\")\n",
    "print(\"Bao g·ªìm c√°c c·ªôt: hadm_id, subject_id, admittime, dischtime, deathtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0043762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç T·ªïng s·ªë b·ªánh nh√¢n c·∫ßn gi·ªØ: 52,610\n",
      "‚úÖ ƒê√£ l·ªçc 52,610 b·ªánh nh√¢n trong t·∫≠p admissions subset\n",
      "üìÇ ƒê√£ l∆∞u file r√∫t g·ªçn ‚Üí /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset/patients.csv\n",
      "Bao g·ªìm c√°c c·ªôt: subject_id, gender, anchor_age, anchor_year\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ ƒê·ªçc danh s√°ch b·ªánh nh√¢n trong subset admissions\n",
    "# ==========================\n",
    "adm_subset_path = SUBSET_DIR / \"admissions.csv\"\n",
    "if not adm_subset_path.exists():\n",
    "    adm_subset_path = SUBSET_DIR / \"admissions.csv.gz\"\n",
    "\n",
    "admissions = pd.read_csv(adm_subset_path, usecols=[\"hadm_id\", \"subject_id\"])\n",
    "subject_ids = set(admissions[\"subject_id\"].dropna().astype(int).tolist())\n",
    "\n",
    "print(f\"üîç T·ªïng s·ªë b·ªánh nh√¢n c·∫ßn gi·ªØ: {len(subject_ids):,}\")\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ ƒê·ªçc v√† l·ªçc b·∫£ng patients\n",
    "# ==========================\n",
    "patients_path = HOSP_DIR / \"patients.csv.gz\"\n",
    "usecols = [\"subject_id\", \"gender\", \"anchor_age\", \"anchor_year\"]\n",
    "\n",
    "patients = pd.read_csv(patients_path, usecols=usecols, compression=\"gzip\")\n",
    "patients = patients[patients[\"subject_id\"].isin(subject_ids)].copy()\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l·ªçc {len(patients):,} b·ªánh nh√¢n trong t·∫≠p admissions subset\")\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Chu·∫©n ho√° gi·ªõi t√≠nh v√† l∆∞u CSV (kh√¥ng n√©n)\n",
    "# ==========================\n",
    "patients[\"gender\"] = patients[\"gender\"].astype(str).str.upper().str[0]  # chu·∫©n h√≥a 'M', 'F', 'U'\n",
    "\n",
    "out_path = SUBSET_DIR / \"patients.csv\"\n",
    "patients.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"üìÇ ƒê√£ l∆∞u file r√∫t g·ªçn ‚Üí {out_path.resolve()}\")\n",
    "print(\"Bao g·ªìm c√°c c·ªôt: subject_id, gender, anchor_age, anchor_year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe031fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c·∫ßn l·∫•y: 87,951\n",
      "‚úÖ ƒê√£ l∆∞u 87,951 ghi ch√∫ r√∫t g·ªçn ‚Üí ../data/mimiciv_subset/discharge.csv.gz\n",
      "üíæ Dung l∆∞·ª£ng sau n√©n: nh·ªè g·ªçn & s·∫µn s√†ng cho training ho·∫∑c ph√¢n t√≠ch NLP\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "admissions_path = SUBSET_DIR / \"admissions.csv\"\n",
    "note_path = NOTE_DIR / \"discharge.csv.gz\"\n",
    "out_path = SUBSET_DIR / \"discharge.csv.gz\"\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc danh s√°ch hadm_id trong subset\n",
    "# ==========================\n",
    "admissions = pd.read_csv(admissions_path, usecols=[\"hadm_id\"])\n",
    "hadm_ids = set(admissions[\"hadm_id\"].astype(int))\n",
    "print(f\"üìã T·ªïng s·ªë l∆∞·ª£t nh·∫≠p vi·ªán c·∫ßn l·∫•y: {len(hadm_ids):,}\")\n",
    "\n",
    "# ==========================\n",
    "# H√†m l√†m s·∫°ch ghi ch√∫\n",
    "# ==========================\n",
    "import re\n",
    "\n",
    "def clean_discharge_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract specific sections from a MIMIC-IV discharge note: Service, Chief Complaint,\n",
    "    History of Present Illness, Past Medical History, Family History, and Physical Exam\n",
    "    (admission only). Maintains compatibility with the original function's input/output format\n",
    "    and cleaning logic.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw discharge note text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text containing only the specified sections, with excessive whitespace\n",
    "             and lab result lines removed.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    # Define sections to keep\n",
    "    sections_to_keep = [\n",
    "        r\"Service:\\s*\",\n",
    "        r\"Chief Complaint:\\s*\",\n",
    "        r\"History of Present Illness:\\s*\",\n",
    "        r\"Past Medical History:\\s*\",\n",
    "        r\"Family History:\\s*\",\n",
    "        r\"Physical Exam:\\s*\"\n",
    "    ]\n",
    "\n",
    "    # Combine section headers into a regex pattern\n",
    "    section_pattern = r\"^(?:\" + \"|\".join(sections_to_keep) + r\")\"\n",
    "    section_headers = [re.escape(s).rstrip(r\"\\s*\") for s in sections_to_keep]\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.splitlines()\n",
    "    result = []\n",
    "    capture = False\n",
    "    current_section = None\n",
    "    physical_exam_count = 0  # Track to capture only the first Physical Exam\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if the line starts with a desired section header\n",
    "        if re.match(section_pattern, line, re.MULTILINE | re.IGNORECASE):\n",
    "            section_name = next((h for h in section_headers if line.lower().startswith(h.lower())), None)\n",
    "            if section_name:\n",
    "                # Only capture the first Physical Exam (admission)\n",
    "                if section_name == r\"Physical Exam:\":\n",
    "                    physical_exam_count += 1\n",
    "                    if physical_exam_count > 1:  # Skip subsequent Physical Exam sections\n",
    "                        capture = False\n",
    "                        continue\n",
    "                current_section = section_name\n",
    "                capture = True\n",
    "                result.append(line)\n",
    "                continue\n",
    "        # Capture lines for the current section\n",
    "        if capture:\n",
    "            # Stop capturing if we hit another section header or \"Discharge\"\n",
    "            if re.match(r\"^[A-Z][a-zA-Z\\s]*:\\s*$\", line, re.MULTILINE) or \"Discharge\" in line:\n",
    "                capture = False\n",
    "                continue\n",
    "            result.append(line)\n",
    "\n",
    "    # Join lines and clean up\n",
    "    text = \"\\n\".join(result).strip()\n",
    "    \n",
    "    # Remove lab result-like lines (e.g., \"Na-136\", \"WBC-5.0\") to match original function\n",
    "    text = re.sub(r\"\\b[A-Z]{2,}[A-Za-z0-9/%\\-]*-?\\d+(\\.\\d+)?\\*?\\b\", \"\", text)\n",
    "    \n",
    "    # Collapse excessive newlines to match original function\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc file discharge, l·ªçc theo hadm_id, l√†m s·∫°ch, l∆∞u l·∫°i .gz\n",
    "# ==========================\n",
    "out_chunks = []\n",
    "for chunk in pd.read_csv(note_path, usecols=[\"hadm_id\", \"charttime\", \"text\"], compression=\"gzip\", chunksize=20_000):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"])\n",
    "    chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(int)\n",
    "    chunk = chunk[chunk[\"hadm_id\"].isin(hadm_ids)]\n",
    "\n",
    "    if not chunk.empty:\n",
    "        chunk[\"text\"] = chunk[\"text\"].astype(str).apply(clean_discharge_text)\n",
    "        out_chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(out_chunks, ignore_index=True)\n",
    "df.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u {len(df):,} ghi ch√∫ r√∫t g·ªçn ‚Üí {out_path}\")\n",
    "print(f\"üíæ Dung l∆∞·ª£ng sau n√©n: nh·ªè g·ªçn & s·∫µn s√†ng cho training ho·∫∑c ph√¢n t√≠ch NLP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eda0089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò X·ª≠ l√Ω icd.csv ‚Üí d_icd_diagnoses.csv\n",
      "‚úÖ L∆∞u 50 d√≤ng ‚Üí ../data/mimiciv_subset/d_icd_diagnoses.csv\n",
      "\n",
      "üìò X·ª≠ l√Ω procedures.csv ‚Üí d_icd_procedures.csv\n",
      "‚úÖ L∆∞u 50 d√≤ng ‚Üí ../data/mimiciv_subset/d_icd_procedures.csv\n",
      "\n",
      "=== T·ªîNG K·∫æT ===\n",
      "- d_icd_diagnoses.csv: 50 d√≤ng\n",
      "- d_icd_procedures.csv: 50 d√≤ng\n",
      "üìÇ ƒê√£ l∆∞u v√†o: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/mimiciv_subset\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ H√†m ti·ªán √≠ch\n",
    "# ==========================\n",
    "def export_filtered_dict(top_file, dict_file, out_name, code_col=\"icd_code\"):\n",
    "    print(f\"\\nüìò X·ª≠ l√Ω {top_file.name} ‚Üí {out_name}\")\n",
    "    \n",
    "    # ƒê·ªçc danh s√°ch top\n",
    "    top_df = pd.read_csv(top_file)\n",
    "    top_codes = top_df[top_df.columns[0]].astype(str).tolist()\n",
    "    \n",
    "    # ƒê·ªçc dictionary g·ªëc\n",
    "    dict_df = pd.read_csv(dict_file, compression=\"gzip\")\n",
    "    dict_df[code_col] = dict_df[code_col].astype(str)\n",
    "    \n",
    "    # L·ªçc theo top\n",
    "    filtered = dict_df[dict_df[code_col].isin(top_codes)].copy()\n",
    "    \n",
    "    # Merge th√™m c·ªôt count ƒë·ªÉ ti·ªán tra c·ª©u\n",
    "    filtered = filtered.merge(top_df, left_on=code_col, right_on=top_df.columns[0], how=\"left\")\n",
    "    filtered = filtered.sort_values(\"count\", ascending=False)\n",
    "    \n",
    "    # L∆∞u file v·ªõi t√™n g·ªëc\n",
    "    out_path = SUBSET_DIR / out_name\n",
    "    filtered.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ L∆∞u {len(filtered):,} d√≤ng ‚Üí {out_path}\")\n",
    "    return filtered\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ ICD Diagnoses\n",
    "# ==========================\n",
    "diag_filtered = export_filtered_dict(\n",
    "    top_file = SUBSET_DIR / \"icd.csv\",\n",
    "    dict_file = HOSP_DIR / \"d_icd_diagnoses.csv.gz\",\n",
    "    out_name  = \"d_icd_diagnoses.csv\",\n",
    "    code_col  = \"icd_code\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ ICD Procedures\n",
    "# ==========================\n",
    "proc_filtered = export_filtered_dict(\n",
    "    top_file = SUBSET_DIR / \"procedures.csv\",\n",
    "    dict_file = HOSP_DIR / \"d_icd_procedures.csv.gz\",\n",
    "    out_name  = \"d_icd_procedures.csv\",\n",
    "    code_col  = \"icd_code\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ T·ªïng k·∫øt\n",
    "# ==========================\n",
    "print(\"\\n=== T·ªîNG K·∫æT ===\")\n",
    "print(f\"- d_icd_diagnoses.csv: {len(diag_filtered):,} d√≤ng\")\n",
    "print(f\"- d_icd_procedures.csv: {len(proc_filtered):,} d√≤ng\")\n",
    "print(f\"üìÇ ƒê√£ l∆∞u v√†o: {SUBSET_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afad636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã ƒê·ªçc 87,951 l∆∞·ª£t nh·∫≠p vi·ªán trong subset admissions.\n",
      "‚úÖ ƒê√£ l∆∞u 87,951 ghi ch√∫ xu·∫•t vi·ªán v√†o ‚Üí ../data/mimiciv_subset/discharge_full.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ ƒê·ªçc danh s√°ch hadm_id t·ª´ admissions subset\n",
    "# ==========================\n",
    "adm_path = SUBSET_DIR / \"admissions.csv\"\n",
    "admissions = pd.read_csv(adm_path, usecols=[\"hadm_id\"])\n",
    "hadm_set = set(admissions[\"hadm_id\"].astype(int))\n",
    "print(f\"üìã ƒê·ªçc {len(hadm_set):,} l∆∞·ª£t nh·∫≠p vi·ªán trong subset admissions.\")\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ ƒê·ªçc discharge note v√† l·ªçc theo hadm_id\n",
    "# ==========================\n",
    "note_path = NOTE_DIR / \"discharge.csv.gz\"\n",
    "out_path = SUBSET_DIR / \"discharge_full.csv.gz\"\n",
    "\n",
    "rows = []\n",
    "for chunk in pd.read_csv(\n",
    "    note_path, \n",
    "    usecols=[\"hadm_id\", \"charttime\", \"text\"], \n",
    "    compression=\"gzip\",\n",
    "    chunksize=100_000\n",
    "):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"]).copy()\n",
    "    chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(int)\n",
    "    matched = chunk[chunk[\"hadm_id\"].isin(hadm_set)]\n",
    "    if not matched.empty:\n",
    "        rows.append(matched)\n",
    "\n",
    "if rows:\n",
    "    df_out = pd.concat(rows, ignore_index=True)\n",
    "else:\n",
    "    df_out = pd.DataFrame(columns=[\"hadm_id\", \"charttime\", \"text\"])\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ L∆∞u k·∫øt qu·∫£ ra file n√©n gzip\n",
    "# ==========================\n",
    "df_out.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u {len(df_out):,} ghi ch√∫ xu·∫•t vi·ªán v√†o ‚Üí {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd358d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc danh s√°ch l∆∞·ª£t nh·∫≠p vi·ªán trong subset\n",
    "# ==========================\n",
    "adm_subset_path = SUBSET_DIR / \"admissions.csv\"\n",
    "admissions = pd.read_csv(adm_subset_path, usecols=[\"hadm_id\"])\n",
    "hadm_set = set(admissions[\"hadm_id\"].astype(int))\n",
    "\n",
    "# ==========================\n",
    "# L·∫•y 1 ghi ch√∫ xu·∫•t vi·ªán ƒë·∫ßy ƒë·ªß (ƒë·∫ßu ti√™n trong subset)\n",
    "# ==========================\n",
    "note_path = NOTE_DIR / \"discharge.csv.gz\"\n",
    "for chunk in pd.read_csv(note_path, usecols=[\"hadm_id\", \"charttime\", \"text\"], compression=\"gzip\", chunksize=50_000):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"])\n",
    "    chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(int)\n",
    "    match = chunk[chunk[\"hadm_id\"].isin(hadm_set)]\n",
    "    if not match.empty:\n",
    "        sample_df = match.iloc[[0]]\n",
    "        break\n",
    "else:\n",
    "    sample_df = pd.DataFrame(columns=[\"hadm_id\", \"charttime\", \"text\"])\n",
    "\n",
    "# ==========================\n",
    "# L∆∞u ra file CSV (1 ghi ch√∫ duy nh·∫•t)\n",
    "# ==========================\n",
    "out_path = SUBSET_DIR / \"sample_discharge.csv\"\n",
    "sample_df.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fceef01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1).to_csv(\"../data/mimiciv_subset/discharge_example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60e346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08863cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä T·ªîNG K·∫æT CHO L·∫¶N NH·∫¨P VI·ªÜN 27783915\n",
      "- T·ªïng s·ªë ch·∫©n ƒëo√°n: 25  |  N·∫±m trong top: 8\n",
      "- T·ªïng s·ªë th·ªß thu·∫≠t: 7  |  N·∫±m trong top: 5\n",
      "- T·ªïng s·ªë x√©t nghi·ªám: 230  |  N·∫±m trong top: 189\n",
      "üìÇ ƒê√£ l∆∞u th·ªëng k√™ ‚Üí ../data/mimiciv_subset/summary_27783915.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================\n",
    "# C·∫§U H√åNH\n",
    "# ==============================\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "HADM_ID = 27783915  # üè• L·∫ßn nh·∫≠p vi·ªán c·∫ßn ki·ªÉm tra\n",
    "\n",
    "# ==============================\n",
    "# 1Ô∏è‚É£ ƒê·ªåC DANH S√ÅCH TOP M√É\n",
    "# ==============================\n",
    "top_icd = pd.read_csv(SUBSET_DIR / \"icd.csv\")[\"icd_code\"].astype(str).tolist()\n",
    "top_proc = pd.read_csv(SUBSET_DIR / \"procedures.csv\")[\"proc_code\"].astype(str).tolist()\n",
    "top_lab = pd.read_csv(SUBSET_DIR / \"labs.csv\")[\"itemid\"].astype(int).tolist()\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ ƒê·ªåC C√ÅC FILE C·ª¶A B·ªÜNH NH√ÇN\n",
    "# ==============================\n",
    "diag = pd.read_csv(SUBSET_DIR / f\"diagnoses_{HADM_ID}.csv\")\n",
    "proc = pd.read_csv(SUBSET_DIR / f\"procedures_{HADM_ID}.csv\")\n",
    "labs = pd.read_csv(SUBSET_DIR / f\"labs_{HADM_ID}.csv\")\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£ ƒê·∫æM S·ªê L∆Ø·ª¢NG N·∫∞M TRONG TOP\n",
    "# ==============================\n",
    "diag_in_top = diag[diag[\"icd_code\"].astype(str).isin(top_icd)]\n",
    "proc_in_top = proc[proc[\"icd_code\"].astype(str).isin(top_proc)]\n",
    "labs_in_top = labs[labs[\"itemid\"].astype(int).isin(top_lab)]\n",
    "\n",
    "summary = {\n",
    "    \"hadm_id\": HADM_ID,\n",
    "    \"total_diagnoses\": len(diag),\n",
    "    \"total_procedures\": len(proc),\n",
    "    \"total_labs\": len(labs),\n",
    "    \"diagnoses_in_top\": len(diag_in_top),\n",
    "    \"procedures_in_top\": len(proc_in_top),\n",
    "    \"labs_in_top\": len(labs_in_top),\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_path = SUBSET_DIR / f\"summary_{HADM_ID}.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ IN K·∫æT QU·∫¢\n",
    "# ==============================\n",
    "print(\"üìä T·ªîNG K·∫æT CHO L·∫¶N NH·∫¨P VI·ªÜN\", HADM_ID)\n",
    "print(f\"- T·ªïng s·ªë ch·∫©n ƒëo√°n: {summary['total_diagnoses']:,}  |  N·∫±m trong top: {summary['diagnoses_in_top']:,}\")\n",
    "print(f\"- T·ªïng s·ªë th·ªß thu·∫≠t: {summary['total_procedures']:,}  |  N·∫±m trong top: {summary['procedures_in_top']:,}\")\n",
    "print(f\"- T·ªïng s·ªë x√©t nghi·ªám: {summary['total_labs']:,}  |  N·∫±m trong top: {summary['labs_in_top']:,}\")\n",
    "print(f\"üìÇ ƒê√£ l∆∞u th·ªëng k√™ ‚Üí {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d0c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Name:  ___                     Unit No:   ___\n",
      " \n",
      "Admission Date:  ___              Discharge Date:   ___\n",
      " \n",
      "Date of Birth:  ___             Sex:   F\n",
      " \n",
      "Service: MEDICINE\n",
      " \n",
      "Allergies: \n",
      "No Known Allergies / Adverse Drug Reactions\n",
      " \n",
      "Attending: ___\n",
      " \n",
      "Chief Complaint:\n",
      "Worsening ABD distension and pain \n",
      " \n",
      "Major Surgical or Invasive Procedure:\n",
      "Paracentesis\n",
      "\n",
      " \n",
      "History of Present Illness:\n",
      "___ HCV cirrhosis c/b ascites, hiv on ART, h/o IVDU, COPD, \n",
      "bioplar, PTSD, presented from OSH ED with worsening abd \n",
      "distension over past week.  \n",
      "Pt reports self-discontinuing lasix and spirnolactone ___ weeks \n",
      "ago, because she feels like \"they don't do anything\" and that \n",
      "she \"doesn't want to put more chemicals in her.\" She does not \n",
      "follow Na-restricted diets. In the past week, she notes that she \n",
      "has been having worsening abd distension and discomfort. She \n",
      "denies ___ edema, or SOB, or orthopnea. She denies f/c/n/v, d/c, \n",
      "dysuria. She had food poisoning a week ago from eating stale \n",
      "cake (n/v 20 min after food ingestion), which resolved the same \n",
      "day. She denies other recent illness or sick contacts. She notes \n",
      "that she has been noticing gum bleeding while brushing her teeth \n",
      "in recent weeks. she denies easy bruising, melena, BRBPR, \n",
      "hemetesis, hemoptysis, or hematuria.  \n",
      "Because of her abd pain, she went to OSH ED and was transferred \n",
      "to ___ for further care. Per ED report, pt has brief period of \n",
      "confusion - she did not recall the ultrasound or bloodwork at \n",
      "osh. She denies recent drug use or alcohol use. She denies \n",
      "feeling confused, but reports that she is forgetful at times.  \n",
      "In the ED, initial vitals were 98.4 70 106/63 16 97%RA  \n",
      "Labs notable for ALT/AST/AP ___ ___: ___, \n",
      "Tbili1.6, WBC 5K, platelet 77, INR 1.6  \n",
      "\n",
      " \n",
      "Past Medical History:\n",
      "1. HCV Cirrhosis  \n",
      "2. No history of abnormal Pap smears.  \n",
      "3. She had calcification in her breast, which was removed  \n",
      "previously and per patient not, it was benign.  \n",
      "4. For HIV disease, she is being followed by Dr. ___ Dr.  \n",
      "___.  \n",
      "5. COPD  \n",
      "6. Past history of smoking.  \n",
      "7. She also had a skin lesion, which was biopsied and showed  \n",
      "skin cancer per patient report and is scheduled for a complete  \n",
      "removal of the skin lesion in ___ of this year.  \n",
      "8. She also had another lesion in her forehead with purple  \n",
      "discoloration. It was biopsied to exclude the possibility of  \n",
      "___'s sarcoma, the results is pending.  \n",
      "9. A 15 mm hypoechoic lesion on her ultrasound on ___  \n",
      "and is being monitored by an MRI.  \n",
      "10. History of dysplasia of anus in ___.  \n",
      "11. Bipolar affective disorder, currently manic, mild, and PTSD. \n",
      " \n",
      "12. History of cocaine and heroin use.  \n",
      "\n",
      " \n",
      "Social History:\n",
      "___\n",
      "Family History:\n",
      "She a total of five siblings, but she is not  \n",
      "talking to most of them. She only has one brother that she is in \n",
      " \n",
      "touch with and lives in ___. She is not aware of any  \n",
      "known GI or liver disease in her family.  \n",
      "Her last alcohol consumption was one drink two months ago. No  \n",
      "regular alcohol consumption. Last drug use ___ years ago. She  \n",
      "quit smoking a couple of years ago.  \n",
      "\n",
      " \n",
      "Physical Exam:\n",
      "VS: 98.1 107/61 78 18 97RA  \n",
      "General: in NAD  \n",
      "HEENT: CTAB, anicteric sclera, OP clear  \n",
      "Neck: supple, no LAD  \n",
      "CV: RRR,S1S2, no m/r/g  \n",
      "Lungs: CTAb, prolonged expiratory phase, no w/r/r  \n",
      "Abdomen: distended, mild diffuse tenderness, +flank dullness, \n",
      "cannot percuss liver/spleen edge ___ distension  \n",
      "GU: no foley  \n",
      "Ext: wwp, no c/e/e, + clubbing  \n",
      "Neuro: AAO3, converse normally, able to recall 3 times after 5 \n",
      "minutes, CN II-XII intact  \n",
      "\n",
      "Discharge:\n",
      "\n",
      "PHYSICAL EXAMINATION:  \n",
      "VS: 98 105/70 95\n",
      "General: in NAD  \n",
      "HEENT: anicteric sclera, OP clear  \n",
      "Neck: supple, no LAD  \n",
      "CV: RRR,S1S2, no m/r/g  \n",
      "Lungs: CTAb, prolonged expiratory phase, no w/r/r  \n",
      "Abdomen: distended but improved, TTP in RUQ, \n",
      "GU: no foley  \n",
      "Ext: wwp, no c/e/e, + clubbing  \n",
      "Neuro: AAO3,  CN II-XII intact  \n",
      "\n",
      " \n",
      "Pertinent Results:\n",
      "___ 10:25PM   GLUCOSE-109* UREA N-25* CREAT-0.3* SODIUM-138 \n",
      "POTASSIUM-3.4 CHLORIDE-105 TOTAL CO2-27 ANION GAP-9\n",
      "___ 10:25PM   estGFR-Using this\n",
      "___ 10:25PM   ALT(SGPT)-100* AST(SGOT)-114* ALK PHOS-114* \n",
      "TOT BILI-1.6*\n",
      "___ 10:25PM   LIPASE-77*\n",
      "___ 10:25PM   ALBUMIN-3.3*\n",
      "___ 10:25PM   WBC-5.0# RBC-4.29 HGB-14.3 HCT-42.6 MCV-99* \n",
      "MCH-33.3* MCHC-33.5 RDW-15.7*\n",
      "___ 10:25PM   NEUTS-70.3* LYMPHS-16.5* MONOS-8.1 EOS-4.2* \n",
      "BASOS-0.8\n",
      "___ 10:25PM   PLT COUNT-71*\n",
      "___ 10:25PM   ___ PTT-30.9 ___\n",
      "___ 10:25PM   ___\n",
      ".\n",
      "\n",
      "CXR: No acute cardiopulmonary process.  \n",
      "U/S:  \n",
      "1. Nodular appearance of the liver compatible with cirrhosis. \n",
      "Signs of portal  \n",
      "hypertension including small amount of ascites and splenomegaly. \n",
      " \n",
      "2. Cholelithiasis.  \n",
      "3. Patent portal veins with normal hepatopetal flow.  \n",
      "Diagnostic para attempted in the ED, unsuccessful.  \n",
      "On the floor, pt c/o abd distension and discomfort.\n",
      " \n",
      "Brief Hospital Course:\n",
      "___ HCV cirrhosis c/b ascites, hiv on ART, h/o IVDU, COPD, \n",
      "bioplar, PTSD, presented from OSH ED with worsening abd \n",
      "distension over past week and confusion.  \n",
      "\n",
      "# Ascites - p/w worsening abd distension and discomfort for last \n",
      "week. likely ___ portal HTN given underlying liver disease, \n",
      "though no ascitic fluid available on night of admission. No \n",
      "signs of heart failure noted on exam. This was ___ to med \n",
      "non-compliance and lack of diet restriction. SBP negative\n",
      "diuretics:  \n",
      "> Furosemide 40 mg PO DAILY  \n",
      "> Spironolactone 50 mg PO DAILY, chosen over the usual 100mg \n",
      "dose d/t K+ of 4.5.   \n",
      " CXR was wnl, UA negative, Urine culture blood culture negative. \n",
      " \n",
      "Pt was losing excess fluid appropriately with stable lytes on \n",
      "the above regimen. Pt was scheduled with current PCP for \n",
      "___ check upon discharge.   \n",
      "Pt was scheduled for new PCP with Dr. ___ at ___ and \n",
      "follow up in Liver clinic to schedule outpatient screening EGD \n",
      "and ___.   \n",
      " \n",
      "\n",
      " \n",
      "Medications on Admission:\n",
      "The Preadmission Medication list is accurate and complete.\n",
      "1. Furosemide 20 mg PO DAILY \n",
      "2. Spironolactone 50 mg PO DAILY \n",
      "3. Albuterol Inhaler 2 PUFF IH Q4H:PRN wheezing, SOB \n",
      "4. Raltegravir 400 mg PO BID \n",
      "5. Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY \n",
      "6. Nicotine Patch 14 mg TD DAILY \n",
      "7. Ipratropium Bromide Neb 1 NEB IH Q6H SOB \n",
      "\n",
      " \n",
      "Discharge Medications:\n",
      "1. Albuterol Inhaler 2 PUFF IH Q4H:PRN wheezing, SOB \n",
      "2. Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY \n",
      "3. Furosemide 40 mg PO DAILY \n",
      "RX *furosemide 40 mg 1 tablet(s) by mouth Daily Disp #*30 Tablet \n",
      "Refills:*3\n",
      "4. Ipratropium Bromide Neb 1 NEB IH Q6H SOB \n",
      "5. Nicotine Patch 14 mg TD DAILY \n",
      "6. Raltegravir 400 mg PO BID \n",
      "7. Spironolactone 50 mg PO DAILY \n",
      "8. Acetaminophen 500 mg PO Q6H:PRN pain \n",
      "\n",
      " \n",
      "Discharge Disposition:\n",
      "Home\n",
      " \n",
      "Discharge Diagnosis:\n",
      "Ascites from Portal HTN\n",
      "\n",
      " \n",
      "Discharge Condition:\n",
      "Mental Status: Clear and coherent.\n",
      "Level of Consciousness: Alert and interactive.\n",
      "Activity Status: Ambulatory - Independent.\n",
      "\n",
      " \n",
      "Discharge Instructions:\n",
      "Dear Ms. ___,\n",
      "It was a pleasure taking care of you! You came to us with \n",
      "stomach pain and worsening distension. While you were here we \n",
      "did a paracentesis to remove 1.5L of fluid from your belly. We \n",
      "also placed you on you 40 mg of Lasix and 50 mg of Aldactone to \n",
      "help you urinate the excess fluid still in your belly. As we \n",
      "discussed, everyone has a different dose of lasix required to \n",
      "make them urinate and it's likely that you weren't taking a high \n",
      "enough dose. Please take these medications daily to keep excess \n",
      "fluid off and eat a low salt diet. You will follow up with Dr. \n",
      "___ in liver clinic and from there have your colonoscopy \n",
      "and EGD scheduled. Of course, we are always here if you need us. \n",
      "We wish you all the best!\n",
      "Your ___ Team.  \n",
      " \n",
      "Followup Instructions:\n",
      "___\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "file_path = SUBSET_DIR / \"discharge_full.csv.gz\"\n",
    "\n",
    "# ƒê·ªçc 1 d√≤ng b·∫•t k·ª≥ (·ªü ƒë√¢y l√† d√≤ng ƒë·∫ßu ti√™n)\n",
    "sample = pd.read_csv(file_path, nrows=1)\n",
    "print(sample.iloc[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22bdebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü©∫ S·ªë ghi ch√∫: 87,951\n",
      "üìä Trung b√¨nh: 1,866 t·ª´/note\n",
      "üîπ Ng·∫Øn nh·∫•t: 44 t·ª´\n",
      "üî∏ D√†i nh·∫•t: 9,026 t·ª´\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "SUBSET_DIR = Path(\"../data/mimiciv_subset\")\n",
    "discharge_path = SUBSET_DIR / \"discharge_full.csv.gz\"\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc file v√† t√≠nh s·ªë t·ª´\n",
    "# ==========================\n",
    "df = pd.read_csv(discharge_path, usecols=[\"text\"])\n",
    "\n",
    "# Lo·∫°i b·ªè NaN & t√≠nh s·ªë t·ª´ trong m·ªói ghi ch√∫\n",
    "df[\"word_count\"] = df[\"text\"].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "\n",
    "avg_words = df[\"word_count\"].mean()\n",
    "max_words = df[\"word_count\"].max()\n",
    "min_words = df[\"word_count\"].min()\n",
    "\n",
    "print(f\"ü©∫ S·ªë ghi ch√∫: {len(df):,}\")\n",
    "print(f\"üìä Trung b√¨nh: {avg_words:,.0f} t·ª´/note\")\n",
    "print(f\"üîπ Ng·∫Øn nh·∫•t: {min_words:,} t·ª´\")\n",
    "print(f\"üî∏ D√†i nh·∫•t: {max_words:,} t·ª´\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722a8b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u 87,544 note ƒë√£ g·ªôp 6 m·ª•c ‚Üí ../data/mimiciv_subset/discharge.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n\n",
    "# ==========================\n",
    "DATA_ROOT  = Path(\"../data\")\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "NOTE_DIR   = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"  # ch·ªâ d√πng khi fallback\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ngu·ªìn note ∆∞u ti√™n: file full ƒë√£ gom v·ªÅ subset\n",
    "note_path = SUBSET_DIR / \"discharge_full.csv.gz\"\n",
    "if not note_path.exists():\n",
    "    # fallback n·∫øu ch∆∞a c√≥ file subset\n",
    "    note_path = NOTE_DIR / \"discharge.csv.gz\"\n",
    "\n",
    "adm_path  = SUBSET_DIR / \"admissions.csv\"   # danh s√°ch hadm trong subset\n",
    "out_path  = SUBSET_DIR / \"discharge.csv.gz\" # OUTPUT\n",
    "\n",
    "CHUNKSIZE = 200_000\n",
    "\n",
    "# ==========================\n",
    "# 1) T·∫≠p HADM c·∫ßn l·∫•y\n",
    "# ==========================\n",
    "admissions = pd.read_csv(adm_path, usecols=[\"hadm_id\"])\n",
    "HADM_SET = set(admissions[\"hadm_id\"].astype(int))\n",
    "\n",
    "# ==========================\n",
    "# 2) Regex & h√†m tr√≠ch xu·∫•t 6 m·ª•c\n",
    "# ==========================\n",
    "# Headings c·∫ßn gi·ªØ (key: label chu·∫©n, value: pattern ƒë·ªÉ match ti√™u ƒë·ªÅ)\n",
    "HEADINGS = {\n",
    "    \"Chief Complaint\": r\"Chief Complaint\",\n",
    "    \"Major Surgical or Invasive Procedure\": r\"Major Surgical\\s+or\\s+Invasive Procedure\",\n",
    "    \"History of Present Illness\": r\"History of Present Illness\",\n",
    "    \"Past Medical History\": r\"Past Medical History\",\n",
    "    # Physical Exam c√≥ th·ªÉ xu·∫•t hi·ªán ·ªü d·∫°ng \"Physical Exam\" ho·∫∑c \"PHYSICAL EXAMINATION\"\n",
    "    \"Physical Exam\": r\"(?:Physical Exam|PHYSICAL EXAMINATION)\",\n",
    "    \"Pertinent Results\": r\"Pertinent Results\",\n",
    "}\n",
    "\n",
    "# M·∫´u ƒë·ªÉ x√°c ƒë·ªãnh ranh gi·ªõi c√°c heading n√≥i chung (b·∫Øt ƒë·∫ßu d√≤ng, c·ª•m ch·ªØ c√°i + c√≥ th·ªÉ kho·∫£ng tr·∫Øng r·ªìi d·∫•u :)\n",
    "GEN_HEADING = re.compile(r\"^[A-Z][A-Za-z/ ()'#\\.\\-]*:\\s*$\", flags=re.M)\n",
    "\n",
    "def extract_selected_sections(text: str) -> str:\n",
    "    \"\"\"Tr√≠ch 6 m·ª•c v√† g·ªôp l·∫°i th√†nh 1 text.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    # Chu·∫©n ho√° xu·ªëng d·∫°ng c√≥ d√≤ng tr·ªëng ph√¢n c√°ch nh·∫π\n",
    "    s = re.sub(r\"\\r\\n?\", \"\\n\", text)\n",
    "\n",
    "    # T√¨m t·∫•t c·∫£ heading v·ªã tr√≠\n",
    "    # Chi·∫øn l∆∞·ª£c: t√¨m v·ªã tr√≠ c·ªßa t·ª´ng heading c·ª• th·ªÉ c·∫ßn gi·ªØ; c·∫Øt ƒë·∫øn heading k·∫ø ti·∫øp b·∫•t k·ª≥ (GEN_HEADING)\n",
    "    pieces = []\n",
    "    for label, pat in HEADINGS.items():\n",
    "        m = re.search(rf\"(?mi)^\\s*{pat}\\s*:\\s*$\", s)\n",
    "        if not m:\n",
    "            continue\n",
    "        start = m.end()  # b·∫Øt ƒë·∫ßu n·ªôi dung sau ti√™u ƒë·ªÅ n√†y\n",
    "\n",
    "        # T√¨m heading k·∫ø ti·∫øp b·∫•t k·ª≥\n",
    "        next_m = GEN_HEADING.search(s, pos=start)\n",
    "        end = next_m.start() if next_m else len(s)\n",
    "\n",
    "        body = s[start:end].strip()\n",
    "        if body:\n",
    "            pieces.append(f\"{label}:\\n{body}\")\n",
    "\n",
    "    return \"\\n\\n\".join(pieces).strip()\n",
    "\n",
    "# ==========================\n",
    "# 3) ƒê·ªçc note theo chunk, l·ªçc HADM, tr√≠ch & g·ªôp text\n",
    "# ==========================\n",
    "rows = []\n",
    "usecols = [\"hadm_id\", \"charttime\", \"text\"]\n",
    "\n",
    "for chunk in pd.read_csv(note_path, usecols=usecols, compression=\"gzip\", chunksize=CHUNKSIZE):\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\", \"text\"]).copy()\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    chunk[\"hadm_id\"]  = chunk[\"hadm_id\"].astype(int)\n",
    "    chunk = chunk[chunk[\"hadm_id\"].isin(HADM_SET)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Gi·ªØ record m·ªõi nh·∫•t theo hadm_id n·∫øu c√≥ tr√πng (sort desc charttime, drop_duplicates keep first)\n",
    "    if \"charttime\" in chunk.columns:\n",
    "        # parse v·ª´a ƒë·ªß: ch·ªâ v·ªõi chunk n√†y ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√∫ng sort\n",
    "        chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"], errors=\"coerce\")\n",
    "        chunk = (\n",
    "            chunk.sort_values([\"hadm_id\", \"charttime\"], ascending=[True, False])\n",
    "                 .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "        )\n",
    "\n",
    "    # Tr√≠ch 6 m·ª•c v√† g·ªôp text\n",
    "    chunk[\"text\"] = chunk[\"text\"].astype(str).map(extract_selected_sections)\n",
    "\n",
    "    # Ch·ªâ gi·ªØ c√°c b·∫£n c√≥ text sau khi tr√≠ch (kh√¥ng r·ªóng)\n",
    "    chunk = chunk[chunk[\"text\"].str.len() > 0]\n",
    "\n",
    "    if not chunk.empty:\n",
    "        rows.append(chunk[[\"hadm_id\", \"charttime\", \"text\"]])\n",
    "\n",
    "# ==========================\n",
    "# 4) G·ªôp & l∆∞u output\n",
    "# ==========================\n",
    "if rows:\n",
    "    out_df = pd.concat(rows, ignore_index=True)\n",
    "    # N·∫øu c√≥ tr√πng hadm gi·ªØa c√°c chunk (hi·∫øm), gi·ªØ b·∫£n m·ªõi nh·∫•t\n",
    "    if \"charttime\" in out_df.columns:\n",
    "        out_df = (\n",
    "            out_df.sort_values([\"hadm_id\", \"charttime\"], ascending=[True, False])\n",
    "                  .drop_duplicates(subset=[\"hadm_id\"], keep=\"first\")\n",
    "                  .sort_values(\"hadm_id\")\n",
    "        )\n",
    "else:\n",
    "    out_df = pd.DataFrame(columns=[\"hadm_id\", \"charttime\", \"text\"])\n",
    "\n",
    "out_df.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u {len(out_df):,} note ƒë√£ g·ªôp 6 m·ª•c ‚Üí {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d6d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadm_id: 24151113\n",
      "charttime: 2153-02-07\n",
      "\n",
      "--- TEXT ---\n",
      "\n",
      "Chief Complaint:\n",
      "Febrile neutropenia, bacteremia\n",
      "\n",
      "Major Surgical or Invasive Procedure:\n",
      "None\n",
      "\n",
      "History of Present Illness:\n",
      "Ms. ___ is a ___ year-old female with an unclear \n",
      "neurologic condition (currently being evaluated at ___, \n",
      "hepatitis C s/p 12 weeks of Harvoni now with a negative VL, and \n",
      "MDS diagnosed in ___, now transfusion-dependent, referred to \n",
      "___ for consideration of reduced intensity allogeneic stem \n",
      "cell transplantation, presented to OSH with febrile neutropenia \n",
      "and coag-neg staph bacteremia, and transferred here for further \n",
      "care. \n",
      "\n",
      "On ___, she developed fevers, chills, and weakness \n",
      "without any localizing symptoms (no cough, abdominal pain, \n",
      "dysuria, headaches, chest pain, rashes). She developed a fever \n",
      "to 102.1, called her physician, who encouraged her to present \n",
      "emergently to the ED. In OSH ED, blood cultures were drawn, and \n",
      "there was concern for UTI so zosyn was started. Blood cx grew \n",
      "GPCs, so zosyn was switched to vancomycin. There was concern for \n",
      "infection of her right POC vs left titanium rod, so the right \n",
      "port was removed on ___ and a PICC was placed. During her \n",
      "hospital course at OSH, she developed left abdominal pain \n",
      "concerning for colitis, diarrhea concerning for cdiff, \n",
      "bronchitis and cough concerning for pneumonia. Cultures were \n",
      "consistently positive for CoNS, and she was treated with \n",
      "vancomycin, levofloxacin, and flagyl. She has been afebrile for \n",
      "the past several days. She was transferred to ___ unit for \n",
      "further management. \n",
      "\n",
      "Upon arrival to the floor, she endorses pain in her right middle \n",
      "finger, which she says was caused by a fingerstick glucose at \n",
      "OSH. She denies abdominal pain, trouble breathing, chest pain, \n",
      "headaches, rashes, night sweats, and chills. She is not in any \n",
      "pain and does not need pain medications. \n",
      "\n",
      "She does endorse a cough. She has a history of whooping cough \n",
      "for 3 months (for which she was reportedly quarantined by CDC).\n",
      "\n",
      "Pertinent Results:\n",
      "___ 07:00PM   GLUCOSE-246* UREA N-6 CREAT-0.5 SODIUM-127* \n",
      "POTASSIUM-4.0 CHLORIDE-90* TOTAL CO2-27 ANION GAP-14\n",
      "___ 07:00PM   estGFR-Using this\n",
      "___ 07:00PM   ALT(SGPT)-31 AST(SGOT)-26 LD(LDH)-165 ALK \n",
      "PHOS-51 TOT BILI-0.5\n",
      "___ 07:00PM   ALBUMIN-2.8* CALCIUM-8.1* PHOSPHATE-4.5 \n",
      "MAGNESIUM-1.8\n",
      "___ 07:00PM   WBC-1.4* RBC-2.62* HGB-7.6* HCT-22.5* MCV-86 \n",
      "MCH-29.0 MCHC-33.8 RDW-13.7 RDWSD-42.8\n",
      "___ 07:00PM   NEUTS-55 BANDS-3 ___ MONOS-6 EOS-0 \n",
      "BASOS-1 ATYPS-2* METAS-2* MYELOS-3* PROMYELO-1* AbsNeut-0.81* \n",
      "AbsLymp-0.41* AbsMono-0.08* AbsEos-0.00* AbsBaso-0.01\n",
      "___ 07:00PM   HYPOCHROM-OCCASIONAL ANISOCYT-NORMAL \n",
      "POIKILOCY-OCCASIONAL MACROCYT-NORMAL MICROCYT-NORMAL \n",
      "POLYCHROM-NORMAL OVALOCYT-OCCASIONAL\n",
      "___ 07:00PM   PLT SMR-RARE PLT COUNT-11*\n",
      "___ 07:00PM   ___ PTT-33.8 ___\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "SUBSET_DIR = Path(\"../data/mimiciv_subset\")\n",
    "path = SUBSET_DIR / \"discharge.csv.gz\"\n",
    "\n",
    "rng = np.random.default_rng(1)  # t√°i l·∫≠p\n",
    "sample = None\n",
    "seen = 0\n",
    "\n",
    "usecols = [\"hadm_id\", \"charttime\", \"text\"]\n",
    "\n",
    "for chunk in pd.read_csv(path, compression=\"gzip\", usecols=usecols, chunksize=100_000):\n",
    "    # lo·∫°i NaN ƒë·ªÉ tr√°nh l·ªói in ·∫•n\n",
    "    chunk[\"text\"] = chunk[\"text\"].fillna(\"\")\n",
    "    for _, row in chunk.iterrows():\n",
    "        seen += 1\n",
    "        if rng.random() < 1.0 / seen:  # reservoir sampling\n",
    "            sample = row\n",
    "\n",
    "if sample is None:\n",
    "    print(\"File r·ªóng?\")\n",
    "else:\n",
    "    print(\"hadm_id:\", sample[\"hadm_id\"])\n",
    "    print(\"charttime:\", sample[\"charttime\"])\n",
    "    print(\"\\n--- TEXT ---\\n\")\n",
    "    # in nguy√™n vƒÉn to√†n b·ªô ghi ch√∫\n",
    "    print(str(sample[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14328a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>chief_complaint</th>\n",
       "      <th>major_procedures</th>\n",
       "      <th>hpi</th>\n",
       "      <th>pmh</th>\n",
       "      <th>physical_exam</th>\n",
       "      <th>pertinent_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000147</td>\n",
       "      <td>2121-09-03</td>\n",
       "      <td>chest pain \\n \\nMajor Surgical or Invasive Pro...</td>\n",
       "      <td>___ Cardiac catheterization \\n___ Emergent cor...</td>\n",
       "      <td>___ year old male with a history of\\ndiabetes ...</td>\n",
       "      <td>Hypercholesterolemia \\nChronic back pain \\nHyp...</td>\n",
       "      <td>Pulse:86 Resp:27  O2 sat: 99/RA\\nB/P: 128/79\\n...</td>\n",
       "      <td>___ 04:30PM BLOOD WBC-6.9 RBC-4.28* Hgb-13.1* ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hadm_id   charttime                                    chief_complaint  \\\n",
       "0  20000147  2121-09-03  chest pain \\n \\nMajor Surgical or Invasive Pro...   \n",
       "\n",
       "                                    major_procedures  \\\n",
       "0  ___ Cardiac catheterization \\n___ Emergent cor...   \n",
       "\n",
       "                                                 hpi  \\\n",
       "0  ___ year old male with a history of\\ndiabetes ...   \n",
       "\n",
       "                                                 pmh  \\\n",
       "0  Hypercholesterolemia \\nChronic back pain \\nHyp...   \n",
       "\n",
       "                                       physical_exam  \\\n",
       "0  Pulse:86 Resp:27  O2 sat: 99/RA\\nB/P: 128/79\\n...   \n",
       "\n",
       "                                   pertinent_results  \n",
       "0  ___ 04:30PM BLOOD WBC-6.9 RBC-4.28* Hgb-13.1* ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SUBSET_DIR = Path(\"../data/mimiciv_subset\")\n",
    "path = SUBSET_DIR / \"discharge.csv.gz\"\n",
    "pd.read_csv(path, compression=\"gzip\", nrows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951b45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü©∫ S·ªë ghi ch√∫: 87,544\n",
      "üìä Trung b√¨nh: 482 t·ª´/note\n",
      "üîπ Ng·∫Øn nh·∫•t: 5 t·ª´\n",
      "üî∏ D√†i nh·∫•t: 4,267 t·ª´\n",
      "‚úÖ S·ªë ghi ch√∫ < 512 t·ª´: 56,296 (~64.3%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "SUBSET_DIR = Path(\"../data/mimiciv_subset\")\n",
    "discharge_path = SUBSET_DIR / \"discharge.csv.gz\"\n",
    "\n",
    "# ==========================\n",
    "# ƒê·ªçc file v√† t√≠nh s·ªë t·ª´\n",
    "# ==========================\n",
    "df = pd.read_csv(discharge_path, usecols=[\"hadm_id\", \"text\"])\n",
    "\n",
    "# Lo·∫°i b·ªè NaN & t√≠nh s·ªë t·ª´ trong m·ªói ghi ch√∫\n",
    "df[\"word_count\"] = df[\"text\"].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "\n",
    "avg_words = df[\"word_count\"].mean()\n",
    "max_words = df[\"word_count\"].max()\n",
    "min_words = df[\"word_count\"].min()\n",
    "\n",
    "# Ghi ch√∫ < 512 t·ª´\n",
    "threshold = 512\n",
    "short_notes = df[df[\"word_count\"] < threshold]\n",
    "\n",
    "print(f\"ü©∫ S·ªë ghi ch√∫: {len(df):,}\")\n",
    "print(f\"üìä Trung b√¨nh: {avg_words:,.0f} t·ª´/note\")\n",
    "print(f\"üîπ Ng·∫Øn nh·∫•t: {min_words:,} t·ª´\")\n",
    "print(f\"üî∏ D√†i nh·∫•t: {max_words:,} t·ª´\")\n",
    "print(f\"‚úÖ S·ªë ghi ch√∫ < {threshold} t·ª´: {len(short_notes):,} \"\n",
    "      f\"(~{len(short_notes)/len(df):.1%})\")\n",
    "\n",
    "# (Tu·ª≥ ch·ªçn) L∆∞u danh s√°ch c√°c HADM c√≥ note ng·∫Øn\n",
    "# short_notes[[\"hadm_id\", \"word_count\"]].to_csv(SUBSET_DIR / \"discharge_short_under_512.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa2799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë HADM c√≥ note < 512 t·ª´: 56,296\n",
      "ƒê√£ ghi ƒë√® admissions.csv ‚Äî c√≤n 56,296/87,951 l∆∞·ª£t nh·∫≠p vi·ªán (note < 512 t·ª´).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# C·∫•u h√¨nh\n",
    "# ==========================\n",
    "SUBSET_DIR = Path(\"../data/mimiciv_subset\")\n",
    "adm_path = SUBSET_DIR / \"admissions.csv\"          # admissions hi·ªán t·∫°i (CSV)\n",
    "dis_path = SUBSET_DIR / \"discharge.csv.gz\"        # ghi ch√∫ ƒë√£ chu·∫©n b·ªã\n",
    "\n",
    "THRESH = 512  # s·ªë t·ª´ t·ªëi ƒëa cho ghi ch√∫\n",
    "\n",
    "# ==========================\n",
    "# 1) L·∫•y danh s√°ch hadm_id c√≥ note < 512 t·ª´\n",
    "# ==========================\n",
    "dis = pd.read_csv(dis_path, usecols=[\"hadm_id\", \"text\"], compression=\"gzip\")\n",
    "dis[\"hadm_id\"] = dis[\"hadm_id\"].astype(int)\n",
    "dis[\"word_count\"] = dis[\"text\"].fillna(\"\").map(lambda s: len(str(s).split()))\n",
    "\n",
    "hadm_under = set(dis.loc[dis[\"word_count\"] < THRESH, \"hadm_id\"].tolist())\n",
    "\n",
    "print(f\"S·ªë HADM c√≥ note < {THRESH} t·ª´: {len(hadm_under):,}\")\n",
    "\n",
    "# ==========================\n",
    "# 2) ƒê·ªçc admissions hi·ªán t·∫°i v√† l·ªçc theo danh s√°ch tr√™n\n",
    "# ==========================\n",
    "adm = pd.read_csv(adm_path)\n",
    "before = len(adm)\n",
    "adm[\"hadm_id\"] = adm[\"hadm_id\"].astype(int)\n",
    "\n",
    "adm_filtered = adm[adm[\"hadm_id\"].isin(hadm_under)].copy()\n",
    "after = len(adm_filtered)\n",
    "\n",
    "# (Tu·ª≥ ch·ªçn) Sao l∆∞u tr∆∞·ªõc khi ghi ƒë√®\n",
    "# adm.to_csv(SUBSET_DIR / \"admissions.backup.csv\", index=False)\n",
    "\n",
    "# ==========================\n",
    "# 3) Ghi ƒë√® admissions.csv\n",
    "# ==========================\n",
    "adm_filtered.to_csv(adm_path, index=False)\n",
    "print(f\"ƒê√£ ghi ƒë√® admissions.csv ‚Äî c√≤n {after:,}/{before:,} l∆∞·ª£t nh·∫≠p vi·ªán (note < {THRESH} t·ª´).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee1fa69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Diagnoses: 33 d√≤ng ‚Üí ../data/mimiciv_subset/24151113_diagnoses.csv\n",
      "‚úÖ Procedures: 5 d√≤ng ‚Üí ../data/mimiciv_subset/24151113_procedures.csv\n",
      "‚úÖ Labs: 127 d√≤ng ‚Üí ../data/mimiciv_subset/24151113_labs.csv\n",
      "\n",
      "--- SUMMARY ---\n",
      "Ch·∫©n ƒëo√°n (ICD): 33\n",
      "Th·ªß thu·∫≠t (ICD): 5\n",
      "X√©t nghi·ªám (itemid): 127 (unique)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n =====\n",
    "DATA_ROOT = Path(\"../data\")\n",
    "HOSP_DIR = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "NOTE_DIR = DATA_ROOT / \"mimic-iv-note\" / \"2.2\" / \"note\"  # kh√¥ng d√πng ·ªü ƒë√¢y nh∆∞ng ƒë·ªÉ tham chi·∫øu\n",
    "SUBSET_DIR = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HADM = 24151113\n",
    "\n",
    "# ===== 1) CH·∫®N ƒêO√ÅN (Diagnoses) =====\n",
    "diag_path = HOSP_DIR / \"diagnoses_icd.csv.gz\"\n",
    "d_diag_path = HOSP_DIR / \"d_icd_diagnoses.csv.gz\"\n",
    "\n",
    "# ƒê·ªçc dictionary t√™n ch·∫©n ƒëo√°n\n",
    "d_diag = pd.read_csv(d_diag_path, usecols=[\"icd_code\", \"icd_version\", \"long_title\"], compression=\"gzip\")\n",
    "d_diag[\"icd_code\"] = d_diag[\"icd_code\"].astype(str)\n",
    "\n",
    "# L·ªçc theo hadm_id b·∫±ng chunks ƒë·ªÉ ti·∫øt ki·ªám RAM\n",
    "diag_rows = []\n",
    "for chunk in pd.read_csv(diag_path,\n",
    "                         usecols=[\"subject_id\", \"hadm_id\", \"icd_code\", \"icd_version\"],\n",
    "                         compression=\"gzip\",\n",
    "                         chunksize=200_000):\n",
    "    chunk = chunk[chunk[\"hadm_id\"].astype(int) == HADM]\n",
    "    if not chunk.empty:\n",
    "        chunk[\"icd_code\"] = chunk[\"icd_code\"].astype(str)\n",
    "        diag_rows.append(chunk)\n",
    "\n",
    "if diag_rows:\n",
    "    dx = pd.concat(diag_rows, ignore_index=True).dropna(subset=[\"icd_code\"])\n",
    "    dx = dx.merge(d_diag, on=[\"icd_code\", \"icd_version\"], how=\"left\")\n",
    "    dx = dx[[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\",\"long_title\"]].drop_duplicates()\n",
    "else:\n",
    "    dx = pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\",\"long_title\"])\n",
    "\n",
    "out_dx = SUBSET_DIR / f\"{HADM}_diagnoses.csv\"\n",
    "dx.to_csv(out_dx, index=False)\n",
    "print(f\"‚úÖ Diagnoses: {len(dx)} d√≤ng ‚Üí {out_dx}\")\n",
    "\n",
    "# ===== 2) TH·ª¶ THU·∫¨T (Procedures) =====\n",
    "proc_path = HOSP_DIR / \"procedures_icd.csv.gz\"\n",
    "d_proc_path = HOSP_DIR / \"d_icd_procedures.csv.gz\"\n",
    "\n",
    "# ƒê·ªçc dictionary t√™n th·ªß thu·∫≠t\n",
    "d_proc = pd.read_csv(d_proc_path, usecols=[\"icd_code\", \"icd_version\", \"long_title\"], compression=\"gzip\")\n",
    "d_proc[\"icd_code\"] = d_proc[\"icd_code\"].astype(str)\n",
    "\n",
    "proc_rows = []\n",
    "for chunk in pd.read_csv(proc_path,\n",
    "                         usecols=[\"subject_id\", \"hadm_id\", \"icd_code\", \"icd_version\"],\n",
    "                         compression=\"gzip\",\n",
    "                         chunksize=200_000):\n",
    "    chunk = chunk[chunk[\"hadm_id\"].astype(int) == HADM]\n",
    "    if not chunk.empty:\n",
    "        chunk[\"icd_code\"] = chunk[\"icd_code\"].astype(str)\n",
    "        proc_rows.append(chunk)\n",
    "\n",
    "if proc_rows:\n",
    "    px = pd.concat(proc_rows, ignore_index=True).dropna(subset=[\"icd_code\"])\n",
    "    px = px.merge(d_proc, on=[\"icd_code\", \"icd_version\"], how=\"left\")\n",
    "    px = px[[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\",\"long_title\"]].drop_duplicates()\n",
    "else:\n",
    "    px = pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\",\"long_title\"])\n",
    "\n",
    "out_px = SUBSET_DIR / f\"{HADM}_procedures.csv\"\n",
    "px.to_csv(out_px, index=False)\n",
    "print(f\"‚úÖ Procedures: {len(px)} d√≤ng ‚Üí {out_px}\")\n",
    "\n",
    "# ===== 3) X√âT NGHI·ªÜM (Labs) =====\n",
    "lab_path = HOSP_DIR / \"labevents.csv.gz\"\n",
    "dlab_path = HOSP_DIR / \"d_labitems.csv.gz\"\n",
    "\n",
    "# T·ª´ ƒëi·ªÉn itemid -> label\n",
    "dlab = pd.read_csv(dlab_path, usecols=[\"itemid\",\"label\"], compression=\"gzip\")\n",
    "dlab[\"itemid\"] = dlab[\"itemid\"].astype(int)\n",
    "\n",
    "lab_rows = []\n",
    "for chunk in pd.read_csv(lab_path,\n",
    "                         usecols=[\"hadm_id\",\"itemid\"],\n",
    "                         compression=\"gzip\",\n",
    "                         chunksize=400_000):\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\",\"itemid\"])\n",
    "    chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(int)\n",
    "    if HADM not in set(chunk[\"hadm_id\"].unique()):\n",
    "        continue\n",
    "    sub = chunk[chunk[\"hadm_id\"] == HADM].copy()\n",
    "    if not sub.empty:\n",
    "        sub[\"itemid\"] = sub[\"itemid\"].astype(int)\n",
    "        lab_rows.append(sub[[\"hadm_id\",\"itemid\"]])\n",
    "\n",
    "if lab_rows:\n",
    "    labs = pd.concat(lab_rows, ignore_index=True)\n",
    "    # L·∫•y danh s√°ch x√©t nghi·ªám (unique) v√† t√™n\n",
    "    labs = labs.drop_duplicates().merge(dlab, on=\"itemid\", how=\"left\")\n",
    "    labs = labs[[\"hadm_id\",\"itemid\",\"label\"]].drop_duplicates().sort_values([\"label\",\"itemid\"])\n",
    "else:\n",
    "    labs = pd.DataFrame(columns=[\"hadm_id\",\"itemid\",\"label\"])\n",
    "\n",
    "out_lab = SUBSET_DIR / f\"{HADM}_labs.csv\"\n",
    "labs.to_csv(out_lab, index=False)\n",
    "print(f\"‚úÖ Labs: {len(labs)} d√≤ng ‚Üí {out_lab}\")\n",
    "\n",
    "# ===== T√≥m t·∫Øt nhanh tr√™n m√†n h√¨nh =====\n",
    "print(\"\\n--- SUMMARY ---\")\n",
    "print(f\"Ch·∫©n ƒëo√°n (ICD): {len(dx)}\")\n",
    "print(f\"Th·ªß thu·∫≠t (ICD): {len(px)}\")\n",
    "print(f\"X√©t nghi·ªám (itemid): {len(labs)} (unique)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6151b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ n·∫°p vocab top: ICD=50, PROC=50, LAB=50\n",
      "\n",
      "=== T√ìM T·∫ÆT ===\n",
      "- Ch·∫©n ƒëo√°n (trong top): 4 h√†ng ‚Üí hadm_24151113_diagnoses_top.csv\n",
      "- Th·ªß thu·∫≠t   (trong top): 3 h√†ng ‚Üí hadm_24151113_procedures_top.csv\n",
      "- X√©t nghi·ªám  (trong top): 49 h√†ng ‚Üí hadm_24151113_labs_top.csv\n",
      "\n",
      "V√≠ d·ª• 3 d√≤ng ƒë·∫ßu m·ªói b·∫£ng:\n",
      "   subject_id   hadm_id icd_code  icd_version  \\\n",
      "0    14471034  24151113     N179           10   \n",
      "1    14471034  24151113     E871           10   \n",
      "2    14471034  24151113      Z66           10   \n",
      "\n",
      "                          long_title  \n",
      "0  Acute kidney failure, unspecified  \n",
      "1   Hypo-osmolality and hyponatremia  \n",
      "2                 Do not resuscitate  \n",
      "   subject_id   hadm_id icd_code  icd_version  \\\n",
      "0    14471034  24151113  5A1945Z           10   \n",
      "1    14471034  24151113  0BH17EZ           10   \n",
      "2    14471034  24151113  02HV33Z           10   \n",
      "\n",
      "                                          long_title  \n",
      "0   Respiratory Ventilation, 24-96 Consecutive Hours  \n",
      "1  Insertion of Endotracheal Airway into Trachea,...  \n",
      "2  Insertion of Infusion Device into Superior Ven...  \n",
      "      hadm_id  itemid                           label\n",
      "0  24151113.0   50861  Alanine Aminotransferase (ALT)\n",
      "1  24151113.0   50862                         Albumin\n",
      "2  24151113.0   50863            Alkaline Phosphatase\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= C·∫•u h√¨nh =========\n",
    "DATA_ROOT   = Path(\"../data\")\n",
    "HOSP_DIR    = DATA_ROOT / \"mimiciv\" / \"3.1\" / \"hosp\"\n",
    "SUBSET_DIR  = DATA_ROOT / \"mimiciv_subset\"\n",
    "SUBSET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HADM_ID = 24151113   # <-- ƒê·ªîI hadm_id b·∫°n mu·ªën l·∫•y ·ªü ƒë√¢y\n",
    "\n",
    "# ========= Ti·ªán √≠ch: ƒë·ªçc top-list (h·ªó tr·ª£ 2 ki·ªÉu t√™n file) =========\n",
    "def load_top_codes(subdir: Path, candidates: list[str], col: str, as_type=\"str\"):\n",
    "    for name in candidates:\n",
    "        p = subdir / name\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p)\n",
    "            vals = df[col]\n",
    "            vals = vals.astype(int) if as_type == \"int\" else vals.astype(str)\n",
    "            return set(vals.tolist())\n",
    "    raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file top codes trong {subdir} v·ªõi c√°c t√™n: {candidates}\")\n",
    "\n",
    "top_icd = load_top_codes(\n",
    "    SUBSET_DIR,\n",
    "    candidates=[\"top_50_icd.csv\", \"icd.csv\"],\n",
    "    col=\"icd_code\",\n",
    "    as_type=\"str\",\n",
    ")\n",
    "\n",
    "top_proc = load_top_codes(\n",
    "    SUBSET_DIR,\n",
    "    candidates=[\"top_50_procedures.csv\", \"procedures.csv\"],\n",
    "    col=\"proc_code\",   # l∆∞u √Ω c·ªôt trong file top th·ªß thu·∫≠t l√† 'proc_code'\n",
    "    as_type=\"str\",\n",
    ")\n",
    "\n",
    "top_lab = load_top_codes(\n",
    "    SUBSET_DIR,\n",
    "    candidates=[\"top_50_labs.csv\", \"labs.csv\"],\n",
    "    col=\"itemid\",\n",
    "    as_type=\"int\",\n",
    ")\n",
    "\n",
    "print(f\"ƒê√£ n·∫°p vocab top: ICD={len(top_icd)}, PROC={len(top_proc)}, LAB={len(top_lab)}\")\n",
    "\n",
    "# ========= 1) CH·∫®N ƒêO√ÅN (ICD) =========\n",
    "diag_rows = []\n",
    "for chunk in pd.read_csv(\n",
    "    HOSP_DIR / \"diagnoses_icd.csv.gz\",\n",
    "    usecols=[\"subject_id\", \"hadm_id\", \"icd_code\", \"icd_version\"],\n",
    "    compression=\"gzip\",\n",
    "    chunksize=200_000\n",
    "):\n",
    "    chunk = chunk[chunk[\"hadm_id\"] == HADM_ID]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk[\"icd_code\"] = chunk[\"icd_code\"].astype(str)\n",
    "    keep = chunk[chunk[\"icd_code\"].isin(top_icd)]\n",
    "    if not keep.empty:\n",
    "        diag_rows.append(keep)\n",
    "\n",
    "if diag_rows:\n",
    "    dx = pd.concat(diag_rows, ignore_index=True).drop_duplicates()\n",
    "    # G·∫Øn long_title t·ª´ d_icd_diagnoses (theo phi√™n b·∫£n)\n",
    "    dicd = pd.read_csv(\n",
    "        HOSP_DIR / \"d_icd_diagnoses.csv.gz\",\n",
    "        usecols=[\"icd_code\", \"icd_version\", \"long_title\"],\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    dx = dx.merge(dicd, on=[\"icd_code\", \"icd_version\"], how=\"left\")\n",
    "else:\n",
    "    dx = pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\",\"long_title\"])\n",
    "\n",
    "# ========= 2) TH·ª¶ THU·∫¨T (Procedures) =========\n",
    "proc_rows = []\n",
    "for chunk in pd.read_csv(\n",
    "    HOSP_DIR / \"procedures_icd.csv.gz\",\n",
    "    usecols=[\"subject_id\", \"hadm_id\", \"icd_code\", \"icd_version\"],\n",
    "    compression=\"gzip\",\n",
    "    chunksize=200_000\n",
    "):\n",
    "    chunk = chunk[chunk[\"hadm_id\"] == HADM_ID]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk[\"icd_code\"] = chunk[\"icd_code\"].astype(str)\n",
    "    keep = chunk[chunk[\"icd_code\"].isin(top_proc)]\n",
    "    if not keep.empty:\n",
    "        proc_rows.append(keep)\n",
    "\n",
    "if proc_rows:\n",
    "    pr = pd.concat(proc_rows, ignore_index=True).drop_duplicates()\n",
    "    dicp = pd.read_csv(\n",
    "        HOSP_DIR / \"d_icd_procedures.csv.gz\",\n",
    "        usecols=[\"icd_code\", \"icd_version\", \"long_title\"],\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    pr = pr.merge(dicp, on=[\"icd_code\", \"icd_version\"], how=\"left\")\n",
    "else:\n",
    "    pr = pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\",\"long_title\"])\n",
    "\n",
    "# ========= 3) X√âT NGHI·ªÜM (Labs) =========\n",
    "lab_rows = []\n",
    "for chunk in pd.read_csv(\n",
    "    HOSP_DIR / \"labevents.csv.gz\",\n",
    "    usecols=[\"hadm_id\",\"itemid\"],\n",
    "    compression=\"gzip\",\n",
    "    chunksize=200_000\n",
    "):\n",
    "    chunk = chunk[chunk[\"hadm_id\"] == HADM_ID]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk[\"itemid\"] = chunk[\"itemid\"].astype(int)\n",
    "    keep = chunk[chunk[\"itemid\"].isin(top_lab)]\n",
    "    if not keep.empty:\n",
    "        lab_rows.append(keep)\n",
    "\n",
    "if lab_rows:\n",
    "    lb = pd.concat(lab_rows, ignore_index=True).drop_duplicates()\n",
    "    dlab = pd.read_csv(\n",
    "        HOSP_DIR / \"d_labitems.csv.gz\",\n",
    "        usecols=[\"itemid\",\"label\"],\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    lb = lb.merge(dlab, on=\"itemid\", how=\"left\")\n",
    "else:\n",
    "    lb = pd.DataFrame(columns=[\"hadm_id\",\"itemid\",\"label\"])\n",
    "\n",
    "# ========= L∆∞u & In nhanh =========\n",
    "out_dx  = SUBSET_DIR / f\"hadm_{HADM_ID}_diagnoses_top.csv\"\n",
    "out_pr  = SUBSET_DIR / f\"hadm_{HADM_ID}_procedures_top.csv\"\n",
    "out_lab = SUBSET_DIR / f\"hadm_{HADM_ID}_labs_top.csv\"\n",
    "\n",
    "dx.to_csv(out_dx, index=False)\n",
    "pr.to_csv(out_pr, index=False)\n",
    "lb.to_csv(out_lab, index=False)\n",
    "\n",
    "print(\"\\n=== T√ìM T·∫ÆT ===\")\n",
    "print(f\"- Ch·∫©n ƒëo√°n (trong top): {len(dx)} h√†ng ‚Üí {out_dx.name}\")\n",
    "print(f\"- Th·ªß thu·∫≠t   (trong top): {len(pr)} h√†ng ‚Üí {out_pr.name}\")\n",
    "print(f\"- X√©t nghi·ªám  (trong top): {len(lb)} h√†ng ‚Üí {out_lab.name}\")\n",
    "\n",
    "# N·∫øu mu·ªën xem nhanh:\n",
    "print(\"\\nV√≠ d·ª• 3 d√≤ng ƒë·∫ßu m·ªói b·∫£ng:\")\n",
    "print(dx.head(3))\n",
    "print(pr.head(3))\n",
    "print(lb.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03904b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadm_24151113_procedures_top.csv\n",
      ".DS_Store\n",
      "diagnoses_27783915.csv\n",
      "admissions.csv\n",
      "discharge_example.csv\n",
      "discharge.csv.gz\n",
      "hadm_24151113_diagnoses_top.csv\n",
      "sample_discharge.csv\n",
      "hadm_24151113_labs_top.csv\n",
      "icd.csv\n",
      "procedures.csv\n",
      "summary_27783915.csv\n",
      "discharge_full.csv.gz\n",
      "labs_27783915.csv\n",
      "procedures_27783915.csv\n",
      "24151113_diagnoses.csv\n",
      "patients.csv\n",
      "24151113_labs.csv\n",
      "labs.csv\n",
      "d_icd_diagnoses.csv\n",
      "24151113_procedures.csv\n",
      "d_icd_procedures.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ch√≠nh (nh·ªõ r·∫±ng notebook n·∫±m trong /notebooks)\n",
    "subset_dir = Path(\"../data/mimiciv_subset\")\n",
    "\n",
    "# C√°c file c·∫ßn ƒë·ªçc\n",
    "target_files = [\n",
    "    \"admissions.csv\",\n",
    "    \"discharge.csv.gz\",\n",
    "    \"icd.csv\",\n",
    "    \"procedures.csv\",\n",
    "    \"patients.csv\",\n",
    "    \"labs.csv\",\n",
    "    \"d_icd_diagnoses.csv\",\n",
    "    \"d_icd_procedures.csv\",\n",
    "]\n",
    "\n",
    "def explore_csv(path: Path, nrows=5):\n",
    "    \"\"\"In th√¥ng tin s∆° b·ªô c·ªßa 1 file CSV\"\"\"\n",
    "    print(f\"\\n=== {path.name} ===\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, compression='infer', nrows=nrows)\n",
    "        print(f\"Shape (approx): {pd.read_csv(path, compression='infer', usecols=[0]).shape[0]} rows √ó {len(df.columns)} cols\")\n",
    "        print(\"Columns:\", list(df.columns))\n",
    "        print(\"\\nSample rows:\")\n",
    "        display(df.head())\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error reading file:\", e)\n",
    "\n",
    "# Duy·ªát t·ª´ng file\n",
    "for file in target_files:\n",
    "    path = subset_dir / file\n",
    "    if path.exists():\n",
    "        explore_csv(path)\n",
    "    else:\n",
    "        print(f\"‚ùå Missing file: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e3047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== admissions.csv ===\n",
      "Shape (approx): 56296 rows √ó 5 cols\n",
      "Columns: ['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16904137</td>\n",
       "      <td>21081215</td>\n",
       "      <td>2105-10-04 17:26:00</td>\n",
       "      <td>2105-10-12 11:11:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18106347</td>\n",
       "      <td>24305596</td>\n",
       "      <td>2110-01-11 10:14:00</td>\n",
       "      <td>2110-01-15 17:31:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16284044</td>\n",
       "      <td>23864737</td>\n",
       "      <td>2110-01-11 19:58:00</td>\n",
       "      <td>2110-01-17 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12257372</td>\n",
       "      <td>23964251</td>\n",
       "      <td>2110-01-13 07:15:00</td>\n",
       "      <td>2110-01-19 14:43:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13201095</td>\n",
       "      <td>28453791</td>\n",
       "      <td>2110-01-18 14:46:00</td>\n",
       "      <td>2110-01-25 09:40:00</td>\n",
       "      <td>2110-01-25 09:40:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id            admittime            dischtime  \\\n",
       "0    16904137  21081215  2105-10-04 17:26:00  2105-10-12 11:11:00   \n",
       "1    18106347  24305596  2110-01-11 10:14:00  2110-01-15 17:31:00   \n",
       "2    16284044  23864737  2110-01-11 19:58:00  2110-01-17 16:00:00   \n",
       "3    12257372  23964251  2110-01-13 07:15:00  2110-01-19 14:43:00   \n",
       "4    13201095  28453791  2110-01-18 14:46:00  2110-01-25 09:40:00   \n",
       "\n",
       "             deathtime  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4  2110-01-25 09:40:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== discharge.csv.gz ===\n",
      "Shape (approx): 87544 rows √ó 3 cols\n",
      "Columns: ['hadm_id', 'charttime', 'text']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000147</td>\n",
       "      <td>2121-09-03</td>\n",
       "      <td>Chief Complaint:\\nchest pain\\n\\nMajor Surgical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000235</td>\n",
       "      <td>2139-12-03</td>\n",
       "      <td>Chief Complaint:\\nAltered mental status\\n\\nMaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000374</td>\n",
       "      <td>2132-12-07</td>\n",
       "      <td>Chief Complaint:\\nabdominal pain\\n\\nMajor Surg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000471</td>\n",
       "      <td>2171-04-13</td>\n",
       "      <td>Chief Complaint:\\nMalaise\\n\\nMajor Surgical or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000629</td>\n",
       "      <td>2161-12-26</td>\n",
       "      <td>Chief Complaint:\\nmental status change\\n\\nMajo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hadm_id   charttime                                               text\n",
       "0  20000147  2121-09-03  Chief Complaint:\\nchest pain\\n\\nMajor Surgical...\n",
       "1  20000235  2139-12-03  Chief Complaint:\\nAltered mental status\\n\\nMaj...\n",
       "2  20000374  2132-12-07  Chief Complaint:\\nabdominal pain\\n\\nMajor Surg...\n",
       "3  20000471  2171-04-13  Chief Complaint:\\nMalaise\\n\\nMajor Surgical or...\n",
       "4  20000629  2161-12-26  Chief Complaint:\\nmental status change\\n\\nMajo..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== icd.csv ===\n",
      "Shape (approx): 50 rows √ó 2 cols\n",
      "Columns: ['icd_code', 'count']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icd_code</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4019</td>\n",
       "      <td>102368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E785</td>\n",
       "      <td>84570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I10</td>\n",
       "      <td>83775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2724</td>\n",
       "      <td>67293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z87891</td>\n",
       "      <td>62806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  icd_code   count\n",
       "0     4019  102368\n",
       "1     E785   84570\n",
       "2      I10   83775\n",
       "3     2724   67293\n",
       "4   Z87891   62806"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== procedures.csv ===\n",
      "Shape (approx): 50 rows √ó 2 cols\n",
      "Columns: ['proc_code', 'count']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proc_code</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3893</td>\n",
       "      <td>14644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02HV33Z</td>\n",
       "      <td>14353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8938</td>\n",
       "      <td>10519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3897</td>\n",
       "      <td>10347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8856</td>\n",
       "      <td>9549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proc_code  count\n",
       "0      3893  14644\n",
       "1   02HV33Z  14353\n",
       "2      8938  10519\n",
       "3      3897  10347\n",
       "4      8856   9549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== patients.csv ===\n",
      "Shape (approx): 52610 rows √ó 4 cols\n",
      "Columns: ['subject_id', 'gender', 'anchor_age', 'anchor_year']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>anchor_age</th>\n",
       "      <th>anchor_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>F</td>\n",
       "      <td>52</td>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000826</td>\n",
       "      <td>F</td>\n",
       "      <td>32</td>\n",
       "      <td>2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000980</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001217</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001401</td>\n",
       "      <td>F</td>\n",
       "      <td>89</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id gender  anchor_age  anchor_year\n",
       "0    10000032      F          52         2180\n",
       "1    10000826      F          32         2146\n",
       "2    10000980      F          73         2186\n",
       "3    10001217      F          55         2157\n",
       "4    10001401      F          89         2131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== labs.csv ===\n",
      "Shape (approx): 50 rows √ó 3 cols\n",
      "Columns: ['itemid', 'count', 'label']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50971</td>\n",
       "      <td>2648172</td>\n",
       "      <td>Potassium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50983</td>\n",
       "      <td>2625912</td>\n",
       "      <td>Sodium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50902</td>\n",
       "      <td>2601418</td>\n",
       "      <td>Chloride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50912</td>\n",
       "      <td>2587046</td>\n",
       "      <td>Creatinine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51221</td>\n",
       "      <td>2580696</td>\n",
       "      <td>Hematocrit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemid    count       label\n",
       "0   50971  2648172   Potassium\n",
       "1   50983  2625912      Sodium\n",
       "2   50902  2601418    Chloride\n",
       "3   50912  2587046  Creatinine\n",
       "4   51221  2580696  Hematocrit"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== d_icd_diagnoses.csv ===\n",
      "Shape (approx): 50 rows √ó 4 cols\n",
      "Columns: ['icd_code', 'icd_version', 'long_title', 'count']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>long_title</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4019</td>\n",
       "      <td>9</td>\n",
       "      <td>Unspecified essential hypertension</td>\n",
       "      <td>102368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E785</td>\n",
       "      <td>10</td>\n",
       "      <td>Hyperlipidemia, unspecified</td>\n",
       "      <td>84570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I10</td>\n",
       "      <td>10</td>\n",
       "      <td>Essential (primary) hypertension</td>\n",
       "      <td>83775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2724</td>\n",
       "      <td>9</td>\n",
       "      <td>Other and unspecified hyperlipidemia</td>\n",
       "      <td>67293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z87891</td>\n",
       "      <td>10</td>\n",
       "      <td>Personal history of nicotine dependence</td>\n",
       "      <td>62806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  icd_code  icd_version                               long_title   count\n",
       "0     4019            9       Unspecified essential hypertension  102368\n",
       "1     E785           10              Hyperlipidemia, unspecified   84570\n",
       "2      I10           10         Essential (primary) hypertension   83775\n",
       "3     2724            9     Other and unspecified hyperlipidemia   67293\n",
       "4   Z87891           10  Personal history of nicotine dependence   62806"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== d_icd_procedures.csv ===\n",
      "Shape (approx): 50 rows √ó 5 cols\n",
      "Columns: ['icd_code', 'icd_version', 'long_title', 'proc_code', 'count']\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>long_title</th>\n",
       "      <th>proc_code</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3893</td>\n",
       "      <td>9</td>\n",
       "      <td>Venous catheterization, not elsewhere classified</td>\n",
       "      <td>3893</td>\n",
       "      <td>14644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02HV33Z</td>\n",
       "      <td>10</td>\n",
       "      <td>Insertion of Infusion Device into Superior Ven...</td>\n",
       "      <td>02HV33Z</td>\n",
       "      <td>14353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8938</td>\n",
       "      <td>9</td>\n",
       "      <td>Other nonoperative respiratory measurements</td>\n",
       "      <td>8938</td>\n",
       "      <td>10519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3897</td>\n",
       "      <td>9</td>\n",
       "      <td>Central venous catheter placement with guidance</td>\n",
       "      <td>3897</td>\n",
       "      <td>10347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8856</td>\n",
       "      <td>9</td>\n",
       "      <td>Coronary arteriography using two catheters</td>\n",
       "      <td>8856</td>\n",
       "      <td>9549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  icd_code  icd_version                                         long_title  \\\n",
       "0     3893            9   Venous catheterization, not elsewhere classified   \n",
       "1  02HV33Z           10  Insertion of Infusion Device into Superior Ven...   \n",
       "2     8938            9        Other nonoperative respiratory measurements   \n",
       "3     3897            9    Central venous catheter placement with guidance   \n",
       "4     8856            9         Coronary arteriography using two catheters   \n",
       "\n",
       "  proc_code  count  \n",
       "0      3893  14644  \n",
       "1   02HV33Z  14353  \n",
       "2      8938  10519  \n",
       "3      3897  10347  \n",
       "4      8856   9549  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ch√≠nh (nh·ªõ r·∫±ng notebook n·∫±m trong /notebooks)\n",
    "subset_dir = Path(\"../data/mimiciv_subset\")\n",
    "\n",
    "# C√°c file c·∫ßn ƒë·ªçc\n",
    "target_files = [\n",
    "    \"admissions.csv\",\n",
    "    \"discharge.csv.gz\",\n",
    "    \"icd.csv\",\n",
    "    \"procedures.csv\",\n",
    "    \"patients.csv\",\n",
    "    \"labs.csv\",\n",
    "    \"d_icd_diagnoses.csv\",\n",
    "    \"d_icd_procedures.csv\",\n",
    "]\n",
    "\n",
    "def explore_csv(path: Path, nrows=5):\n",
    "    \"\"\"In th√¥ng tin s∆° b·ªô c·ªßa 1 file CSV\"\"\"\n",
    "    print(f\"\\n=== {path.name} ===\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, compression='infer', nrows=nrows)\n",
    "        print(f\"Shape (approx): {pd.read_csv(path, compression='infer', usecols=[0]).shape[0]} rows √ó {len(df.columns)} cols\")\n",
    "        print(\"Columns:\", list(df.columns))\n",
    "        print(\"\\nSample rows:\")\n",
    "        display(df.head())\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error reading file:\", e)\n",
    "\n",
    "# Duy·ªát t·ª´ng file\n",
    "for file in target_files:\n",
    "    path = subset_dir / file\n",
    "    if path.exists():\n",
    "        explore_csv(path)\n",
    "    else:\n",
    "        print(f\"‚ùå Missing file: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544d04b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admissions.csv: 5 columns -> ['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime']\n",
      "discharge.csv.gz: 3 columns -> ['hadm_id', 'charttime', 'text']\n",
      "patients.csv: 4 columns -> ['subject_id', 'gender', 'anchor_age', 'anchor_year']\n",
      "icd.csv: 2 columns -> ['icd_code', 'count']\n",
      "procedures.csv: 2 columns -> ['proc_code', 'count']\n",
      "labs.csv: 3 columns -> ['itemid', 'count', 'label']\n",
      "d_icd_diagnoses.csv: 4 columns -> ['icd_code', 'icd_version', 'long_title', 'count']\n",
      "d_icd_procedures.csv: 5 columns -> ['icd_code', 'icd_version', 'long_title', 'proc_code', 'count']\n"
     ]
    }
   ],
   "source": [
    "schemas = {\n",
    "    \"admissions.csv\": [\"subject_id\", \"hadm_id\", \"admittime\", \"dischtime\", \"deathtime\"],\n",
    "    \"discharge.csv.gz\": [\"hadm_id\", \"charttime\", \"text\"],\n",
    "    \"patients.csv\": [\"subject_id\", \"gender\", \"anchor_age\", \"anchor_year\"],\n",
    "    \"icd.csv\": [\"icd_code\", \"count\"],\n",
    "    \"procedures.csv\": [\"proc_code\", \"count\"],\n",
    "    \"labs.csv\": [\"itemid\", \"count\", \"label\"],\n",
    "    \"d_icd_diagnoses.csv\": [\"icd_code\", \"icd_version\", \"long_title\", \"count\"],\n",
    "    \"d_icd_procedures.csv\": [\"icd_code\", \"icd_version\", \"long_title\", \"proc_code\", \"count\"],\n",
    "}\n",
    "\n",
    "for name, cols in schemas.items():\n",
    "    print(f\"{name}: {len(cols)} columns -> {cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ad85a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªçc 56,296 l∆∞·ª£t nh·∫≠p vi·ªán t·ª´ examples.parquet\n",
      "\n",
      "[y_icd]\n",
      "- T·ª∑ l·ªá vector to√†n 0: 0.000%\n",
      "- Trung b√¨nh s·ªë nh√£n d∆∞∆°ng / m·∫´u: 1.979\n",
      "\n",
      "[y_proc]\n",
      "- T·ª∑ l·ªá vector to√†n 0: 0.000%\n",
      "- Trung b√¨nh s·ªë nh√£n d∆∞∆°ng / m·∫´u: 1.979\n",
      "\n",
      "[y_lab]\n",
      "- T·ª∑ l·ªá vector to√†n 0: 0.000%\n",
      "- Trung b√¨nh s·ªë nh√£n d∆∞∆°ng / m·∫´u: 2.445\n",
      "\n",
      "üßæ S·ªë ca kh√¥ng c√≥ ghi ch√∫ text: 0 / 56296\n",
      "\n",
      "üìò Th√¥ng tin vocab:\n",
      "- ICD vocab size:  50\n",
      "- PROC vocab size: 50\n",
      "- LAB vocab size:  50\n",
      "y_icd kh·ªõp vocab (50 nh√£n): ‚úÖ OK\n",
      "y_proc kh·ªõp vocab (50 nh√£n): ‚úÖ OK\n",
      "y_lab kh·ªõp vocab (50 nh√£n): ‚úÖ OK\n",
      "\n",
      "üß© V√≠ d·ª• m·ªôt m·∫´u b·∫•t k·ª≥:\n",
      "hadm_id: 28611522\n",
      "gender: M | age: 53\n",
      "ICD > 0: [25 30 41]\n",
      "PROC > 0: [25 30 41]\n",
      "LAB  > 0: [21]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ƒê∆∞·ªùng d·∫´n ---\n",
    "DATA_ROOT = Path(\"..\") / \"data\" / \"proc\"\n",
    "PARQUET_PATH = DATA_ROOT / \"examples.parquet\"\n",
    "VOCAB_PATH   = DATA_ROOT / \"vocab_meta.json\"\n",
    "\n",
    "# --- 1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu ---\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "print(f\"‚úÖ ƒê√£ ƒë·ªçc {len(df):,} l∆∞·ª£t nh·∫≠p vi·ªán t·ª´ {PARQUET_PATH.name}\")\n",
    "\n",
    "# --- 2Ô∏è‚É£ Ki·ªÉm tra c·ªôt c√≥ t·ªìn t·∫°i kh√¥ng ---\n",
    "expected_cols = [\"hadm_id\", \"y_icd\", \"y_proc\", \"y_lab\"]\n",
    "missing_cols = [c for c in expected_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    print(\"‚ö†Ô∏è Thi·∫øu c·ªôt:\", missing_cols)\n",
    "\n",
    "# --- 3Ô∏è‚É£ T√≠nh th·ªëng k√™ nh√£n ---\n",
    "def check_multihot(name):\n",
    "    n_total = len(df)\n",
    "    n_all_zero = (df[name].apply(lambda a: np.sum(a) == 0)).sum()\n",
    "    mean_pos = df[name].apply(lambda a: np.sum(a)).mean()\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"- T·ª∑ l·ªá vector to√†n 0: {n_all_zero / n_total:.3%}\")\n",
    "    print(f\"- Trung b√¨nh s·ªë nh√£n d∆∞∆°ng / m·∫´u: {mean_pos:.3f}\")\n",
    "\n",
    "check_multihot(\"y_icd\")\n",
    "check_multihot(\"y_proc\")\n",
    "check_multihot(\"y_lab\")\n",
    "\n",
    "# --- 4Ô∏è‚É£ Ki·ªÉm tra d·ªØ li·ªáu r·ªóng ho·∫∑c text b·ªã thi·∫øu ---\n",
    "missing_text = (df[\"text\"].str.strip() == \"\").sum()\n",
    "print(f\"\\nüßæ S·ªë ca kh√¥ng c√≥ ghi ch√∫ text: {missing_text} / {len(df)}\")\n",
    "\n",
    "# --- 5Ô∏è‚É£ T·∫£i vocab ƒë·ªÉ ƒë·ªëi chi·∫øu k√≠ch th∆∞·ªõc ---\n",
    "with open(VOCAB_PATH) as f:\n",
    "    vocab_meta = json.load(f)\n",
    "print(\"\\nüìò Th√¥ng tin vocab:\")\n",
    "print(f\"- ICD vocab size:  {vocab_meta.get('n_icd', '?')}\")\n",
    "print(f\"- PROC vocab size: {vocab_meta.get('n_proc', '?')}\")\n",
    "print(f\"- LAB vocab size:  {vocab_meta.get('n_lab', '?')}\")\n",
    "\n",
    "# --- 6Ô∏è‚É£ Ki·ªÉm tra ƒë·ªô d√†i vector c√≥ kh·ªõp v·ªõi vocab kh√¥ng ---\n",
    "for name, key in zip([\"y_icd\", \"y_proc\", \"y_lab\"],\n",
    "                     [\"n_icd\", \"n_proc\", \"n_lab\"]):\n",
    "    n_vocab = vocab_meta.get(key)\n",
    "    if n_vocab:\n",
    "        ok = df[name].apply(lambda a: len(a) == n_vocab).all()\n",
    "        print(f\"{name} kh·ªõp vocab ({n_vocab} nh√£n):\", \"‚úÖ OK\" if ok else \"‚ùå Sai ƒë·ªô d√†i\")\n",
    "\n",
    "# --- 7Ô∏è‚É£ Hi·ªÉn th·ªã v√≠ d·ª• m·∫´u ---\n",
    "print(\"\\nüß© V√≠ d·ª• m·ªôt m·∫´u b·∫•t k·ª≥:\")\n",
    "sample = df.sample(1).iloc[0]\n",
    "print(\"hadm_id:\", sample[\"hadm_id\"])\n",
    "print(\"gender:\", sample[\"gender\"], \"| age:\", sample[\"age_at_admit\"])\n",
    "print(\"ICD > 0:\", np.where(np.array(sample[\"y_icd\"]) == 1)[0])\n",
    "print(\"PROC > 0:\", np.where(np.array(sample[\"y_proc\"]) == 1)[0])\n",
    "print(\"LAB  > 0:\", np.where(np.array(sample[\"y_lab\"]) == 1)[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
