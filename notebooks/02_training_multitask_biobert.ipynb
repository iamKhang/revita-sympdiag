{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b197f8ef",
   "metadata": {},
   "source": [
    "C·∫•u h√¨nh v√† ph·ª• thu·ªôc\n",
    "\n",
    "M·ª•c ƒë√≠ch: thi·∫øt l·∫≠p tham s·ªë hu·∫•n luy·ªán v√† th∆∞ vi·ªán c·∫ßn d√πng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814d776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer OK: BertTokenizerFast\n",
      "Model OK: TFBertModel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "print(\"Tokenizer OK:\", type(tok).__name__)\n",
    "print(\"Model OK:\", type(bert).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0cbcb",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 1 ‚Äî ƒê∆∞a ƒë·∫ßu v√†o (Input)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "ƒê·ªçc d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω (examples.parquet, vocab_meta.json).\n",
    "\n",
    "Bi·∫øn vƒÉn b·∫£n text th√†nh token ID v√† attention mask m√† BioBERT hi·ªÉu (tokenizer c·ªßa BioBERT).\n",
    "\n",
    "T·∫°o tf.data.Dataset cho train v√† validation (ch·ªâ chu·∫©n b·ªã ƒë·∫ßu v√†o, ch∆∞a x√¢y m√¥ h√¨nh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c0fae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang d√πng:\n",
      " - PARQUET_PATH: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/proc/examples.parquet\n",
      " - VOCAB_PATH  : /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/proc/vocab_meta.json\n",
      "S·ªë m·∫´u: 828 | n_icd: 200 | n_lab: 50\n",
      "S·ªë m·∫´u train: 717\n",
      "S·ªë m·∫´u val  : 111\n",
      "K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (X): {'input_ids': TensorShape([8, 256]), 'attention_mask': TensorShape([8, 256])}\n",
      "K√≠ch th∆∞·ªõc nh√£n (Y):    {'icd': TensorShape([8, 200]), 'lab': TensorShape([8, 50])}\n",
      "V√≠ d·ª• text g·ªëc (r√∫t g·ªçn): EXAMINATION: LIVER OR GALLBLADDER US (SINGLE ORGAN) INDICATION: ___ year-old female with cirrhosis, jaundice. TECHNIQUE:\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# B∆Ø·ªöC 1 ‚Äî ƒê∆ØA ƒê·∫¶U V√ÄO (INPUT)\n",
    "# =========================\n",
    "\n",
    "# 1) Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 2) C·∫•u h√¨nh c∆° b·∫£n cho vi·ªác t·∫°o batch\n",
    "MAX_LENGTH = 256   # ƒë·ªô d√†i t·ªëi ƒëa c·ªßa chu·ªói token (c·∫Øt n·∫øu d√†i h∆°n, pad n·∫øu ng·∫Øn h∆°n)\n",
    "BATCH_SIZE = 8     # k√≠ch th∆∞·ªõc l√¥ (batch)\n",
    "VAL_RATIO  = 0.15  # t·ªâ l·ªá d√†nh cho validation\n",
    "SEED = 42          # h·∫°t gi·ªëng ng·∫´u nhi√™n ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£\n",
    "\n",
    "# 3) C·ªë ƒë·ªãnh ƒë∆∞·ªùng d·∫´n: notebook ƒëang ·ªü 'notebooks/', c√≤n d·ªØ li·ªáu ·ªü '../data/proc/'\n",
    "DATA_ROOT = Path(\"..\") / \"data\" / \"proc\"\n",
    "PARQUET_PATH = DATA_ROOT / \"examples.parquet\"\n",
    "VOCAB_PATH   = DATA_ROOT / \"vocab_meta.json\"\n",
    "\n",
    "print(\"ƒêang d√πng:\")\n",
    "print(\" - PARQUET_PATH:\", PARQUET_PATH.resolve())\n",
    "print(\" - VOCAB_PATH  :\", VOCAB_PATH.resolve())\n",
    "\n",
    "# 4) Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file d·ªØ li·ªáu\n",
    "if not PARQUET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Kh√¥ng t√¨m th·∫•y {PARQUET_PATH}. H√£y ch·∫°y Notebook ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ sinh file, \"\n",
    "        f\"ho·∫∑c ch·ªânh l·∫°i ƒë∆∞·ªùng d·∫´n cho ƒë√∫ng v·ªã tr√≠ th·ª±c t·∫ø.\"\n",
    "    )\n",
    "if not VOCAB_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Kh√¥ng t√¨m th·∫•y {VOCAB_PATH}. H√£y ki·ªÉm tra Notebook ti·ªÅn x·ª≠ l√Ω ho·∫∑c ƒë∆∞·ªùng d·∫´n.\"\n",
    "    )\n",
    "\n",
    "# 5) Ki·ªÉm so√°t ng·∫´u nhi√™n ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 6) ƒê·ªçc d·ªØ li·ªáu b·∫£ng ƒë√£ ti·ªÅn x·ª≠ l√Ω v√† metadata nh√£n\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "icd_vocab        = meta[\"icd_vocab\"]              # danh s√°ch ICD-block (vocab)\n",
    "lab_vocab_items  = meta[\"lab_vocab_items\"]        # danh s√°ch itemid x√©t nghi·ªám (vocab)\n",
    "itemid_to_label  = {int(k): v for k, v in meta[\"itemid_to_label\"].items()}\n",
    "\n",
    "n_icd = len(icd_vocab)\n",
    "n_lab = len(lab_vocab_items)\n",
    "\n",
    "print(f\"S·ªë m·∫´u: {len(df)} | n_icd: {n_icd} | n_lab: {n_lab}\")\n",
    "\n",
    "# 7) ƒê·∫£m b·∫£o 2 c·ªôt nh√£n l√† m·∫£ng float32 ƒë·ªÉ d√πng v·ªõi BinaryCrossentropy (·ªü c√°c b∆∞·ªõc sau)\n",
    "df[\"y_icd\"] = df[\"y_icd\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "df[\"y_lab\"] = df[\"y_lab\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "\n",
    "# 8) Chia train/validation theo subject_id (tr√°nh c√πng 1 b·ªánh nh√¢n xu·∫•t hi·ªán ·ªü c·∫£ 2 t·∫≠p)\n",
    "groups = df[\"subject_id\"].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=groups))\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"S·ªë m·∫´u train:\", len(train_df))\n",
    "print(\"S·ªë m·∫´u val  :\", len(val_df))\n",
    "\n",
    "# 9) Chu·∫©n b·ªã tokenizer c·ªßa BioBERT\n",
    "#    - N·∫øu b·∫°n ƒë√£ ch·∫°y √¥ tr∆∞·ªõc v√† c√≥ bi·∫øn 'tok', ta t·∫≠n d·ª•ng l·∫°i.\n",
    "#    - N·∫øu ch∆∞a, t·ª± n·∫°p t·ª´ MODEL_NAME.\n",
    "try:\n",
    "    tokenizer = tok  # t√°i d√πng tokenizer ƒë√£ n·∫°p\n",
    "except NameError:\n",
    "    MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 10) H√†m m√£ h√≥a 1 l√¥ vƒÉn b·∫£n sang token ID & attention mask (ƒë·∫ßu v√†o m√¥ h√¨nh)\n",
    "def encode_text_batch(text_list, max_length):\n",
    "    \"\"\"\n",
    "    Bi·∫øn danh s√°ch chu·ªói text th√†nh dict numpy arrays:\n",
    "      - input_ids: ID c·ªßa token theo t·ª´ ƒëi·ªÉn c·ªßa BioBERT\n",
    "      - attention_mask: 1 ·ªü v·ªã tr√≠ c√≥ token th·ª±c, 0 ·ªü ch·ªó padding\n",
    "    T·∫•t c·∫£ ƒë∆∞·ª£c pad/c·∫Øt v·ªÅ ƒë√∫ng max_length.\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        list(text_list),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=None   # tr·∫£ v·ªÅ list/np array thay v√¨ tensor c·ªßa HF\n",
    "    )\n",
    "    # √âp ki·ªÉu sang int32 ƒë·ªÉ ti·ªán ƒë∆∞a v√†o tf.data\n",
    "    enc[\"input_ids\"] = np.asarray(enc[\"input_ids\"], dtype=np.int32)\n",
    "    enc[\"attention_mask\"] = np.asarray(enc[\"attention_mask\"], dtype=np.int32)\n",
    "    return enc\n",
    "\n",
    "# 11) T·∫°o tf.data.Dataset theo ki·ªÉu generator ƒë·ªÉ ti·∫øt ki·ªám RAM v√† linh ho·∫°t chi·ªÅu d√†i chu·ªói\n",
    "def make_tfds_from_df(frame, batch_size, max_length):\n",
    "    \"\"\"\n",
    "    T·ª´ DataFrame g·ªìm c·ªôt 'text', 'y_icd', 'y_lab' t·∫°o Dataset:\n",
    "      - X = {\"input_ids\": [B, L], \"attention_mask\": [B, L]}\n",
    "      - y = {\"icd\": [B, n_icd],   \"lab\": [B, n_lab]}\n",
    "    \"\"\"\n",
    "    texts = frame[\"text\"].fillna(\"\").tolist()\n",
    "    y_icd = np.stack(frame[\"y_icd\"].to_list()).astype(np.float32)\n",
    "    y_lab = np.stack(frame[\"y_lab\"].to_list()).astype(np.float32)\n",
    "\n",
    "    def gen():\n",
    "        \"\"\"Ph√°t t·ª´ng ph·∫ßn t·ª≠ (x_i, y_i) ƒë·ªÉ tf.data c√≥ th·ªÉ l√†m padded_batch sau ƒë√≥.\"\"\"\n",
    "        # M√£ h√≥a theo t·ª´ng block nh·ªè b·∫±ng ch√≠nh 'batch_size' ƒë·ªÉ gi·∫£m th·ªùi gian tokenize\n",
    "        N = len(texts)\n",
    "        start = 0\n",
    "        while start < N:\n",
    "            end = min(start + batch_size, N)\n",
    "            enc = encode_text_batch(texts[start:end], max_length=max_length)\n",
    "            for i in range(end - start):\n",
    "                yield (\n",
    "                    {\n",
    "                        \"input_ids\": enc[\"input_ids\"][i],\n",
    "                        \"attention_mask\": enc[\"attention_mask\"][i],\n",
    "                    },\n",
    "                    {\n",
    "                        \"icd\": y_icd[start + i],\n",
    "                        \"lab\": y_lab[start + i],\n",
    "                    }\n",
    "                )\n",
    "            start = end\n",
    "\n",
    "    # Khai b√°o \"h√¨nh th√π\" c·ªßa t·ª´ng ph·∫ßn t·ª≠ ƒë·ªÉ tf.data bi·∫øt c√°ch ƒë√≥ng g√≥i/padding\n",
    "    output_signature = (\n",
    "        {\n",
    "            \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        },\n",
    "        {\n",
    "            \"icd\": tf.TensorSpec(shape=(n_icd,), dtype=tf.float32),\n",
    "            \"lab\": tf.TensorSpec(shape=(n_lab,), dtype=tf.float32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "\n",
    "    # ƒê√≥ng g√≥i th√†nh batch v√† pad/truncate v·ªÅ MAX_LENGTH cho ƒë·ªìng nh·∫•t\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(\n",
    "            {\"input_ids\": [MAX_LENGTH], \"attention_mask\": [MAX_LENGTH]},\n",
    "            {\"icd\": [n_icd], \"lab\": [n_lab]},\n",
    "        ),\n",
    "        padding_values=(\n",
    "            {\"input_ids\": tf.constant(0, tf.int32), \"attention_mask\": tf.constant(0, tf.int32)},\n",
    "            {\"icd\": tf.constant(0.0, tf.float32),   \"lab\": tf.constant(0.0, tf.float32)},\n",
    "        ),\n",
    "        drop_remainder=False\n",
    "    )\n",
    "    # T·ªëi ∆∞u hi·ªáu nƒÉng ƒë·ªçc\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 12) T·∫°o train_ds v√† val_ds ƒë·ªÉ d√πng ·ªü B∆∞·ªõc 2 (BioBERT) v√† B∆∞·ªõc 3 (Heads)\n",
    "train_ds = make_tfds_from_df(train_df, BATCH_SIZE, MAX_LENGTH).shuffle(1024)\n",
    "val_ds   = make_tfds_from_df(val_df,   BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "# 13) Ki·ªÉm tra nhanh m·ªôt batch ƒë·ªÉ hi·ªÉu ƒë√∫ng √Ω\n",
    "for batch in train_ds.take(1):\n",
    "    X, Y = batch\n",
    "    print(\"K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (X):\", {k: v.shape for k, v in X.items()})\n",
    "    print(\"K√≠ch th∆∞·ªõc nh√£n (Y):   \", {k: v.shape for k, v in Y.items()})\n",
    "    # In th·ª≠ 120 k√Ω t·ª± ƒë·∫ßu c·ªßa vƒÉn b·∫£n ƒë·∫ßu ti√™n trong batch (ƒë·ªÉ th·∫•y d·ªØ li·ªáu g·ªëc)\n",
    "    # L∆∞u √Ω: X kh√¥ng ch·ª©a 'text' th√¥, ch·ªâ c√≥ token; ta l·∫•y l·∫°i t·ª´ train_df ƒë·ªÉ minh h·ªça\n",
    "    print(\"V√≠ d·ª• text g·ªëc (r√∫t g·ªçn):\", train_df.iloc[0][\"text\"][:120].replace(\"\\n\", \" \"))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6b456",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 2 ‚Äî X·ª≠ l√Ω trong BioBERT (t·∫°o vector ng·ªØ nghƒ©a)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "ƒê∆∞a batch input_ids + attention_mask v√†o BioBERT.\n",
    "\n",
    "L·∫•y ra vector bi·ªÉu di·ªÖn c√¢u (d√πng token ƒë·∫∑c bi·ªát [CLS]).\n",
    "\n",
    "Ki·ªÉm tra k√≠ch th∆∞·ªõc v√† gi√° tr·ªã ƒëi·ªÉn h√¨nh ƒë·ªÉ x√°c nh·∫≠n ‚Äúd·ªãch vƒÉn b·∫£n ‚Üí ng√¥n ng·ªØ s·ªë‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ceb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K√≠ch th∆∞·ªõc vector CLS: (8, 768)\n",
      "8 s·ªë ƒë·∫ßu c·ªßa CLS[0]: [0.13459999859333038, -0.023900000378489494, -0.17829999327659607, -0.04659999907016754, -0.436599999666214, -0.028200000524520874, 0.3319000005722046, 0.2379000037908554]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# B∆Ø·ªöC 2 ‚Äî X·ª¨ L√ù TRONG BIOBERT\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "DROPOUT = 0.1  # dropout nh·∫π ƒë·ªÉ ·ªïn ƒë·ªãnh bi·ªÉu di·ªÖn khi train (·ªü b∆∞·ªõc n√†y ta ch·ªâ forward th·ª≠)\n",
    "\n",
    "# 2.1 N·∫°p backbone BioBERT (TF). N·∫øu ƒë√£ c√≥ bi·∫øn `bert` t·ª´ tr∆∞·ªõc th√¨ t√°i d√πng.\n",
    "try:\n",
    "    _ = bert  # ki·ªÉm tra bi·∫øn ƒë√£ t·ªìn t·∫°i\n",
    "except NameError:\n",
    "    # from_pt=True: nhi·ªÅu checkpoint BioBERT l√† b·∫£n PyTorch; c·ªù n√†y cho ph√©p n·∫°p sang TF\n",
    "    bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "# 2.2 ƒê·ªãnh nghƒ©a inputs chu·∫©n Keras kh·ªõp v·ªõi batch c·ªßa tf.data\n",
    "input_ids      = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# 2.3 Cho d·ªØ li·ªáu ƒëi qua BioBERT ƒë·ªÉ l·∫•y \"d·∫•u v√¢n tay\" [CLS]\n",
    "#     - last_hidden_state: [batch, seq_len, hidden_size]\n",
    "#     - l·∫•y v·ªã tr√≠ [:, 0, :] t∆∞∆°ng ·ª©ng token [CLS] (ƒë·∫°i di·ªán to√†n c√¢u)\n",
    "bert_outputs = bert(input_ids, attention_mask=attention_mask, training=False)\n",
    "cls_vec = bert_outputs.last_hidden_state[:, 0, :]  # [batch, hidden_size], v√≠ d·ª• [8, 768]\n",
    "\n",
    "# (t√πy ch·ªçn) th√™m dropout nh·∫π ƒë·ªÉ quen v·ªõi train; ƒë√¢y kh√¥ng thay ƒë·ªïi k√≠ch th∆∞·ªõc\n",
    "cls_vec = tf.keras.layers.Dropout(DROPOUT, name=\"cls_dropout\")(cls_vec)\n",
    "\n",
    "# 2.4 G√≥i th√†nh m·ªôt \"encoder model\" xu·∫•t ra vector CLS ‚Äî d√πng l·∫°i ·ªü B∆∞·ªõc 3\n",
    "encoder = tf.keras.Model(\n",
    "    inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "    outputs={\"cls\": cls_vec},\n",
    "    name=\"biobert_encoder_cls\"\n",
    ")\n",
    "\n",
    "# 2.5 Ch·∫°y th·ª≠ 1 batch ƒë·ªÉ ki·ªÉm tra k√≠ch th∆∞·ªõc v√† v√†i s·ªë ƒë·∫ßu ti√™n\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    out = encoder(X_batch, training=False)   # forward pass\n",
    "    cls_batch = out[\"cls\"].numpy()           # [BATCH_SIZE, HIDDEN]\n",
    "    print(\"K√≠ch th∆∞·ªõc vector CLS:\", cls_batch.shape)\n",
    "    # In 8 gi√° tr·ªã ƒë·∫ßu c·ªßa 1 m·∫´u ƒë·ªÉ th·∫•y \"ng√¥n ng·ªØ s·ªë\" (ch·ªâ minh h·ªça)\n",
    "    print(\"8 s·ªë ƒë·∫ßu c·ªßa CLS[0]:\", np.round(cls_batch[0][:8], 4).tolist())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aa579",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c ma tr·∫≠n k√≠ch th∆∞·ªõc [BATCH_SIZE, HIDDEN] (th∆∞·ªùng HIDDEN = 768).\n",
    "\n",
    "M·ªói h√†ng l√† ‚Äúd·∫•u v√¢n tay s·ªë h·ªçc‚Äù c·ªßa 1 vƒÉn b·∫£n ‚Äî ch·ª©a ng·ªØ c·∫£nh y khoa ƒë√£ ƒë∆∞·ª£c BioBERT ‚Äúƒë·ªçc hi·ªÉu‚Äù.\n",
    "\n",
    "ƒê√¢y ch√≠nh l√† ƒë·∫ßu v√†o cho ‚Äúph·∫ßn ra quy·∫øt ƒë·ªãnh‚Äù ·ªü B∆∞·ªõc 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b908",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 3 ‚Äî Classifier head (Ph·∫ßn ra quy·∫øt ƒë·ªãnh)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "G·∫Øn 2 ‚Äúƒë·∫ßu ra quy·∫øt ƒë·ªãnh‚Äù (Dense) l√™n vector CLS t·ª´ BioBERT:\n",
    "\n",
    "ƒê·∫ßu ICD: d·ª± ƒëo√°n ƒëa nh√£n c√°c ICD-block.\n",
    "\n",
    "ƒê·∫ßu Lab: d·ª± ƒëo√°n ƒëa nh√£n c√°c x√©t nghi·ªám s·ªõm.\n",
    "\n",
    "M·ªói ƒë·∫ßu tr·∫£ v·ªÅ logits (s·ªë th·ª±c √¢m/d∆∞∆°ng). Khi qua sigmoid s·∫Ω th√†nh x√°c su·∫•t 0‚Äì1 cho t·ª´ng nh√£n.\n",
    "\n",
    "Ki·ªÉm tra k√≠ch th∆∞·ªõc ƒë·∫ßu ra v√† xem th·ª≠ top-k d·ª± ƒëo√°n tr√™n m·ªôt batch (ch∆∞a hu·∫•n luy·ªán, ch·ªâ ƒë·ªÉ minh h·ªça d√≤ng ch·∫£y d·ªØ li·ªáu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e1308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"biobert_multitask_heads\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                        Param #     Connected to                        \n",
      "========================================================================================================================\n",
      " input_ids (InputLayer)             [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " attention_mask (InputLayer)        [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " tf_bert_model_1 (TFBertModel)      TFBaseModelOutputWithPoolingAndCr   108310272   ['input_ids[0][0]',                 \n",
      "                                    ossAttentions(last_hidden_state=(                'attention_mask[0][0]']            \n",
      "                                    None, None, 768),                                                                   \n",
      "                                     pooler_output=(None, 768),                                                         \n",
      "                                     past_key_values=None, hidden_sta                                                   \n",
      "                                    tes=None, attentions=None, cross_                                                   \n",
      "                                    attentions=None)                                                                    \n",
      "                                                                                                                        \n",
      " tf.__operators__.getitem_1 (Slici  (None, 768)                         0           ['tf_bert_model_1[0][0]']           \n",
      " ngOpLambda)                                                                                                            \n",
      "                                                                                                                        \n",
      " cls_dropout (Dropout)              (None, 768)                         0           ['tf.__operators__.getitem_1[0][0]']\n",
      "                                                                                                                        \n",
      " icd_logits (Dense)                 (None, 200)                         153800      ['cls_dropout[0][0]']               \n",
      "                                                                                                                        \n",
      " lab_logits (Dense)                 (None, 50)                          38450       ['cls_dropout[0][0]']               \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 108502522 (413.90 MB)\n",
      "Trainable params: 108502522 (413.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "________________________________________________________________________________________________________________________\n",
      "K√≠ch th∆∞·ªõc icd_logits: (8, 200)\n",
      "K√≠ch th∆∞·ªõc lab_logits: (8, 50)\n",
      "Top-5 ICD indices: [53, 72, 14, 9, 79]\n",
      "Top-5 ICD probs  : [0.7955999970436096, 0.7822999954223633, 0.7800999879837036, 0.7788000106811523, 0.7644000053405762]\n",
      "Top-5 Lab indices: [11, 44, 37, 28, 7]\n",
      "Top-5 Lab probs  : [0.8439000248908997, 0.8066999912261963, 0.7979999780654907, 0.6883999705314636, 0.6830000281333923]\n",
      "Top-5 ICD codes: ['790', '278', 'I48', 'Y92', '412']\n",
      "Top-5 Lab itemids: [50902, 51274, 51116, 51498, 51279]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# B∆Ø·ªöC 3 ‚Äî CLASSIFIER HEAD\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 3.1 L·∫•y ƒë·∫ßu ra CLS t·ª´ encoder (ƒë√£ c√≥ ·ªü B∆∞·ªõc 2)\n",
    "#     encoder.inputs:  {\"input_ids\", \"attention_mask\"}\n",
    "#     encoder.outputs: {\"cls\"}  (vector k√≠ch th∆∞·ªõc [batch, hidden_size])\n",
    "cls_output = encoder.outputs[0]  # t∆∞∆°ng ƒë∆∞∆°ng encoder.get_layer(\"cls_dropout\").output\n",
    "\n",
    "# 3.2 Th√™m hai l·ªõp Dense (tuy·∫øn t√≠nh) cho 2 t√°c v·ª• ƒëa nh√£n\n",
    "#     - Kh√¥ng d√πng activation t·∫°i ƒë√¢y ƒë·ªÉ m√¥ h√¨nh tr·∫£ v·ªÅ \"logits\".\n",
    "#     - Logits gi√∫p ta d√πng BinaryCrossentropy(from_logits=True) ·ªü b∆∞·ªõc t√≠nh loss.\n",
    "icd_logits = tf.keras.layers.Dense(n_icd, name=\"icd_logits\")(cls_output)  # [batch, n_icd]\n",
    "lab_logits = tf.keras.layers.Dense(n_lab, name=\"lab_logits\")(cls_output)  # [batch, n_lab]\n",
    "\n",
    "# 3.3 T·∫°o m√¥ h√¨nh Keras ƒë·∫ßy ƒë·ªß (encoder + 2 heads)\n",
    "multitask_model = tf.keras.Model(\n",
    "    inputs=encoder.inputs,                    # {\"input_ids\", \"attention_mask\"}\n",
    "    outputs={\"icd\": icd_logits, \"lab\": lab_logits},  # hai nh√°nh logits\n",
    "    name=\"biobert_multitask_heads\"\n",
    ")\n",
    "\n",
    "# 3.4 Ki·ªÉm tra ki·∫øn tr√∫c\n",
    "multitask_model.summary(line_length=120)\n",
    "\n",
    "# 3.5 Ch·∫°y th·ª≠ m·ªôt batch ƒë·ªÉ xem k√≠ch th∆∞·ªõc logits v√† minh h·ªça x√°c su·∫•t (ch∆∞a hu·∫•n luy·ªán)\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    # logits th√¥ t·ª´ hai ƒë·∫ßu\n",
    "    logits = multitask_model.predict(X_batch, verbose=0)\n",
    "    icd_log = logits[\"icd\"]   # [BATCH_SIZE, n_icd]\n",
    "    lab_log = logits[\"lab\"]   # [BATCH_SIZE, n_lab]\n",
    "    print(\"K√≠ch th∆∞·ªõc icd_logits:\", icd_log.shape)\n",
    "    print(\"K√≠ch th∆∞·ªõc lab_logits:\", lab_log.shape)\n",
    "\n",
    "    # Chuy·ªÉn logits -> x√°c su·∫•t b·∫±ng sigmoid ƒë·ªÉ \"ƒë·ªçc\" d·ªÖ h∆°n\n",
    "    probs_icd = tf.sigmoid(icd_log).numpy()   # [BATCH_SIZE, n_icd]\n",
    "    probs_lab = tf.sigmoid(lab_log).numpy()   # [BATCH_SIZE, n_lab]\n",
    "\n",
    "    # L·∫•y top-k (v√≠ d·ª• k=5) cho m·∫´u ƒë·∫ßu ti√™n c·ªßa batch, ƒë·ªÉ minh h·ªça \"√¥ x√°c su·∫•t\"\n",
    "    k = 5\n",
    "    top_icd_idx = probs_icd[0].argsort()[-k:][::-1]  # ch·ªâ s·ªë 5 nh√£n ICD c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "    top_lab_idx = probs_lab[0].argsort()[-k:][::-1]  # ch·ªâ s·ªë 5 x√©t nghi·ªám c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "\n",
    "    # In ra ch·ªâ s·ªë v√† x√°c su·∫•t t∆∞∆°ng ·ª©ng\n",
    "    print(\"Top-5 ICD indices:\", top_icd_idx.tolist())\n",
    "    print(\"Top-5 ICD probs  :\", np.round(probs_icd[0][top_icd_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 Lab indices:\", top_lab_idx.tolist())\n",
    "    print(\"Top-5 Lab probs  :\", np.round(probs_lab[0][top_lab_idx], 4).tolist())\n",
    "\n",
    "    # (Tu·ª≥ ch·ªçn) Map ch·ªâ s·ªë -> m√£ ICD-block / itemid x√©t nghi·ªám ƒë·ªÉ d·ªÖ ƒë·ªçc\n",
    "    top_icd_codes = [icd_vocab[i] for i in top_icd_idx]\n",
    "    top_lab_itemids = [lab_vocab_items[j] for j in top_lab_idx]\n",
    "    print(\"Top-5 ICD codes:\", top_icd_codes)\n",
    "    print(\"Top-5 Lab itemids:\", top_lab_itemids)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98f94a",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "B·∫°n s·∫Ω th·∫•y hai ma tr·∫≠n: icd_logits c√≥ k√≠ch th∆∞·ªõc [BATCH_SIZE, n_icd], lab_logits c√≥ k√≠ch th∆∞·ªõc [BATCH_SIZE, n_lab].\n",
    "\n",
    "Sau khi √°p sigmoid, b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c hai ma tr·∫≠n x√°c su·∫•t probs_icd, probs_lab c√πng k√≠ch th∆∞·ªõc, m·ªói c·ªôt l√† ‚Äúm·ªôt √¥‚Äù x√°c su·∫•t m√† b·∫°n m√¥ t·∫£ (v√≠ d·ª• 0.85 cho I21, 0.92 cho Troponin‚Ä¶).\n",
    "\n",
    "ƒê√¢y ch√≠nh l√† ‚Äúph·∫ßn ra quy·∫øt ƒë·ªãnh‚Äù tr∆∞·ªõc khi so s√°nh v·ªõi nh√£n th·∫≠t ·ªü B∆∞·ªõc 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e088b",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 4 ‚Äî So s√°nh v·ªõi nh√£n th·∫≠t (Ground truth)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "L·∫•y d·ª± ƒëo√°n t·ª´ B∆∞·ªõc 3 (logits ‚Üí x√°c su·∫•t).\n",
    "\n",
    "So s√°nh v·ªõi nh√£n th·∫≠t y_icd, y_lab cho m·ªôt m·∫´u trong validation:\n",
    "\n",
    "Xem nh·ªØng nh√£n n√†o m√¥ h√¨nh d·ª± ƒëo√°n (theo ng∆∞·ª°ng 0.5) so v·ªõi nh√£n th·∫≠t.\n",
    "\n",
    "Xem Top-k (v√≠ d·ª• k=5) nh√£n c√≥ x√°c su·∫•t cao nh·∫•t v√† ƒë·ªëi chi·∫øu nh√£n th·∫≠t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cfca6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== So s√°nh theo NG∆Ø·ª†NG 0.5 (m·∫´u idx=0) ===\n",
      "ICD: th·∫≠t=0 | d·ª± ƒëo√°n=90 | ƒë√∫ng=0\n",
      "LAB: th·∫≠t=0 | d·ª± ƒëo√°n=27 | ƒë√∫ng=0\n",
      "\n",
      "=== So s√°nh theo TOP-K ===\n",
      "Top-5 ICD (index ‚Üí m√£ ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\n",
      "  idx=  53 | ICD=790   | p=0.796 | nh√£n_th·∫≠t=0\n",
      "  idx=  72 | ICD=278   | p=0.782 | nh√£n_th·∫≠t=0\n",
      "  idx=  14 | ICD=I48   | p=0.780 | nh√£n_th·∫≠t=0\n",
      "  idx=   9 | ICD=Y92   | p=0.779 | nh√£n_th·∫≠t=0\n",
      "  idx=  79 | ICD=412   | p=0.764 | nh√£n_th·∫≠t=0\n",
      "\n",
      "Top-5 Lab (index ‚Üí itemid ‚Üí t√™n ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\n",
      "  idx=  11 | itemid=50902 | Chloride | p=0.844 | nh√£n_th·∫≠t=0\n",
      "  idx=  44 | itemid=51274 | PT | p=0.807 | nh√£n_th·∫≠t=0\n",
      "  idx=  37 | itemid=51116 | Lymphocytes | p=0.798 | nh√£n_th·∫≠t=0\n",
      "  idx=  28 | itemid=51498 | Specific Gravity | p=0.688 | nh√£n_th·∫≠t=0\n",
      "  idx=   7 | itemid=51279 | Red Blood Cells | p=0.683 | nh√£n_th·∫≠t=0\n",
      "\n",
      "ICD ‚Äî nh√£n_th·∫≠t (m√£): [] ...\n",
      "ICD ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (m√£): ['401', 'Z87', 'E87', 'Y92', 'V15', 'K21', 'I48', 'V45', 'N17', 'G47', 'Z86', '530', '427', 'N18', 'Z20', 'V10', 'Z85', 'F10', '585', 'E66'] ...\n",
      "ICD ‚Äî giao_nhau_ƒë√∫ng (m√£): [] ...\n",
      "\n",
      "LAB ‚Äî nh√£n_th·∫≠t (itemid, t√™n): [] ...\n",
      "LAB ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (itemid, t√™n): [(51250, 'MCV'), (51265, 'Platelet Count'), (51279, 'Red Blood Cells'), (51301, 'White Blood Cells'), (50882, 'Bicarbonate'), (50902, 'Chloride'), (50911, 'Creatine Kinase, MB Isoenzyme'), (50912, 'Creatinine'), (50960, 'Magnesium'), (51006, 'Urea Nitrogen')] ...\n",
      "LAB ‚Äî giao_nhau_ƒë√∫ng (itemid, t√™n): [] ...\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# B∆Ø·ªöC 4 ‚Äî SO S√ÅNH V·ªöI NH√ÉN TH·∫¨T\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) L·∫•y m·ªôt batch t·ª´ validation ƒë·ªÉ so s√°nh\n",
    "X_batch, Y_batch = next(iter(val_ds))  # X_batch: dict input_ids/attention_mask; Y_batch: dict icd/lab\n",
    "\n",
    "# 2) Ch·∫°y m√¥ h√¨nh ƒëa nhi·ªám (t·ª´ B∆∞·ªõc 3) ƒë·ªÉ l·∫•y logits\n",
    "logits = multitask_model.predict(X_batch, verbose=0)\n",
    "icd_log = logits[\"icd\"]   # [BATCH_SIZE, n_icd]\n",
    "lab_log = logits[\"lab\"]   # [BATCH_SIZE, n_lab]\n",
    "\n",
    "# 3) ƒê·ªïi logits -> x√°c su·∫•t (0..1) b·∫±ng sigmoid ƒë·ªÉ d·ªÖ \"ƒë·ªçc\"\n",
    "probs_icd = tf.sigmoid(icd_log).numpy()\n",
    "probs_lab = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "# 4) Ch·ªçn m·ªôt m·∫´u trong batch ƒë·ªÉ nh√¨n chi ti·∫øt (v√≠ d·ª• m·∫´u ƒë·∫ßu ti√™n)\n",
    "idx = 0\n",
    "y_icd_true = Y_batch[\"icd\"][idx].numpy()   # vector multi-hot th·∫≠t [n_icd]\n",
    "y_lab_true = Y_batch[\"lab\"][idx].numpy()   # vector multi-hot th·∫≠t [n_lab]\n",
    "\n",
    "# 5) Chi·∫øn l∆∞·ª£c A: So s√°nh theo NG∆Ø·ª†NG 0.5 (m·∫∑c ƒë·ªãnh)\n",
    "THR_ICD = 0.5\n",
    "THR_LAB = 0.5\n",
    "\n",
    "y_icd_pred_bin = (probs_icd[idx] >= THR_ICD).astype(int)  # 1 n·∫øu x√°c su·∫•t ‚â• 0.5\n",
    "y_lab_pred_bin = (probs_lab[idx] >= THR_LAB).astype(int)\n",
    "\n",
    "true_icd_idx = np.where(y_icd_true == 1)[0]               # c√°c ch·ªâ s·ªë nh√£n ICD th·∫≠t s·ª± c√≥\n",
    "pred_icd_idx = np.where(y_icd_pred_bin == 1)[0]           # c√°c ch·ªâ s·ªë nh√£n ICD m√¥ h√¨nh ch·ªçn theo ng∆∞·ª°ng\n",
    "hit_icd_idx  = np.intersect1d(true_icd_idx, pred_icd_idx) # giao nhau = d·ª± ƒëo√°n ƒë√∫ng\n",
    "\n",
    "true_lab_idx = np.where(y_lab_true == 1)[0]\n",
    "pred_lab_idx = np.where(y_lab_pred_bin == 1)[0]\n",
    "hit_lab_idx  = np.intersect1d(true_lab_idx, pred_lab_idx)\n",
    "\n",
    "print(\"=== So s√°nh theo NG∆Ø·ª†NG 0.5 (m·∫´u idx=0) ===\")\n",
    "print(f\"ICD: th·∫≠t={len(true_icd_idx)} | d·ª± ƒëo√°n={len(pred_icd_idx)} | ƒë√∫ng={len(hit_icd_idx)}\")\n",
    "print(f\"LAB: th·∫≠t={len(true_lab_idx)} | d·ª± ƒëo√°n={len(pred_lab_idx)} | ƒë√∫ng={len(hit_lab_idx)}\")\n",
    "\n",
    "# 6) Chi·∫øn l∆∞·ª£c B: So s√°nh theo TOP-K (v√≠ d·ª• k=5)\n",
    "K_ICD = 5\n",
    "K_LAB = 5\n",
    "\n",
    "top_icd_idx = probs_icd[idx].argsort()[-K_ICD:][::-1]  # ch·ªâ s·ªë K nh√£n ICD c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "top_lab_idx = probs_lab[idx].argsort()[-K_LAB:][::-1]\n",
    "\n",
    "print(\"\\n=== So s√°nh theo TOP-K ===\")\n",
    "print(\"Top-5 ICD (index ‚Üí m√£ ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\")\n",
    "for j in top_icd_idx:\n",
    "    code = icd_vocab[j]\n",
    "    p = float(probs_icd[idx][j])\n",
    "    is_true = int(y_icd_true[j])  # 1 n·∫øu l√† nh√£n th·∫≠t\n",
    "    print(f\"  idx={j:4d} | ICD={code:5s} | p={p:0.3f} | nh√£n_th·∫≠t={is_true}\")\n",
    "\n",
    "print(\"\\nTop-5 Lab (index ‚Üí itemid ‚Üí t√™n ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\")\n",
    "for j in top_lab_idx:\n",
    "    itemid = int(lab_vocab_items[j])\n",
    "    name = itemid_to_label.get(itemid, str(itemid))\n",
    "    p = float(probs_lab[idx][j])\n",
    "    is_true = int(y_lab_true[j])\n",
    "    print(f\"  idx={j:4d} | itemid={itemid} | {name} | p={p:0.3f} | nh√£n_th·∫≠t={is_true}\")\n",
    "\n",
    "# 7) (T√πy ch·ªçn) In danh s√°ch nh√£n ƒë√∫ng theo ng∆∞·ª°ng ƒë·ªÉ h√¨nh dung\n",
    "def map_icd_indices(indices):\n",
    "    return [icd_vocab[i] for i in indices]\n",
    "\n",
    "def map_lab_indices(indices):\n",
    "    return [(int(lab_vocab_items[i]), itemid_to_label.get(int(lab_vocab_items[i]), str(lab_vocab_items[i]))) for i in indices]\n",
    "\n",
    "print(\"\\nICD ‚Äî nh√£n_th·∫≠t (m√£):\", map_icd_indices(true_icd_idx)[:20], \"...\")  # c·∫Øt b·ªõt n·∫øu d√†i\n",
    "print(\"ICD ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (m√£):\", map_icd_indices(pred_icd_idx)[:20], \"...\")\n",
    "print(\"ICD ‚Äî giao_nhau_ƒë√∫ng (m√£):\", map_icd_indices(hit_icd_idx)[:20], \"...\")\n",
    "\n",
    "print(\"\\nLAB ‚Äî nh√£n_th·∫≠t (itemid, t√™n):\", map_lab_indices(true_lab_idx)[:10], \"...\")\n",
    "print(\"LAB ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (itemid, t√™n):\", map_lab_indices(pred_lab_idx)[:10], \"...\")\n",
    "print(\"LAB ‚Äî giao_nhau_ƒë√∫ng (itemid, t√™n):\", map_lab_indices(hit_lab_idx)[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28058",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "B·∫°n th·∫•y r√µ c√°ch ƒë·ªëi chi·∫øu:\n",
    "\n",
    "‚ÄúD·ª± ƒëo√°n theo ng∆∞·ª°ng‚Äù (‚â• 0.5) vs. nh√£n th·∫≠t ‚Üí ƒë·∫øm ƒë√∫ng/sai.\n",
    "\n",
    "‚ÄúD·ª± ƒëo√°n Top-k‚Äù ‚Üí xem 5 nh√£n cao nh·∫•t v√† ƒë√°nh d·∫•u nh√£n n√†o tr√πng nh√£n th·∫≠t.\n",
    "\n",
    "ƒê√¢y l√† b∆∞·ªõc kh√¥ng t√≠nh loss (ch∆∞a hu·∫•n luy·ªán), ch·ªâ so s√°nh ƒë·ªÉ hi·ªÉu d√≤ng ch·∫£y d·ª± ƒëo√°n ‚Üî nh√£n.\n",
    "(Loss s·∫Ω thi·∫øt l·∫≠p ·ªü B∆∞·ªõc 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c195b65",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 5 ‚Äî T√≠nh sai s·ªë (Loss)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "Khai b√°o h√†m m·∫•t m√°t cho 2 nh√°nh ƒë·∫ßu ra (ICD v√† Lab) ƒë√∫ng v·ªõi thi·∫øt k·∫ø ƒëa nh√£n.\n",
    "\n",
    "Bi√™n d·ªãch (compile) m√¥ h√¨nh v·ªõi optimizer/metrics, ch∆∞a hu·∫•n luy·ªán.\n",
    "\n",
    "T√≠nh th·ª≠ loss & AUPRC tr√™n 1 batch ƒë·ªÉ th·∫•y m√¥ h√¨nh ƒëang ·ªü m·ª©c ‚Äúng·∫´u nhi√™n‚Äù tr∆∞·ªõc khi h·ªçc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0ba5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone BioBERT trainable: False\n",
      "K·∫øt qu·∫£ tr√™n 1 batch (tr∆∞·ªõc khi train):\n",
      "  loss: 1.4856\n",
      "  icd_logits_loss: 0.7190\n",
      "  lab_logits_loss: 0.7666\n",
      "  icd_logits_AUPRC: 0.0261\n",
      "  lab_logits_AUPRC: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# B∆Ø·ªöC 5 ‚Äî T√çNH SAI S·ªê (LOSS) & COMPILE\n",
    "# ======================================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) ƒê·ªãnh nghƒ©a loss cho B√ÄI TO√ÅN ƒêA NH√ÉN:\n",
    "#    - BinaryCrossentropy(from_logits=True): ph√π h·ª£p khi ƒë·∫ßu ra l√† \"logits\" (ch∆∞a qua sigmoid)\n",
    "#    - M·ªói nh√°nh c√≥ m·ªôt loss ri√™ng, Keras s·∫Ω c·ªông c√≥ tr·ªçng s·ªë th√†nh 'loss' t·ªïng.\n",
    "losses = {\n",
    "    \"icd\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    \"lab\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "}\n",
    "\n",
    "# 2) Tr·ªçng s·ªë cho t·ª´ng nh√°nh (c√≥ th·ªÉ ch·ªânh n·∫øu mu·ªën ∆∞u ti√™n 1 nh√°nh n√†o ƒë√≥)\n",
    "loss_weights = {\"icd\": 1.0, \"lab\": 1.0}\n",
    "\n",
    "# 3) Ch·ªâ s·ªë ƒë√°nh gi√°: AUPRC (Area Under Precision‚ÄìRecall) d·∫°ng multi-label\n",
    "#    - Cho bi·∫øt ch·∫•t l∆∞·ª£ng m√¥ h√¨nh ·ªü b·ªëi c·∫£nh m·∫•t c√¢n b·∫±ng nh√£n.\n",
    "metrics = {\n",
    "    \"icd\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "    \"lab\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "}\n",
    "\n",
    "# 4) T·ªëi ∆∞u h√≥a: Adam v·ªõi learning rate nh·ªè (fine-tune m√¥ h√¨nh ng√¥n ng·ªØ)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "# üîí ƒê√≥ng bƒÉng ph·∫ßn backbone BioBERT (kh√¥ng hu·∫•n luy·ªán ph·∫ßn n√†y)\n",
    "backbone = next(l for l in multitask_model.layers if \"bert\" in l.name)\n",
    "backbone.trainable = False\n",
    "print(\"Backbone BioBERT trainable:\", backbone.trainable)\n",
    "\n",
    "# 5) Bi√™n d·ªãch (compile) m√¥ h√¨nh\n",
    "multitask_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# 6) T√çNH TH·ª¨ LOSS TR∆Ø·ªöC KHI HU·∫§N LUY·ªÜN\n",
    "#    - L·∫•y 1 batch t·ª´ validation ƒë·ªÉ \"ƒëo\" loss/AUPRC ban ƒë·∫ßu\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "\n",
    "results = multitask_model.evaluate(\n",
    "    X_batch, Y_batch,\n",
    "    verbose=0,            # in g·ªçn\n",
    "    return_dict=True      # tr·∫£ v·ªÅ dict ƒë·ªÉ ƒë·ªçc t√™n tr∆∞·ªùng cho r√µ\n",
    ")\n",
    "\n",
    "print(\"K·∫øt qu·∫£ tr√™n 1 batch (tr∆∞·ªõc khi train):\")\n",
    "for k, v in results.items():\n",
    "    # L√†m tr√≤n cho d·ªÖ nh√¨n\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f2e1c",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 6 ‚Äî C·∫≠p nh·∫≠t (Training)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "Hu·∫•n luy·ªán to√†n b·ªô m√¥ h√¨nh: BioBERT + 2 ƒë·∫ßu ra (ICD, Lab) b·∫±ng Adam.\n",
    "\n",
    "D√πng EarlyStopping ƒë·ªÉ d·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán.\n",
    "\n",
    "L∆∞u checkpoint t·ªët nh·∫•t theo ch·ªâ s·ªë th·∫©m ƒë·ªãnh (validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86192080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "     90/Unknown - 120s 1s/step - loss: 1.3712 - icd_logits_loss: 0.6643 - lab_logits_loss: 0.7069 - icd_logits_AUPRC: 0.0459 - lab_logits_AUPRC: 0.0076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 15:42:01.032424: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-09-08 15:42:18.976215: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_icd_logits_AUPRC improved from -inf to 0.03958, saving model to checkpoints_biobert_multitask/best\n",
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/best/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 171s 2s/step - loss: 1.3712 - icd_logits_loss: 0.6643 - lab_logits_loss: 0.7069 - icd_logits_AUPRC: 0.0459 - lab_logits_AUPRC: 0.0076 - val_loss: 1.2425 - val_icd_logits_loss: 0.6034 - val_lab_logits_loss: 0.6391 - val_icd_logits_AUPRC: 0.0396 - val_lab_logits_AUPRC: 0.0000e+00\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - ETA: 0s - loss: 1.1564 - icd_logits_loss: 0.5652 - lab_logits_loss: 0.5911 - icd_logits_AUPRC: 0.0465 - lab_logits_AUPRC: 0.0053\n",
      "Epoch 2: val_icd_logits_AUPRC did not improve from 0.03958\n",
      "90/90 [==============================] - 130s 1s/step - loss: 1.1564 - icd_logits_loss: 0.5652 - lab_logits_loss: 0.5911 - icd_logits_AUPRC: 0.0465 - lab_logits_AUPRC: 0.0053 - val_loss: 1.0482 - val_icd_logits_loss: 0.5147 - val_lab_logits_loss: 0.5335 - val_icd_logits_AUPRC: 0.0389 - val_lab_logits_AUPRC: 0.0000e+00\n",
      "14/14 [==============================] - 18s 1s/step - loss: 1.2425 - icd_logits_loss: 0.6034 - lab_logits_loss: 0.6391 - icd_logits_AUPRC: 0.0396 - lab_logits_AUPRC: 0.0000e+00\n",
      "\n",
      "ƒê√°nh gi√° cu·ªëi c√πng tr√™n validation:\n",
      "  loss: 1.2425\n",
      "  icd_logits_loss: 0.6034\n",
      "  lab_logits_loss: 0.6391\n",
      "  icd_logits_AUPRC: 0.0396\n",
      "  lab_logits_AUPRC: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 15:45:19.949599: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh hi·ªán t·∫°i t·∫°i: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/notebooks/checkpoints_biobert_multitask/final\n",
      "Checkpoint t·ªët nh·∫•t (theo val_icd_logits_AUPRC) t·∫°i: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/notebooks/checkpoints_biobert_multitask/best\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# B∆Ø·ªöC 6 ‚Äî HU·∫§N LUY·ªÜN (TRAINING)\n",
    "# ==================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "EPOCHS = 3                # s·ªë v√≤ng l·∫∑p qua to√†n b·ªô d·ªØ li·ªáu\n",
    "OUT_DIR = Path(\"checkpoints_biobert_multitask\")  # n∆°i l∆∞u checkpoint t·ªët nh·∫•t\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) EarlyStopping: d·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán AUPRC tr√™n validation\n",
    "#    - B·∫°n c√≥ th·ªÉ ch·ªçn theo nh√°nh \"icd\" ho·∫∑c \"lab\". ·ªû ƒë√¢y l·∫•y \"icd\" l√†m ti√™u ch√≠.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_icd_logits_AUPRC\",  # t√™n metric do Keras t·ª± ƒë·∫∑t: <output>_<metric>\n",
    "    mode=\"max\",\n",
    "    patience=1,                      # n·∫øu 1 epoch kh√¥ng c·∫£i thi·ªán th√¨ d·ª´ng\n",
    "    restore_best_weights=True        # quay v·ªÅ tr·ªçng s·ªë t·ªët nh·∫•t\n",
    ")\n",
    "\n",
    "# 2) ModelCheckpoint: l∆∞u checkpoint c√≥ AUPRC t·ªët nh·∫•t\n",
    "ckpt_path = OUT_DIR / \"best\"\n",
    "ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(ckpt_path),\n",
    "    monitor=\"val_icd_logits_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,         # l∆∞u c·∫£ c·∫•u tr√∫c + weights (SavedModel)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3) Hu·∫•n luy·ªán\n",
    "history = multitask_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop, ckpt_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4) Sau khi fit, evaluate l·∫°i tr√™n validation ƒë·ªÉ xem t·ªïng quan\n",
    "final_eval = multitask_model.evaluate(val_ds, return_dict=True, verbose=1)\n",
    "print(\"\\nƒê√°nh gi√° cu·ªëi c√πng tr√™n validation:\")\n",
    "for k, v in final_eval.items():\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# 5) L∆∞u th√™m m·ªôt b·∫£n \"final\" (ph√≤ng khi mu·ªën d√πng ngay phi√™n b·∫£n hi·ªán t·∫°i)\n",
    "final_path = OUT_DIR / \"final\"\n",
    "multitask_model.save(final_path)\n",
    "print(f\"\\nƒê√£ l∆∞u m√¥ h√¨nh hi·ªán t·∫°i t·∫°i: {final_path.resolve()}\")\n",
    "print(f\"Checkpoint t·ªët nh·∫•t (theo val_icd_logits_AUPRC) t·∫°i: {ckpt_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f250e",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "loss, icd_loss, lab_loss tr√™n validation s·∫Ω gi·∫£m d·∫ßn qua epoch; AUPRC s·∫Ω tƒÉng d·∫ßn n·∫øu d·ªØ li·ªáu/ti·ªÅn x·ª≠ l√Ω ·ªïn.\n",
    "\n",
    "Sau khi hu·∫•n luy·ªán, m√¥ h√¨nh b·∫Øt ƒë·∫ßu d·ª± ƒëo√°n h·ª£p l√Ω h∆°n (√≠t ‚Äútr√†n nh√£n‚Äù ‚â• 0.5, top-k s√°t th·ª±c t·∫ø h∆°n)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
