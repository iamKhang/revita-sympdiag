{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b197f8ef",
   "metadata": {},
   "source": [
    "Cấu hình và phụ thuộc\n",
    "\n",
    "Mục đích: thiết lập tham số huấn luyện và thư viện cần dùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814d776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lehoangkhang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer OK: BertTokenizerFast\n",
      "Model OK: TFBertModel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "print(\"Tokenizer OK:\", type(tok).__name__)\n",
    "print(\"Model OK:\", type(bert).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0cbcb",
   "metadata": {},
   "source": [
    "BƯỚC 1 — Đưa đầu vào (Input)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Đọc dữ liệu đã tiền xử lý (examples.parquet, vocab_meta.json).\n",
    "\n",
    "Biến văn bản text thành token ID và attention mask mà BioBERT hiểu (tokenizer của BioBERT).\n",
    "\n",
    "Tạo tf.data.Dataset cho train và validation (chỉ chuẩn bị đầu vào, chưa xây mô hình)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang dùng:\n",
      " - PARQUET_PATH: /Users/lehoangkhang/Tài liệu/revita-sympdiag/data/proc/examples.parquet\n",
      " - VOCAB_PATH  : /Users/lehoangkhang/Tài liệu/revita-sympdiag/data/proc/vocab_meta.json\n",
      "Số mẫu: 10000 | n_icd: 50 | n_proc: 50 | n_lab: 50\n",
      "Số mẫu train: 8499\n",
      "Số mẫu val  : 1501\n",
      "TAB_DIM (age_norm + gender_onehot): 4\n",
      "Kích thước đầu vào (X): {'input_ids': TensorShape([8, 256]), 'attention_mask': TensorShape([8, 256]), 'tab_feats': TensorShape([8, 4])}\n",
      "Kích thước nhãn (Y):    {'icd': TensorShape([8, 50]), 'proc': TensorShape([8, 50]), 'lab': TensorShape([8, 50])}\n",
      "Ví dụ text gốc (rút gọn): Chief Complaint: altered mental status Major Surgical or Invasive Procedure: ___ Paracentesis ___ ERCP History of Presen\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BƯỚC 1 — ĐƯA ĐẦU VÀO (INPUT)  [CẬP NHẬT: thêm age & gender]\n",
    "# =========================\n",
    "\n",
    "# 1) Import các thư viện cần thiết\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 2) Cấu hình cơ bản cho việc tạo batch\n",
    "MAX_LENGTH = 512   # độ dài tối đa của chuỗi token (cắt nếu dài hơn, pad nếu ngắn hơn)\n",
    "BATCH_SIZE = 8     # kích thước lô (batch)\n",
    "VAL_RATIO  = 0.15  # tỉ lệ dành cho validation\n",
    "SEED = 42          # hạt giống ngẫu nhiên để tái lập kết quả\n",
    "\n",
    "# 3) Đường dẫn dữ liệu đã tiền xử lý (đã bao gồm gender, age_at_admit)\n",
    "DATA_ROOT = Path(\"..\") / \"data\" / \"proc\"\n",
    "PARQUET_PATH = DATA_ROOT / \"examples.parquet\"\n",
    "VOCAB_PATH   = DATA_ROOT / \"vocab_meta.json\"\n",
    "\n",
    "print(\"Đang dùng:\")\n",
    "print(\" - PARQUET_PATH:\", PARQUET_PATH.resolve())\n",
    "print(\" - VOCAB_PATH  :\", VOCAB_PATH.resolve())\n",
    "\n",
    "# 4) Kiểm tra sự tồn tại của file dữ liệu\n",
    "if not PARQUET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Không tìm thấy {PARQUET_PATH}. Hãy chạy Notebook tiền xử lý để sinh file, \"\n",
    "        f\"hoặc chỉnh lại đường dẫn cho đúng vị trí thực tế.\"\n",
    "    )\n",
    "if not VOCAB_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Không tìm thấy {VOCAB_PATH}. Hãy kiểm tra Notebook tiền xử lý hoặc đường dẫn.\"\n",
    "    )\n",
    "\n",
    "# 5) Kiểm soát ngẫu nhiên để kết quả ổn định\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 6) Đọc dữ liệu bảng đã tiền xử lý và metadata nhãn\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "icd_vocab        = meta[\"icd_vocab\"]              # danh sách ICD-block (vocab)\n",
    "lab_vocab_items  = meta[\"lab_vocab_items\"]        # danh sách itemid xét nghiệm (vocab)\n",
    "proc_vocab = meta[\"proc_vocab\"]     \n",
    "itemid_to_label  = {int(k): v for k, v in meta[\"itemid_to_label\"].items()}\n",
    "\n",
    "n_icd = len(icd_vocab)\n",
    "n_lab = len(lab_vocab_items)\n",
    "n_proc = len(proc_vocab)\n",
    "\n",
    "print(f\"Số mẫu: {len(df)} | n_icd: {n_icd} | n_proc: {n_proc} | n_lab: {n_lab}\")\n",
    "\n",
    "# 7) Đảm bảo 2 cột nhãn là mảng float32 để dùng với BinaryCrossentropy\n",
    "df[\"y_icd\"]  = df[\"y_icd\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "df[\"y_proc\"] = df[\"y_proc\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "df[\"y_lab\"]  = df[\"y_lab\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "\n",
    "# 7.1) [MỚI] Chuẩn hoá demographics -> tabular features\n",
    "# - gender -> one-hot (M/F/U) => 3 chiều\n",
    "# - age_at_admit -> min-max [0..1] bằng cách chia 120 => 1 chiều\n",
    "# Tổng số đặc trưng tabular: TAB_DIM = 4\n",
    "def _normalize_gender(g):\n",
    "    g = (str(g).upper().strip()[:1] if pd.notna(g) else \"U\")\n",
    "    return g if g in (\"M\", \"F\", \"U\") else \"U\"\n",
    "\n",
    "df[\"gender\"] = df[\"gender\"].apply(_normalize_gender)\n",
    "df[\"age_at_admit\"] = df[\"age_at_admit\"].astype(\"float32\")\n",
    "\n",
    "# one-hot cho gender\n",
    "gender_to_idx = {\"M\": 0, \"F\": 1, \"U\": 2}\n",
    "idx = df[\"gender\"].map(gender_to_idx).fillna(2).astype(int).values\n",
    "gender_onehot = np.eye(3, dtype=np.float32)[idx]   # shape [N, 3]\n",
    "\n",
    "# age chuẩn hoá (0..120) -> 0..1; thiếu thì 0\n",
    "age_norm = (df[\"age_at_admit\"].fillna(0.0).clip(0, 120) / 120.0).astype(\"float32\").values.reshape(-1, 1)\n",
    "\n",
    "# ghép thành vector tabular 4 chiều\n",
    "tab_feats = np.concatenate([age_norm, gender_onehot], axis=1).astype(\"float32\")  # [N, 4]\n",
    "TAB_DIM = tab_feats.shape[1]\n",
    "\n",
    "# 8) Chia train/validation theo subject_id (tránh leakage người bệnh)\n",
    "groups = df[\"subject_id\"].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=groups))\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# cắt tương ứng tab_feats\n",
    "train_tab = tab_feats[train_idx]\n",
    "val_tab   = tab_feats[val_idx]\n",
    "\n",
    "print(\"Số mẫu train:\", len(train_df))\n",
    "print(\"Số mẫu val  :\", len(val_df))\n",
    "print(\"TAB_DIM (age_norm + gender_onehot):\", TAB_DIM)\n",
    "\n",
    "# 9) Chuẩn bị tokenizer của BioBERT\n",
    "try:\n",
    "    tokenizer = tok  # tái dùng tokenizer đã nạp ở cell trước (nếu có)\n",
    "except NameError:\n",
    "    MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 10) Hàm mã hóa 1 lô văn bản sang token ID & attention mask (đầu vào mô hình)\n",
    "def encode_text_batch(text_list, max_length):\n",
    "    enc = tokenizer(\n",
    "        list(text_list),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    enc[\"input_ids\"] = np.asarray(enc[\"input_ids\"], dtype=np.int32)\n",
    "    enc[\"attention_mask\"] = np.asarray(enc[\"attention_mask\"], dtype=np.int32)\n",
    "    return enc\n",
    "\n",
    "# 11) Tạo tf.data.Dataset theo kiểu generator (thêm 'tab_feats' vào X)\n",
    "def make_tfds_from_df(frame, tab_array, batch_size, max_length):\n",
    "    texts = frame[\"text\"].fillna(\"\").tolist()\n",
    "    y_icd = np.stack(frame[\"y_icd\"].to_list()).astype(np.float32)\n",
    "    y_proc = np.stack(frame[\"y_proc\"].to_list()).astype(np.float32)   # <<-- mới\n",
    "    y_lab = np.stack(frame[\"y_lab\"].to_list()).astype(np.float32)\n",
    "\n",
    "    def gen():\n",
    "        N = len(texts)\n",
    "        start = 0\n",
    "        while start < N:\n",
    "            end = min(start + batch_size, N)\n",
    "            enc = encode_text_batch(texts[start:end], max_length=max_length)\n",
    "            for i in range(end - start):\n",
    "                yield (\n",
    "                    {\n",
    "                        \"input_ids\": enc[\"input_ids\"][i],\n",
    "                        \"attention_mask\": enc[\"attention_mask\"][i],\n",
    "                        \"tab_feats\": tab_array[start + i],\n",
    "                    },\n",
    "                    {\n",
    "                        \"icd\": y_icd[start + i],\n",
    "                        \"proc\": y_proc[start + i],    # <<-- mới\n",
    "                        \"lab\": y_lab[start + i],\n",
    "                    }\n",
    "                )\n",
    "            start = end\n",
    "\n",
    "    output_signature = (\n",
    "        {\n",
    "            \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"tab_feats\": tf.TensorSpec(shape=(TAB_DIM,), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            \"icd\": tf.TensorSpec(shape=(n_icd,), dtype=tf.float32),\n",
    "            \"proc\": tf.TensorSpec(shape=(n_proc,), dtype=tf.float32),  # <<-- mới\n",
    "            \"lab\": tf.TensorSpec(shape=(n_lab,), dtype=tf.float32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(\n",
    "            {\n",
    "                \"input_ids\": [MAX_LENGTH],\n",
    "                \"attention_mask\": [MAX_LENGTH],\n",
    "                \"tab_feats\": [TAB_DIM],\n",
    "            },\n",
    "            {\n",
    "                \"icd\": [n_icd],\n",
    "                \"proc\": [n_proc],   # <<-- mới\n",
    "                \"lab\": [n_lab],\n",
    "            },\n",
    "        ),\n",
    "        padding_values=(\n",
    "            {\n",
    "                \"input_ids\": tf.constant(0, tf.int32),\n",
    "                \"attention_mask\": tf.constant(0, tf.int32),\n",
    "                \"tab_feats\": tf.constant(0.0, tf.float32),\n",
    "            },\n",
    "            {\n",
    "                \"icd\": tf.constant(0.0, tf.float32),\n",
    "                \"proc\": tf.constant(0.0, tf.float32),  # <<-- mới\n",
    "                \"lab\": tf.constant(0.0, tf.float32),\n",
    "            },\n",
    "        ),\n",
    "        drop_remainder=False\n",
    "    )\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 12) Tạo train_ds và val_ds (đã kèm tab_feats)\n",
    "train_ds = make_tfds_from_df(train_df, train_tab, BATCH_SIZE, MAX_LENGTH).shuffle(1024)\n",
    "val_ds   = make_tfds_from_df(val_df,   val_tab,   BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "# 13) Kiểm tra nhanh một batch\n",
    "for batch in train_ds.take(1):\n",
    "    X, Y = batch\n",
    "    print(\"Kích thước đầu vào (X):\", {k: v.shape for k, v in X.items()})\n",
    "    print(\"Kích thước nhãn (Y):   \", {k: v.shape for k, v in Y.items()})\n",
    "    # gợi ý: text gốc vẫn xem từ train_df\n",
    "    print(\"Ví dụ text gốc (rút gọn):\", train_df.iloc[0][\"text\"][:120].replace(\"\\n\", \" \"))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6b456",
   "metadata": {},
   "source": [
    "BƯỚC 2 — Xử lý trong BioBERT (tạo vector ngữ nghĩa)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Đưa batch input_ids + attention_mask vào BioBERT.\n",
    "\n",
    "Lấy ra vector biểu diễn câu (dùng token đặc biệt [CLS]).\n",
    "\n",
    "Kiểm tra kích thước và giá trị điển hình để xác nhận “dịch văn bản → ngôn ngữ số”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ceb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước vector CLS: (8, 768)\n",
      "8 số đầu của CLS[0]: [-0.0006000000284984708, -0.07100000232458115, -0.48420000076293945, -0.15230000019073486, -0.32659998536109924, 0.133200004696846, 0.16040000319480896, -0.23240000009536743]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BƯỚC 2 — XỬ LÝ TRONG BIOBERT (CẬP NHẬT)\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "DROPOUT = 0.1  # dropout nhẹ cho CLS\n",
    "\n",
    "# 2.1 Nạp backbone BioBERT (TF). Nếu đã có biến `bert` từ trước thì tái dùng.\n",
    "try:\n",
    "    _ = bert  # đã có sẵn\n",
    "except NameError:\n",
    "    bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "# 2.2 Inputs của encoder khớp với tf.data: CHỈ gồm input_ids & attention_mask\n",
    "input_ids      = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# 2.3 Forward qua BioBERT, lấy CLS\n",
    "bert_outputs = bert(input_ids, attention_mask=attention_mask, training=False)\n",
    "cls_vec = bert_outputs.last_hidden_state[:, 0, :]  # [B, H]\n",
    "cls_vec = tf.keras.layers.Dropout(DROPOUT, name=\"cls_dropout\")(cls_vec)\n",
    "\n",
    "# 2.4 Đóng gói encoder (text-only). Fusion với tab_feats sẽ làm ở bước sau.\n",
    "encoder = tf.keras.Model(\n",
    "    inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "    outputs={\"cls\": cls_vec},\n",
    "    name=\"biobert_encoder_cls\"\n",
    ")\n",
    "\n",
    "# 2.5 Kiểm tra nhanh 1 batch\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    # CHỈ truyền 2 khóa mà encoder mong đợi, không đưa 'tab_feats'\n",
    "    enc_inp = {\n",
    "        \"input_ids\": X_batch[\"input_ids\"],\n",
    "        \"attention_mask\": X_batch[\"attention_mask\"],\n",
    "    }\n",
    "    out = encoder(enc_inp, training=False)\n",
    "    cls_batch = out[\"cls\"].numpy()\n",
    "    print(\"Kích thước vector CLS:\", cls_batch.shape)\n",
    "    print(\"8 số đầu của CLS[0]:\", np.round(cls_batch[0][:8], 4).tolist())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aa579",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "Bạn sẽ nhận được ma trận kích thước [BATCH_SIZE, HIDDEN] (thường HIDDEN = 768).\n",
    "\n",
    "Mỗi hàng là “dấu vân tay số học” của 1 văn bản — chứa ngữ cảnh y khoa đã được BioBERT “đọc hiểu”.\n",
    "\n",
    "Đây chính là đầu vào cho “phần ra quyết định” ở Bước 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b908",
   "metadata": {},
   "source": [
    "BƯỚC 3 — Classifier head (Phần ra quyết định)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Gắn 2 “đầu ra quyết định” (Dense) lên vector CLS từ BioBERT:\n",
    "\n",
    "Đầu ICD: dự đoán đa nhãn các ICD-block.\n",
    "\n",
    "Đầu Lab: dự đoán đa nhãn các xét nghiệm sớm.\n",
    "\n",
    "Mỗi đầu trả về logits (số thực âm/dương). Khi qua sigmoid sẽ thành xác suất 0–1 cho từng nhãn.\n",
    "\n",
    "Kiểm tra kích thước đầu ra và xem thử top-k dự đoán trên một batch (chưa huấn luyện, chỉ để minh họa dòng chảy dữ liệu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e1308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"biobert_multitask_fusion\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                        Param #     Connected to                        \n",
      "========================================================================================================================\n",
      " input_ids (InputLayer)             [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " attention_mask (InputLayer)        [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " tf_bert_model (TFBertModel)        TFBaseModelOutputWithPoolingAndCr   108310272   ['input_ids[0][0]',                 \n",
      "                                    ossAttentions(last_hidden_state=(                'attention_mask[0][0]']            \n",
      "                                    None, None, 768),                                                                   \n",
      "                                     pooler_output=(None, 768),                                                         \n",
      "                                     past_key_values=None, hidden_sta                                                   \n",
      "                                    tes=None, attentions=None, cross_                                                   \n",
      "                                    attentions=None)                                                                    \n",
      "                                                                                                                        \n",
      " tf.__operators__.getitem (Slicing  (None, 768)                         0           ['tf_bert_model[0][0]']             \n",
      " OpLambda)                                                                                                              \n",
      "                                                                                                                        \n",
      " cls_dropout (Dropout)              (None, 768)                         0           ['tf.__operators__.getitem[0][0]']  \n",
      "                                                                                                                        \n",
      " tab_feats (InputLayer)             [(None, 4)]                         0           []                                  \n",
      "                                                                                                                        \n",
      " fuse_cls_tab (Concatenate)         (None, 772)                         0           ['cls_dropout[0][0]',               \n",
      "                                                                                     'tab_feats[0][0]']                 \n",
      "                                                                                                                        \n",
      " fuse_dropout (Dropout)             (None, 772)                         0           ['fuse_cls_tab[0][0]']              \n",
      "                                                                                                                        \n",
      " fuse_dense (Dense)                 (None, 512)                         395776      ['fuse_dropout[0][0]']              \n",
      "                                                                                                                        \n",
      " icd_logits (Dense)                 (None, 50)                          25650       ['fuse_dense[0][0]']                \n",
      "                                                                                                                        \n",
      " lab_logits (Dense)                 (None, 50)                          25650       ['fuse_dense[0][0]']                \n",
      "                                                                                                                        \n",
      " proc_logits (Dense)                (None, 50)                          25650       ['fuse_dense[0][0]']                \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 108782998 (414.97 MB)\n",
      "Trainable params: 108782998 (414.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "________________________________________________________________________________________________________________________\n",
      "Kích thước icd_logits: (8, 50)\n",
      "Kích thước lab_logits: (8, 50)\n",
      "Top-5 ICD indices: [25, 32, 26, 1, 45]\n",
      "Top-5 ICD probs  : [0.8086000084877014, 0.7973999977111816, 0.7763000130653381, 0.7738000154495239, 0.7692000269889832]\n",
      "Top-5 Lab indices: [40, 21, 5, 4, 48]\n",
      "Top-5 Lab probs  : [0.8431000113487244, 0.79830002784729, 0.7853000164031982, 0.7466999888420105, 0.70660001039505]\n",
      "Top-5 ICD codes: ['41401', 'E871', '53081', 'I10', 'Z794']\n",
      "Top-5 Lab itemids: ['51200', '51678', '51006', '51221', '52073']\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BƯỚC 3 — CLASSIFIER HEAD (CẬP NHẬT: fusion CLS + tab_feats)\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 3.1 Lấy đầu ra CLS từ encoder (Bước 2)\n",
    "cls_output = encoder.outputs[0]  # [batch, hidden_size], vd 768\n",
    "\n",
    "# 3.2 Khai báo thêm input tabular để khớp với tf.data (Bước 1 đã tạo 'tab_feats')\n",
    "tab_input = tf.keras.Input(shape=(TAB_DIM,), dtype=tf.float32, name=\"tab_feats\")  # [batch, TAB_DIM]\n",
    "\n",
    "# 3.3 Fusion: ghép CLS + tab_feats (có thể thêm MLP nhỏ để ổn định)\n",
    "fused = tf.keras.layers.Concatenate(name=\"fuse_cls_tab\")([cls_output, tab_input])  # [batch, 768+TAB_DIM]\n",
    "fused = tf.keras.layers.Dropout(0.1, name=\"fuse_dropout\")(fused)\n",
    "fused = tf.keras.layers.Dense(512, activation=\"relu\", name=\"fuse_dense\")(fused)\n",
    "\n",
    "# 3.4 Ba nhánh logits cho đa nhãn\n",
    "icd_logits  = tf.keras.layers.Dense(n_icd,  name=\"icd_logits\")(fused)\n",
    "proc_logits = tf.keras.layers.Dense(n_proc, name=\"proc_logits\")(fused)   # <<-- mới\n",
    "lab_logits  = tf.keras.layers.Dense(n_lab,  name=\"lab_logits\")(fused)\n",
    "\n",
    "# 3.5 Mô hình đa nhiệm hoàn chỉnh\n",
    "multitask_model = tf.keras.Model(\n",
    "    inputs={\n",
    "        \"input_ids\": encoder.inputs[0],      # thay vì encoder.inputs[\"input_ids\"]\n",
    "        \"attention_mask\": encoder.inputs[1], # thay vì encoder.inputs[\"attention_mask\"]\n",
    "        \"tab_feats\": tab_input,\n",
    "    },\n",
    "    outputs={\n",
    "        \"icd\": icd_logits,\n",
    "        \"proc\": proc_logits,\n",
    "        \"lab\": lab_logits,\n",
    "    },\n",
    "    name=\"biobert_multitask_fusion\"\n",
    ")\n",
    "\n",
    "\n",
    "# 3.6 Kiểm tra kiến trúc\n",
    "multitask_model.summary(line_length=120)\n",
    "\n",
    "# 3.7 Chạy thử một batch (X_batch đã có 'input_ids', 'attention_mask', 'tab_feats')\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    logits = multitask_model.predict(X_batch, verbose=0)\n",
    "    icd_log = logits[\"icd\"]\n",
    "    lab_log = logits[\"lab\"]\n",
    "    print(\"Kích thước icd_logits:\", icd_log.shape)\n",
    "    print(\"Kích thước lab_logits:\", lab_log.shape)\n",
    "\n",
    "    # Đổi sang xác suất để quan sát\n",
    "    probs_icd = tf.sigmoid(icd_log).numpy()\n",
    "    probs_lab = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "    k = 5\n",
    "    top_icd_idx = probs_icd[0].argsort()[-k:][::-1]\n",
    "    top_lab_idx = probs_lab[0].argsort()[-k:][::-1]\n",
    "    print(\"Top-5 ICD indices:\", top_icd_idx.tolist())\n",
    "    print(\"Top-5 ICD probs  :\", np.round(probs_icd[0][top_icd_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 Lab indices:\", top_lab_idx.tolist())\n",
    "    print(\"Top-5 Lab probs  :\", np.round(probs_lab[0][top_lab_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 ICD codes:\", [icd_vocab[i] for i in top_icd_idx])\n",
    "    print(\"Top-5 Lab itemids:\", [lab_vocab_items[j] for j in top_lab_idx])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98f94a",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "Bạn sẽ thấy hai ma trận: icd_logits có kích thước [BATCH_SIZE, n_icd], lab_logits có kích thước [BATCH_SIZE, n_lab].\n",
    "\n",
    "Sau khi áp sigmoid, bạn sẽ nhận được hai ma trận xác suất probs_icd, probs_lab cùng kích thước, mỗi cột là “một ô” xác suất mà bạn mô tả (ví dụ 0.85 cho I21, 0.92 cho Troponin…).\n",
    "\n",
    "Đây chính là “phần ra quyết định” trước khi so sánh với nhãn thật ở Bước 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e088b",
   "metadata": {},
   "source": [
    "BƯỚC 4 — So sánh với nhãn thật (Ground truth)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Lấy dự đoán từ Bước 3 (logits → xác suất).\n",
    "\n",
    "So sánh với nhãn thật y_icd, y_lab cho một mẫu trong validation:\n",
    "\n",
    "Xem những nhãn nào mô hình dự đoán (theo ngưỡng 0.5) so với nhãn thật.\n",
    "\n",
    "Xem Top-k (ví dụ k=5) nhãn có xác suất cao nhất và đối chiếu nhãn thật."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cfca6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== So sánh theo NGƯỠNG 0.5 ===\n",
      "ICD : 1 33 0\n",
      "PROC: 1 30 1\n",
      "LAB : 2 21 2\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# BƯỚC 4 — SO SÁNH VỚI NHÃN THẬT (CẬP NHẬT: thêm thủ thuật)\n",
    "# ===============================\n",
    "\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "logits = multitask_model.predict(X_batch, verbose=0)\n",
    "\n",
    "icd_log  = logits[\"icd\"]\n",
    "proc_log = logits[\"proc\"]    # <<-- mới\n",
    "lab_log  = logits[\"lab\"]\n",
    "\n",
    "probs_icd  = tf.sigmoid(icd_log).numpy()\n",
    "probs_proc = tf.sigmoid(proc_log).numpy()  # <<-- mới\n",
    "probs_lab  = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "idx = 0\n",
    "y_icd_true  = Y_batch[\"icd\"][idx].numpy()\n",
    "y_proc_true = Y_batch[\"proc\"][idx].numpy()  # <<-- mới\n",
    "y_lab_true  = Y_batch[\"lab\"][idx].numpy()\n",
    "\n",
    "THR = 0.5\n",
    "y_icd_pred  = (probs_icd[idx]  >= THR).astype(int)\n",
    "y_proc_pred = (probs_proc[idx] >= THR).astype(int)  # <<-- mới\n",
    "y_lab_pred  = (probs_lab[idx]  >= THR).astype(int)\n",
    "\n",
    "def stats(y_true, y_pred):  \n",
    "    t, p = np.where(y_true==1)[0], np.where(y_pred==1)[0]\n",
    "    return len(t), len(p), len(np.intersect1d(t,p))\n",
    "\n",
    "print(\"=== So sánh theo NGƯỠNG 0.5 ===\")\n",
    "print(\"ICD :\", *stats(y_icd_true,  y_icd_pred))\n",
    "print(\"PROC:\", *stats(y_proc_true, y_proc_pred))\n",
    "print(\"LAB :\", *stats(y_lab_true,  y_lab_pred))\n",
    "\n",
    "# Có thể thêm so sánh Top-K tương tự nếu cần, chỉ thêm phần PROC theo mẫu ICD/LAB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28058",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "Bạn thấy rõ cách đối chiếu:\n",
    "\n",
    "“Dự đoán theo ngưỡng” (≥ 0.5) vs. nhãn thật → đếm đúng/sai.\n",
    "\n",
    "“Dự đoán Top-k” → xem 5 nhãn cao nhất và đánh dấu nhãn nào trùng nhãn thật.\n",
    "\n",
    "Đây là bước không tính loss (chưa huấn luyện), chỉ so sánh để hiểu dòng chảy dự đoán ↔ nhãn.\n",
    "(Loss sẽ thiết lập ở Bước 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c195b65",
   "metadata": {},
   "source": [
    "BƯỚC 5 — Tính sai số (Loss)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Khai báo hàm mất mát cho 2 nhánh đầu ra (ICD và Lab) đúng với thiết kế đa nhãn.\n",
    "\n",
    "Biên dịch (compile) mô hình với optimizer/metrics, chưa huấn luyện.\n",
    "\n",
    "Tính thử loss & AUPRC trên 1 batch để thấy mô hình đang ở mức “ngẫu nhiên” trước khi học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0ba5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> TRAIN: Fine-tune toàn bộ BioBERT (không đóng băng)\n",
      "==> Train từ đầu (chưa có checkpoint)\n",
      "Kết quả trên 1 batch (trước khi train):\n",
      "  loss: 2.2841\n",
      "  icd_logits_loss: 0.8452\n",
      "  lab_logits_loss: 0.6880\n",
      "  proc_logits_loss: 0.7509\n",
      "  icd_logits_AUPRC: 0.0644\n",
      "  lab_logits_AUPRC: 0.0632\n",
      "  proc_logits_AUPRC: 0.0688\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# BƯỚC 5 — TÍNH SAI SỐ (LOSS) & COMPILE (LUÔN TRAIN BIOBERT)\n",
    "# ======================================\n",
    "\n",
    "LR = 2e-5\n",
    "\n",
    "# 1) Loss cho 3 nhánh\n",
    "losses = {\n",
    "    \"icd\":  tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    \"proc\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    \"lab\":  tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "}\n",
    "\n",
    "# 2) Trọng số\n",
    "loss_weights = {\"icd\": 1.0, \"proc\": 1.0, \"lab\": 1.0}\n",
    "\n",
    "# 3) Metrics\n",
    "metrics = {\n",
    "    \"icd\":  [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "    \"proc\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "    \"lab\":  [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "}\n",
    "\n",
    "# 4) Optimizer (luôn fine-tune)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "print(\"==> TRAIN: Fine-tune toàn bộ BioBERT (không đóng băng)\")\n",
    "\n",
    "# 5) Compile\n",
    "multitask_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# 6) Resume checkpoint\n",
    "OUT_DIR = Path(\"checkpoints_biobert_multitask\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESUME_PATH = OUT_DIR / \"last\"\n",
    "\n",
    "if RESUME_PATH.exists():\n",
    "    print(f\"==> Resume training từ checkpoint: {RESUME_PATH}\")\n",
    "    multitask_model = tf.keras.models.load_model(RESUME_PATH)\n",
    "else:\n",
    "    print(\"==> Train từ đầu (chưa có checkpoint)\")\n",
    "\n",
    "# 7) Đánh giá nhanh\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "results = multitask_model.evaluate(X_batch, Y_batch, verbose=0, return_dict=True)\n",
    "print(\"Kết quả trên 1 batch (trước khi train):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, (int, float)) else f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f2e1c",
   "metadata": {},
   "source": [
    "BƯỚC 6 — Cập nhật (Training)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Huấn luyện toàn bộ mô hình: BioBERT + 2 đầu ra (ICD, Lab) bằng Adam.\n",
    "\n",
    "Dùng EarlyStopping để dừng sớm nếu không cải thiện.\n",
    "\n",
    "Lưu checkpoint tốt nhất theo chỉ số thẩm định (validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86192080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# BƯỚC 6 — HUẤN LUYỆN (TRAINING) [CẬP NHẬT: hỗ trợ 3 nhánh ICD/PROC/LAB]\n",
    "# ==================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv, os\n",
    "\n",
    "OUT_DIR = Path(\"checkpoints_biobert_multitask\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Detect epoch đã train từ log ===\n",
    "initial_epoch = 0\n",
    "log_path = OUT_DIR / \"train_log.csv\"\n",
    "if log_path.exists():\n",
    "    try:\n",
    "        df_log = pd.read_csv(log_path)\n",
    "        if \"epoch\" in df_log.columns:\n",
    "            initial_epoch = int(df_log[\"epoch\"].max()) + 1\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Không đọc được train_log.csv:\", e)\n",
    "\n",
    "# Train thêm đúng 1 epoch\n",
    "target_epochs = initial_epoch + 1\n",
    "print(f\"==> Resume training từ epoch {initial_epoch} đến {target_epochs}\")\n",
    "\n",
    "\n",
    "# === Callback custom để log mỗi 1000 batch ===\n",
    "class BatchLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_file, every_n=1000):\n",
    "        super().__init__()\n",
    "        self.log_file = log_file\n",
    "        self.every_n = every_n\n",
    "        # nếu file chưa tồn tại thì tạo header\n",
    "        if not os.path.exists(log_file):\n",
    "            with open(log_file, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    \"epoch\", \"batch\",\n",
    "                    \"loss\", \"icd_loss\", \"proc_loss\", \"lab_loss\",\n",
    "                    \"icd_AUPRC\", \"proc_AUPRC\", \"lab_AUPRC\"\n",
    "                ])\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = initial_epoch + epoch  # đảm bảo liên tục khi resume\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if (batch + 1) % self.every_n == 0:\n",
    "            row = [\n",
    "                self.epoch, batch + 1,\n",
    "                logs.get(\"loss\"),\n",
    "                logs.get(\"icd_loss\"), logs.get(\"proc_loss\"), logs.get(\"lab_loss\"),\n",
    "                logs.get(\"icd_AUPRC\"), logs.get(\"proc_AUPRC\"), logs.get(\"lab_AUPRC\")\n",
    "            ]\n",
    "            # In ra màn hình\n",
    "            print(f\"[Epoch {self.epoch} | Batch {batch+1}] \"\n",
    "                  f\"loss={row[2]:.4f}, icd={row[3]:.4f}, proc={row[4]:.4f}, lab={row[5]:.4f}, \"\n",
    "                  f\"AUPRC(icd/proc/lab)=({row[6]:.4f}/{row[7]:.4f}/{row[8]:.4f})\")\n",
    "            # Ghi CSV\n",
    "            with open(self.log_file, \"a\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow(row)\n",
    "\n",
    "\n",
    "batch_logger = BatchLogger(str(OUT_DIR / \"batch_log.csv\"), every_n=1000)\n",
    "\n",
    "\n",
    "# === Callbacks khác ===\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    patience=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "ckpt_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(OUT_DIR / \"best\"),\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ckpt_last = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(OUT_DIR / \"last\"),\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rlrop = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(log_path, append=True)\n",
    "\n",
    "# === Train thêm 1 epoch ===\n",
    "history = multitask_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=initial_epoch,\n",
    "    epochs=target_epochs,\n",
    "    callbacks=[early_stop, ckpt_best, ckpt_last, rlrop, csv_logger, batch_logger],\n",
    "    verbose=0   # tắt log mặc định keras\n",
    ")\n",
    "\n",
    "# === Evaluate cuối ===\n",
    "final_eval = multitask_model.evaluate(val_ds, return_dict=True, verbose=1)\n",
    "print(\"\\nĐánh giá cuối cùng trên validation:\")\n",
    "for k, v in final_eval.items():\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# === Lưu mô hình ===\n",
    "final_path = OUT_DIR / \"final\"\n",
    "multitask_model.save(final_path)\n",
    "print(f\"\\nĐã lưu mô hình hiện tại tại: {final_path.resolve()}\")\n",
    "print(f\"Checkpoint tốt nhất tại: {(OUT_DIR / 'best').resolve()}\")\n",
    "print(f\"Checkpoint mới nhất tại: {(OUT_DIR / 'last').resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f250e",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "loss, icd_loss, lab_loss trên validation sẽ giảm dần qua epoch; AUPRC sẽ tăng dần nếu dữ liệu/tiền xử lý ổn.\n",
    "\n",
    "Sau khi huấn luyện, mô hình bắt đầu dự đoán hợp lý hơn (ít “tràn nhãn” ≥ 0.5, top-k sát thực tế hơn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
