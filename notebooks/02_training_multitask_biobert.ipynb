{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b197f8ef",
   "metadata": {},
   "source": [
    "Cấu hình và phụ thuộc\n",
    "\n",
    "Mục đích: thiết lập tham số huấn luyện và thư viện cần dùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "print(\"Tokenizer OK:\", type(tok).__name__)\n",
    "print(\"Model OK:\", type(bert).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0cbcb",
   "metadata": {},
   "source": [
    "BƯỚC 1 — Đưa đầu vào (Input)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Đọc dữ liệu đã tiền xử lý (examples.parquet, vocab_meta.json).\n",
    "\n",
    "Biến văn bản text thành token ID và attention mask mà BioBERT hiểu (tokenizer của BioBERT).\n",
    "\n",
    "Tạo tf.data.Dataset cho train và validation (chỉ chuẩn bị đầu vào, chưa xây mô hình)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BƯỚC 1 — ĐƯA ĐẦU VÀO (INPUT)  [CẬP NHẬT: thêm age & gender]\n",
    "# =========================\n",
    "\n",
    "# 1) Import các thư viện cần thiết\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 2) Cấu hình cơ bản cho việc tạo batch\n",
    "MAX_LENGTH = 256   # độ dài tối đa của chuỗi token (cắt nếu dài hơn, pad nếu ngắn hơn)\n",
    "BATCH_SIZE = 8     # kích thước lô (batch)\n",
    "VAL_RATIO  = 0.15  # tỉ lệ dành cho validation\n",
    "SEED = 42          # hạt giống ngẫu nhiên để tái lập kết quả\n",
    "\n",
    "# 3) Đường dẫn dữ liệu đã tiền xử lý (đã bao gồm gender, age_at_admit)\n",
    "DATA_ROOT = Path(\"..\") / \"data\" / \"proc\"\n",
    "PARQUET_PATH = DATA_ROOT / \"examples.parquet\"\n",
    "VOCAB_PATH   = DATA_ROOT / \"vocab_meta.json\"\n",
    "\n",
    "print(\"Đang dùng:\")\n",
    "print(\" - PARQUET_PATH:\", PARQUET_PATH.resolve())\n",
    "print(\" - VOCAB_PATH  :\", VOCAB_PATH.resolve())\n",
    "\n",
    "# 4) Kiểm tra sự tồn tại của file dữ liệu\n",
    "if not PARQUET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Không tìm thấy {PARQUET_PATH}. Hãy chạy Notebook tiền xử lý để sinh file, \"\n",
    "        f\"hoặc chỉnh lại đường dẫn cho đúng vị trí thực tế.\"\n",
    "    )\n",
    "if not VOCAB_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Không tìm thấy {VOCAB_PATH}. Hãy kiểm tra Notebook tiền xử lý hoặc đường dẫn.\"\n",
    "    )\n",
    "\n",
    "# 5) Kiểm soát ngẫu nhiên để kết quả ổn định\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 6) Đọc dữ liệu bảng đã tiền xử lý và metadata nhãn\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "icd_vocab        = meta[\"icd_vocab\"]              # danh sách ICD-block (vocab)\n",
    "lab_vocab_items  = meta[\"lab_vocab_items\"]        # danh sách itemid xét nghiệm (vocab)\n",
    "itemid_to_label  = {int(k): v for k, v in meta[\"itemid_to_label\"].items()}\n",
    "\n",
    "n_icd = len(icd_vocab)\n",
    "n_lab = len(lab_vocab_items)\n",
    "\n",
    "print(f\"Số mẫu: {len(df)} | n_icd: {n_icd} | n_lab: {n_lab}\")\n",
    "\n",
    "# 7) Đảm bảo 2 cột nhãn là mảng float32 để dùng với BinaryCrossentropy\n",
    "df[\"y_icd\"] = df[\"y_icd\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "df[\"y_lab\"] = df[\"y_lab\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "\n",
    "# 7.1) [MỚI] Chuẩn hoá demographics -> tabular features\n",
    "# - gender -> one-hot (M/F/U) => 3 chiều\n",
    "# - age_at_admit -> min-max [0..1] bằng cách chia 120 => 1 chiều\n",
    "# Tổng số đặc trưng tabular: TAB_DIM = 4\n",
    "def _normalize_gender(g):\n",
    "    g = (str(g).upper().strip()[:1] if pd.notna(g) else \"U\")\n",
    "    return g if g in (\"M\", \"F\", \"U\") else \"U\"\n",
    "\n",
    "df[\"gender\"] = df[\"gender\"].apply(_normalize_gender)\n",
    "df[\"age_at_admit\"] = df[\"age_at_admit\"].astype(\"float32\")\n",
    "\n",
    "# one-hot cho gender\n",
    "gender_to_idx = {\"M\": 0, \"F\": 1, \"U\": 2}\n",
    "idx = df[\"gender\"].map(gender_to_idx).fillna(2).astype(int).values\n",
    "gender_onehot = np.eye(3, dtype=np.float32)[idx]   # shape [N, 3]\n",
    "\n",
    "# age chuẩn hoá (0..120) -> 0..1; thiếu thì 0\n",
    "age_norm = (df[\"age_at_admit\"].fillna(0.0).clip(0, 120) / 120.0).astype(\"float32\").values.reshape(-1, 1)\n",
    "\n",
    "# ghép thành vector tabular 4 chiều\n",
    "tab_feats = np.concatenate([age_norm, gender_onehot], axis=1).astype(\"float32\")  # [N, 4]\n",
    "TAB_DIM = tab_feats.shape[1]\n",
    "\n",
    "# 8) Chia train/validation theo subject_id (tránh leakage người bệnh)\n",
    "groups = df[\"subject_id\"].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=groups))\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# cắt tương ứng tab_feats\n",
    "train_tab = tab_feats[train_idx]\n",
    "val_tab   = tab_feats[val_idx]\n",
    "\n",
    "print(\"Số mẫu train:\", len(train_df))\n",
    "print(\"Số mẫu val  :\", len(val_df))\n",
    "print(\"TAB_DIM (age_norm + gender_onehot):\", TAB_DIM)\n",
    "\n",
    "# 9) Chuẩn bị tokenizer của BioBERT\n",
    "try:\n",
    "    tokenizer = tok  # tái dùng tokenizer đã nạp ở cell trước (nếu có)\n",
    "except NameError:\n",
    "    MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 10) Hàm mã hóa 1 lô văn bản sang token ID & attention mask (đầu vào mô hình)\n",
    "def encode_text_batch(text_list, max_length):\n",
    "    enc = tokenizer(\n",
    "        list(text_list),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    enc[\"input_ids\"] = np.asarray(enc[\"input_ids\"], dtype=np.int32)\n",
    "    enc[\"attention_mask\"] = np.asarray(enc[\"attention_mask\"], dtype=np.int32)\n",
    "    return enc\n",
    "\n",
    "# 11) Tạo tf.data.Dataset theo kiểu generator (thêm 'tab_feats' vào X)\n",
    "def make_tfds_from_df(frame, tab_array, batch_size, max_length):\n",
    "    \"\"\"\n",
    "    Từ DataFrame gồm cột 'text', 'y_icd', 'y_lab' và mảng tab_array [N, TAB_DIM] tạo Dataset:\n",
    "      - X = {\n",
    "          \"input_ids\": [B, L],\n",
    "          \"attention_mask\": [B, L],\n",
    "          \"tab_feats\": [B, TAB_DIM],         # <<-- mới\n",
    "        }\n",
    "      - y = {\n",
    "          \"icd\": [B, n_icd],\n",
    "          \"lab\": [B, n_lab],\n",
    "        }\n",
    "    \"\"\"\n",
    "    texts = frame[\"text\"].fillna(\"\").tolist()\n",
    "    y_icd = np.stack(frame[\"y_icd\"].to_list()).astype(np.float32)\n",
    "    y_lab = np.stack(frame[\"y_lab\"].to_list()).astype(np.float32)\n",
    "\n",
    "    def gen():\n",
    "        N = len(texts)\n",
    "        start = 0\n",
    "        while start < N:\n",
    "            end = min(start + batch_size, N)\n",
    "            enc = encode_text_batch(texts[start:end], max_length=max_length)\n",
    "            for i in range(end - start):\n",
    "                yield (\n",
    "                    {\n",
    "                        \"input_ids\": enc[\"input_ids\"][i],\n",
    "                        \"attention_mask\": enc[\"attention_mask\"][i],\n",
    "                        \"tab_feats\": tab_array[start + i],   # <<-- mới\n",
    "                    },\n",
    "                    {\n",
    "                        \"icd\": y_icd[start + i],\n",
    "                        \"lab\": y_lab[start + i],\n",
    "                    }\n",
    "                )\n",
    "            start = end\n",
    "\n",
    "    output_signature = (\n",
    "        {\n",
    "            \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"tab_feats\": tf.TensorSpec(shape=(TAB_DIM,), dtype=tf.float32),  # <<-- mới\n",
    "        },\n",
    "        {\n",
    "            \"icd\": tf.TensorSpec(shape=(n_icd,), dtype=tf.float32),\n",
    "            \"lab\": tf.TensorSpec(shape=(n_lab,), dtype=tf.float32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(\n",
    "            {\n",
    "                \"input_ids\": [MAX_LENGTH],\n",
    "                \"attention_mask\": [MAX_LENGTH],\n",
    "                \"tab_feats\": [TAB_DIM],          # cố định, không cần padding thêm\n",
    "            },\n",
    "            {\n",
    "                \"icd\": [n_icd],\n",
    "                \"lab\": [n_lab],\n",
    "            },\n",
    "        ),\n",
    "        padding_values=(\n",
    "            {\n",
    "                \"input_ids\": tf.constant(0, tf.int32),\n",
    "                \"attention_mask\": tf.constant(0, tf.int32),\n",
    "                \"tab_feats\": tf.constant(0.0, tf.float32),\n",
    "            },\n",
    "            {\n",
    "                \"icd\": tf.constant(0.0, tf.float32),\n",
    "                \"lab\": tf.constant(0.0, tf.float32),\n",
    "            },\n",
    "        ),\n",
    "        drop_remainder=False\n",
    "    )\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 12) Tạo train_ds và val_ds (đã kèm tab_feats)\n",
    "train_ds = make_tfds_from_df(train_df, train_tab, BATCH_SIZE, MAX_LENGTH).shuffle(1024)\n",
    "val_ds   = make_tfds_from_df(val_df,   val_tab,   BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "# 13) Kiểm tra nhanh một batch\n",
    "for batch in train_ds.take(1):\n",
    "    X, Y = batch\n",
    "    print(\"Kích thước đầu vào (X):\", {k: v.shape for k, v in X.items()})\n",
    "    print(\"Kích thước nhãn (Y):   \", {k: v.shape for k, v in Y.items()})\n",
    "    # gợi ý: text gốc vẫn xem từ train_df\n",
    "    print(\"Ví dụ text gốc (rút gọn):\", train_df.iloc[0][\"text\"][:120].replace(\"\\n\", \" \"))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6b456",
   "metadata": {},
   "source": [
    "BƯỚC 2 — Xử lý trong BioBERT (tạo vector ngữ nghĩa)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Đưa batch input_ids + attention_mask vào BioBERT.\n",
    "\n",
    "Lấy ra vector biểu diễn câu (dùng token đặc biệt [CLS]).\n",
    "\n",
    "Kiểm tra kích thước và giá trị điển hình để xác nhận “dịch văn bản → ngôn ngữ số”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ceb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BƯỚC 2 — XỬ LÝ TRONG BIOBERT (CẬP NHẬT)\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "DROPOUT = 0.1  # dropout nhẹ cho CLS\n",
    "\n",
    "# 2.1 Nạp backbone BioBERT (TF). Nếu đã có biến `bert` từ trước thì tái dùng.\n",
    "try:\n",
    "    _ = bert  # đã có sẵn\n",
    "except NameError:\n",
    "    bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "# 2.2 Inputs của encoder khớp với tf.data: CHỈ gồm input_ids & attention_mask\n",
    "input_ids      = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# 2.3 Forward qua BioBERT, lấy CLS\n",
    "bert_outputs = bert(input_ids, attention_mask=attention_mask, training=False)\n",
    "cls_vec = bert_outputs.last_hidden_state[:, 0, :]  # [B, H]\n",
    "cls_vec = tf.keras.layers.Dropout(DROPOUT, name=\"cls_dropout\")(cls_vec)\n",
    "\n",
    "# 2.4 Đóng gói encoder (text-only). Fusion với tab_feats sẽ làm ở bước sau.\n",
    "encoder = tf.keras.Model(\n",
    "    inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "    outputs={\"cls\": cls_vec},\n",
    "    name=\"biobert_encoder_cls\"\n",
    ")\n",
    "\n",
    "# 2.5 Kiểm tra nhanh 1 batch\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    # CHỈ truyền 2 khóa mà encoder mong đợi, không đưa 'tab_feats'\n",
    "    enc_inp = {\n",
    "        \"input_ids\": X_batch[\"input_ids\"],\n",
    "        \"attention_mask\": X_batch[\"attention_mask\"],\n",
    "    }\n",
    "    out = encoder(enc_inp, training=False)\n",
    "    cls_batch = out[\"cls\"].numpy()\n",
    "    print(\"Kích thước vector CLS:\", cls_batch.shape)\n",
    "    print(\"8 số đầu của CLS[0]:\", np.round(cls_batch[0][:8], 4).tolist())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aa579",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "Bạn sẽ nhận được ma trận kích thước [BATCH_SIZE, HIDDEN] (thường HIDDEN = 768).\n",
    "\n",
    "Mỗi hàng là “dấu vân tay số học” của 1 văn bản — chứa ngữ cảnh y khoa đã được BioBERT “đọc hiểu”.\n",
    "\n",
    "Đây chính là đầu vào cho “phần ra quyết định” ở Bước 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b908",
   "metadata": {},
   "source": [
    "BƯỚC 3 — Classifier head (Phần ra quyết định)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Gắn 2 “đầu ra quyết định” (Dense) lên vector CLS từ BioBERT:\n",
    "\n",
    "Đầu ICD: dự đoán đa nhãn các ICD-block.\n",
    "\n",
    "Đầu Lab: dự đoán đa nhãn các xét nghiệm sớm.\n",
    "\n",
    "Mỗi đầu trả về logits (số thực âm/dương). Khi qua sigmoid sẽ thành xác suất 0–1 cho từng nhãn.\n",
    "\n",
    "Kiểm tra kích thước đầu ra và xem thử top-k dự đoán trên một batch (chưa huấn luyện, chỉ để minh họa dòng chảy dữ liệu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BƯỚC 3 — CLASSIFIER HEAD (CẬP NHẬT: fusion CLS + tab_feats)\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 3.1 Lấy đầu ra CLS từ encoder (Bước 2)\n",
    "cls_output = encoder.outputs[0]  # [batch, hidden_size], vd 768\n",
    "\n",
    "# 3.2 Khai báo thêm input tabular để khớp với tf.data (Bước 1 đã tạo 'tab_feats')\n",
    "tab_input = tf.keras.Input(shape=(TAB_DIM,), dtype=tf.float32, name=\"tab_feats\")  # [batch, TAB_DIM]\n",
    "\n",
    "# 3.3 Fusion: ghép CLS + tab_feats (có thể thêm MLP nhỏ để ổn định)\n",
    "fused = tf.keras.layers.Concatenate(name=\"fuse_cls_tab\")([cls_output, tab_input])  # [batch, 768+TAB_DIM]\n",
    "fused = tf.keras.layers.Dropout(0.1, name=\"fuse_dropout\")(fused)\n",
    "fused = tf.keras.layers.Dense(512, activation=\"relu\", name=\"fuse_dense\")(fused)\n",
    "\n",
    "# 3.4 Hai nhánh logits cho tác vụ đa nhãn (from_logits=True ở compile)\n",
    "icd_logits = tf.keras.layers.Dense(n_icd, name=\"icd_logits\")(fused)  # [batch, n_icd]\n",
    "lab_logits = tf.keras.layers.Dense(n_lab, name=\"lab_logits\")(fused)  # [batch, n_lab]\n",
    "\n",
    "# 3.5 Mô hình đa nhiệm hoàn chỉnh (inputs = text + tabular)\n",
    "multitask_model = tf.keras.Model(\n",
    "    inputs={\"input_ids\": encoder.inputs[\"input_ids\"],\n",
    "            \"attention_mask\": encoder.inputs[\"attention_mask\"],\n",
    "            \"tab_feats\": tab_input},\n",
    "    outputs={\"icd\": icd_logits, \"lab\": lab_logits},\n",
    "    name=\"biobert_multitask_fusion\"\n",
    ")\n",
    "\n",
    "# 3.6 Kiểm tra kiến trúc\n",
    "multitask_model.summary(line_length=120)\n",
    "\n",
    "# 3.7 Chạy thử một batch (X_batch đã có 'input_ids', 'attention_mask', 'tab_feats')\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    logits = multitask_model.predict(X_batch, verbose=0)\n",
    "    icd_log = logits[\"icd\"]\n",
    "    lab_log = logits[\"lab\"]\n",
    "    print(\"Kích thước icd_logits:\", icd_log.shape)\n",
    "    print(\"Kích thước lab_logits:\", lab_log.shape)\n",
    "\n",
    "    # Đổi sang xác suất để quan sát\n",
    "    probs_icd = tf.sigmoid(icd_log).numpy()\n",
    "    probs_lab = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "    k = 5\n",
    "    top_icd_idx = probs_icd[0].argsort()[-k:][::-1]\n",
    "    top_lab_idx = probs_lab[0].argsort()[-k:][::-1]\n",
    "    print(\"Top-5 ICD indices:\", top_icd_idx.tolist())\n",
    "    print(\"Top-5 ICD probs  :\", np.round(probs_icd[0][top_icd_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 Lab indices:\", top_lab_idx.tolist())\n",
    "    print(\"Top-5 Lab probs  :\", np.round(probs_lab[0][top_lab_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 ICD codes:\", [icd_vocab[i] for i in top_icd_idx])\n",
    "    print(\"Top-5 Lab itemids:\", [lab_vocab_items[j] for j in top_lab_idx])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98f94a",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "Bạn sẽ thấy hai ma trận: icd_logits có kích thước [BATCH_SIZE, n_icd], lab_logits có kích thước [BATCH_SIZE, n_lab].\n",
    "\n",
    "Sau khi áp sigmoid, bạn sẽ nhận được hai ma trận xác suất probs_icd, probs_lab cùng kích thước, mỗi cột là “một ô” xác suất mà bạn mô tả (ví dụ 0.85 cho I21, 0.92 cho Troponin…).\n",
    "\n",
    "Đây chính là “phần ra quyết định” trước khi so sánh với nhãn thật ở Bước 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e088b",
   "metadata": {},
   "source": [
    "BƯỚC 4 — So sánh với nhãn thật (Ground truth)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Lấy dự đoán từ Bước 3 (logits → xác suất).\n",
    "\n",
    "So sánh với nhãn thật y_icd, y_lab cho một mẫu trong validation:\n",
    "\n",
    "Xem những nhãn nào mô hình dự đoán (theo ngưỡng 0.5) so với nhãn thật.\n",
    "\n",
    "Xem Top-k (ví dụ k=5) nhãn có xác suất cao nhất và đối chiếu nhãn thật."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfca6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# BƯỚC 4 — SO SÁNH VỚI NHÃN THẬT (CẬP NHẬT)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) Lấy một batch từ validation để so sánh\n",
    "#    X_batch giờ gồm: input_ids, attention_mask, tab_feats\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "\n",
    "# 2) Chạy mô hình đa nhiệm (fusion từ Bước 3) để lấy logits\n",
    "logits = multitask_model.predict(X_batch, verbose=0)\n",
    "icd_log = logits[\"icd\"]   # [BATCH_SIZE, n_icd]\n",
    "lab_log = logits[\"lab\"]   # [BATCH_SIZE, n_lab]\n",
    "\n",
    "# 3) Đổi logits -> xác suất (0..1) bằng sigmoid để dễ \"đọc\"\n",
    "probs_icd = tf.sigmoid(icd_log).numpy()\n",
    "probs_lab = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "# 4) Chọn một mẫu trong batch để nhìn chi tiết (ví dụ mẫu đầu tiên)\n",
    "idx = 0\n",
    "y_icd_true = Y_batch[\"icd\"][idx].numpy()   # vector multi-hot thật [n_icd]\n",
    "y_lab_true = Y_batch[\"lab\"][idx].numpy()   # vector multi-hot thật [n_lab]\n",
    "\n",
    "# 5) Chiến lược A: So sánh theo NGƯỠNG 0.5 (mặc định)\n",
    "THR_ICD = 0.5\n",
    "THR_LAB = 0.5\n",
    "\n",
    "y_icd_pred_bin = (probs_icd[idx] >= THR_ICD).astype(int)\n",
    "y_lab_pred_bin = (probs_lab[idx] >= THR_LAB).astype(int)\n",
    "\n",
    "true_icd_idx = np.where(y_icd_true == 1)[0]\n",
    "pred_icd_idx = np.where(y_icd_pred_bin == 1)[0]\n",
    "hit_icd_idx  = np.intersect1d(true_icd_idx, pred_icd_idx)\n",
    "\n",
    "true_lab_idx = np.where(y_lab_true == 1)[0]\n",
    "pred_lab_idx = np.where(y_lab_pred_bin == 1)[0]\n",
    "hit_lab_idx  = np.intersect1d(true_lab_idx, pred_lab_idx)\n",
    "\n",
    "print(\"=== So sánh theo NGƯỠNG 0.5 (mẫu idx=0) ===\")\n",
    "print(f\"ICD: thật={len(true_icd_idx)} | dự đoán={len(pred_icd_idx)} | đúng={len(hit_icd_idx)}\")\n",
    "print(f\"LAB: thật={len(true_lab_idx)} | dự đoán={len(pred_lab_idx)} | đúng={len(hit_lab_idx)}\")\n",
    "\n",
    "# 6) Chiến lược B: So sánh theo TOP-K (ví dụ k=5)\n",
    "K_ICD = 5\n",
    "K_LAB = 5\n",
    "\n",
    "top_icd_idx = probs_icd[idx].argsort()[-K_ICD:][::-1]\n",
    "top_lab_idx = probs_lab[idx].argsort()[-K_LAB:][::-1]\n",
    "\n",
    "print(\"\\n=== So sánh theo TOP-K ===\")\n",
    "print(\"Top-5 ICD (index → mã → xác suất → có phải nhãn thật?):\")\n",
    "for j in top_icd_idx:\n",
    "    code = icd_vocab[j]\n",
    "    p = float(probs_icd[idx][j])\n",
    "    is_true = int(y_icd_true[j])\n",
    "    print(f\"  idx={j:4d} | ICD={code:5s} | p={p:0.3f} | nhãn_thật={is_true}\")\n",
    "\n",
    "print(\"\\nTop-5 Lab (index → itemid → tên → xác suất → có phải nhãn thật?):\")\n",
    "for j in top_lab_idx:\n",
    "    itemid = int(lab_vocab_items[j])\n",
    "    name = itemid_to_label.get(itemid, str(itemid))\n",
    "    p = float(probs_lab[idx][j])\n",
    "    is_true = int(y_lab_true[j])\n",
    "    print(f\"  idx={j:4d} | itemid={itemid} | {name} | p={p:0.3f} | nhãn_thật={is_true}\")\n",
    "\n",
    "# 7) (Tùy chọn) In danh sách nhãn đúng theo ngưỡng để hình dung\n",
    "def map_icd_indices(indices):\n",
    "    return [icd_vocab[i] for i in indices]\n",
    "\n",
    "def map_lab_indices(indices):\n",
    "    return [(int(lab_vocab_items[i]),\n",
    "             itemid_to_label.get(int(lab_vocab_items[i]), str(lab_vocab_items[i])))\n",
    "            for i in indices]\n",
    "\n",
    "print(\"\\nICD — nhãn_thật (mã):\", map_icd_indices(true_icd_idx)[:20], \"...\")\n",
    "print(\"ICD — dự_đoán_theo_ngưỡng (mã):\", map_icd_indices(pred_icd_idx)[:20], \"...\")\n",
    "print(\"ICD — giao_nhau_đúng (mã):\", map_icd_indices(hit_icd_idx)[:20], \"...\")\n",
    "\n",
    "print(\"\\nLAB — nhãn_thật (itemid, tên):\", map_lab_indices(true_lab_idx)[:10], \"...\")\n",
    "print(\"LAB — dự_đoán_theo_ngưỡng (itemid, tên):\", map_lab_indices(pred_lab_idx)[:10], \"...\")\n",
    "print(\"LAB — giao_nhau_đúng (itemid, tên):\", map_lab_indices(hit_lab_idx)[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28058",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "Bạn thấy rõ cách đối chiếu:\n",
    "\n",
    "“Dự đoán theo ngưỡng” (≥ 0.5) vs. nhãn thật → đếm đúng/sai.\n",
    "\n",
    "“Dự đoán Top-k” → xem 5 nhãn cao nhất và đánh dấu nhãn nào trùng nhãn thật.\n",
    "\n",
    "Đây là bước không tính loss (chưa huấn luyện), chỉ so sánh để hiểu dòng chảy dự đoán ↔ nhãn.\n",
    "(Loss sẽ thiết lập ở Bước 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c195b65",
   "metadata": {},
   "source": [
    "BƯỚC 5 — Tính sai số (Loss)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Khai báo hàm mất mát cho 2 nhánh đầu ra (ICD và Lab) đúng với thiết kế đa nhãn.\n",
    "\n",
    "Biên dịch (compile) mô hình với optimizer/metrics, chưa huấn luyện.\n",
    "\n",
    "Tính thử loss & AUPRC trên 1 batch để thấy mô hình đang ở mức “ngẫu nhiên” trước khi học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ba5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# BƯỚC 5 — TÍNH SAI SỐ (LOSS) & COMPILE (CẬP NHẬT + RESUME, CÓ FREEZE_BERT)\n",
    "# ======================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "\n",
    "# === Biến cấu hình ===\n",
    "FREEZE_BERT = True   # <<== đổi True/False để bật/tắt đóng băng backbone\n",
    "LR = 2e-5            # learning rate\n",
    "\n",
    "# 1) Loss đa nhãn cho 2 nhánh (logits)\n",
    "losses = {\n",
    "    \"icd\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    \"lab\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "}\n",
    "\n",
    "# 2) Trọng số loss (tuỳ chỉnh nếu muốn ưu tiên 1 nhánh)\n",
    "loss_weights = {\"icd\": 1.0, \"lab\": 1.0}\n",
    "\n",
    "# 3) Metrics: AUPRC (phù hợp mất cân bằng nhãn)\n",
    "metrics = {\n",
    "    \"icd\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "    \"lab\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "}\n",
    "\n",
    "# 4) Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "# 4.1 Đóng băng backbone BioBERT nếu FREEZE_BERT=True\n",
    "if FREEZE_BERT:\n",
    "    try:\n",
    "        bert.trainable = False\n",
    "    except NameError:\n",
    "        pass\n",
    "    for sub in multitask_model.submodules:\n",
    "        if \"bert\" in sub.name.lower():\n",
    "            sub.trainable = False\n",
    "    print(\"==> Đang TRAIN với BERT đóng băng (freeze).\")\n",
    "else:\n",
    "    try:\n",
    "        bert.trainable = True\n",
    "    except NameError:\n",
    "        pass\n",
    "    for sub in multitask_model.submodules:\n",
    "        if \"bert\" in sub.name.lower():\n",
    "            sub.trainable = True\n",
    "    print(\"==> Đang TRAIN với BERT fine-tune toàn bộ (unfreeze).\")\n",
    "\n",
    "# 5) Compile mô hình\n",
    "multitask_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# 6) Resume checkpoint nếu có\n",
    "OUT_DIR = Path(\"checkpoints_biobert_multitask\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESUME_PATH = OUT_DIR / \"last\"\n",
    "\n",
    "if RESUME_PATH.exists():\n",
    "    print(f\"==> Resume training từ checkpoint: {RESUME_PATH}\")\n",
    "    multitask_model = keras.models.load_model(RESUME_PATH)\n",
    "else:\n",
    "    print(\"==> Train từ đầu (chưa có checkpoint)\")\n",
    "\n",
    "# 7) Đánh giá nhanh trên 1 batch validation\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "results = multitask_model.evaluate(\n",
    "    X_batch, Y_batch,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"Kết quả trên 1 batch (trước khi train):\")\n",
    "for k, v in results.items():\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f2e1c",
   "metadata": {},
   "source": [
    "BƯỚC 6 — Cập nhật (Training)\n",
    "\n",
    "Mục tiêu\n",
    "\n",
    "Huấn luyện toàn bộ mô hình: BioBERT + 2 đầu ra (ICD, Lab) bằng Adam.\n",
    "\n",
    "Dùng EarlyStopping để dừng sớm nếu không cải thiện.\n",
    "\n",
    "Lưu checkpoint tốt nhất theo chỉ số thẩm định (validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86192080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# BƯỚC 6 — HUẤN LUYỆN (TRAINING) [CẬP NHẬT: log sau mỗi 1000 batch]\n",
    "# ==================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv, os\n",
    "\n",
    "OUT_DIR = Path(\"checkpoints_biobert_multitask\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Detect epoch đã train từ log ===\n",
    "initial_epoch = 0\n",
    "log_path = OUT_DIR / \"train_log.csv\"\n",
    "if log_path.exists():\n",
    "    try:\n",
    "        df_log = pd.read_csv(log_path)\n",
    "        if \"epoch\" in df_log.columns:\n",
    "            initial_epoch = int(df_log[\"epoch\"].max()) + 1\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Không đọc được train_log.csv:\", e)\n",
    "\n",
    "# Luôn train thêm đúng 1 epoch\n",
    "target_epochs = initial_epoch + 1\n",
    "print(f\"==> Resume training từ epoch {initial_epoch} đến {target_epochs}\")\n",
    "\n",
    "\n",
    "# === Callback custom để log mỗi 1000 batch ===\n",
    "class BatchLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_file, every_n=1000):\n",
    "        super().__init__()\n",
    "        self.log_file = log_file\n",
    "        self.every_n = every_n\n",
    "        # nếu file chưa tồn tại thì tạo header\n",
    "        if not os.path.exists(log_file):\n",
    "            with open(log_file, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"epoch\", \"batch\", \"loss\", \"icd_loss\", \"lab_loss\", \"icd_AUPRC\", \"lab_AUPRC\"])\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        if (batch + 1) % self.every_n == 0:\n",
    "            row = [\n",
    "                self.epoch, \n",
    "                batch + 1, \n",
    "                logs.get(\"loss\", None), \n",
    "                logs.get(\"icd_loss\", None), \n",
    "                logs.get(\"lab_loss\", None), \n",
    "                logs.get(\"icd_AUPRC\", None), \n",
    "                logs.get(\"lab_AUPRC\", None)\n",
    "            ]\n",
    "            # In ra màn hình\n",
    "            print(f\"[Epoch {self.epoch} | Batch {batch+1}] \"\n",
    "                  f\"loss={row[2]:.4f}, icd_loss={row[3]:.4f}, lab_loss={row[4]:.4f}, \"\n",
    "                  f\"icd_AUPRC={row[5]:.4f}, lab_AUPRC={row[6]:.4f}\")\n",
    "\n",
    "            # Append vào CSV\n",
    "            with open(self.log_file, \"a\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = initial_epoch + epoch  # đảm bảo liên tục khi resume\n",
    "\n",
    "\n",
    "batch_logger = BatchLogger(str(OUT_DIR / \"batch_log.csv\"), every_n=1000)\n",
    "\n",
    "\n",
    "# === Callbacks khác ===\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    patience=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "ckpt_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(OUT_DIR / \"best\"),\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ckpt_last = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(OUT_DIR / \"last\"),\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rlrop = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# (CSVLogger gốc để log epoch-level vẫn giữ, append)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(log_path, append=True)\n",
    "\n",
    "# === Train thêm 1 epoch ===\n",
    "history = multitask_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=initial_epoch,\n",
    "    epochs=target_epochs,\n",
    "    callbacks=[early_stop, ckpt_best, ckpt_last, rlrop, csv_logger, batch_logger],\n",
    "    verbose=0   # tắt log mặc định của keras\n",
    ")\n",
    "\n",
    "# === Evaluate cuối ===\n",
    "final_eval = multitask_model.evaluate(val_ds, return_dict=True, verbose=1)\n",
    "print(\"\\nĐánh giá cuối cùng trên validation:\")\n",
    "for k, v in final_eval.items():\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# === Lưu bản final ===\n",
    "final_path = OUT_DIR / \"final\"\n",
    "multitask_model.save(final_path)\n",
    "print(f\"\\nĐã lưu mô hình hiện tại tại: {final_path.resolve()}\")\n",
    "print(f\"Checkpoint tốt nhất tại: { (OUT_DIR / 'best').resolve() }\")\n",
    "print(f\"Checkpoint mới nhất tại: { (OUT_DIR / 'last').resolve() }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f250e",
   "metadata": {},
   "source": [
    "Ý nghĩa của kết quả\n",
    "\n",
    "loss, icd_loss, lab_loss trên validation sẽ giảm dần qua epoch; AUPRC sẽ tăng dần nếu dữ liệu/tiền xử lý ổn.\n",
    "\n",
    "Sau khi huấn luyện, mô hình bắt đầu dự đoán hợp lý hơn (ít “tràn nhãn” ≥ 0.5, top-k sát thực tế hơn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
