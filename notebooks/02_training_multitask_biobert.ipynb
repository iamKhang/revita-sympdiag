{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b197f8ef",
   "metadata": {},
   "source": [
    "C·∫•u h√¨nh v√† ph·ª• thu·ªôc\n",
    "\n",
    "M·ª•c ƒë√≠ch: thi·∫øt l·∫≠p tham s·ªë hu·∫•n luy·ªán v√† th∆∞ vi·ªán c·∫ßn d√πng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814d776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer OK: BertTokenizerFast\n",
      "Model OK: TFBertModel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "print(\"Tokenizer OK:\", type(tok).__name__)\n",
    "print(\"Model OK:\", type(bert).__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0cbcb",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 1 ‚Äî ƒê∆∞a ƒë·∫ßu v√†o (Input)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "ƒê·ªçc d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω (examples.parquet, vocab_meta.json).\n",
    "\n",
    "Bi·∫øn vƒÉn b·∫£n text th√†nh token ID v√† attention mask m√† BioBERT hi·ªÉu (tokenizer c·ªßa BioBERT).\n",
    "\n",
    "T·∫°o tf.data.Dataset cho train v√† validation (ch·ªâ chu·∫©n b·ªã ƒë·∫ßu v√†o, ch∆∞a x√¢y m√¥ h√¨nh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang d√πng:\n",
      " - PARQUET_PATH: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/proc/examples.parquet\n",
      " - VOCAB_PATH  : /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/data/proc/vocab_meta.json\n",
      "S·ªë m·∫´u: 828 | n_icd: 200 | n_lab: 50\n",
      "S·ªë m·∫´u train: 717\n",
      "S·ªë m·∫´u val  : 111\n",
      "K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (X): {'input_ids': TensorShape([8, 256]), 'attention_mask': TensorShape([8, 256])}\n",
      "K√≠ch th∆∞·ªõc nh√£n (Y):    {'icd': TensorShape([8, 200]), 'lab': TensorShape([8, 50])}\n",
      "V√≠ d·ª• text g·ªëc (r√∫t g·ªçn): EXAMINATION: LIVER OR GALLBLADDER US (SINGLE ORGAN) INDICATION: ___ year-old female with cirrhosis, jaundice. TECHNIQUE:\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# B∆Ø·ªöC 1 ‚Äî ƒê∆ØA ƒê·∫¶U V√ÄO (INPUT)  [C·∫¨P NH·∫¨T: th√™m age & gender]\n",
    "# =========================\n",
    "\n",
    "# 1) Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 2) C·∫•u h√¨nh c∆° b·∫£n cho vi·ªác t·∫°o batch\n",
    "MAX_LENGTH = 256   # ƒë·ªô d√†i t·ªëi ƒëa c·ªßa chu·ªói token (c·∫Øt n·∫øu d√†i h∆°n, pad n·∫øu ng·∫Øn h∆°n)\n",
    "BATCH_SIZE = 8     # k√≠ch th∆∞·ªõc l√¥ (batch)\n",
    "VAL_RATIO  = 0.15  # t·ªâ l·ªá d√†nh cho validation\n",
    "SEED = 42          # h·∫°t gi·ªëng ng·∫´u nhi√™n ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£\n",
    "\n",
    "# 3) ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω (ƒë√£ bao g·ªìm gender, age_at_admit)\n",
    "DATA_ROOT = Path(\"..\") / \"data\" / \"proc\"\n",
    "PARQUET_PATH = DATA_ROOT / \"examples.parquet\"\n",
    "VOCAB_PATH   = DATA_ROOT / \"vocab_meta.json\"\n",
    "\n",
    "print(\"ƒêang d√πng:\")\n",
    "print(\" - PARQUET_PATH:\", PARQUET_PATH.resolve())\n",
    "print(\" - VOCAB_PATH  :\", VOCAB_PATH.resolve())\n",
    "\n",
    "# 4) Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file d·ªØ li·ªáu\n",
    "if not PARQUET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Kh√¥ng t√¨m th·∫•y {PARQUET_PATH}. H√£y ch·∫°y Notebook ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ sinh file, \"\n",
    "        f\"ho·∫∑c ch·ªânh l·∫°i ƒë∆∞·ªùng d·∫´n cho ƒë√∫ng v·ªã tr√≠ th·ª±c t·∫ø.\"\n",
    "    )\n",
    "if not VOCAB_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Kh√¥ng t√¨m th·∫•y {VOCAB_PATH}. H√£y ki·ªÉm tra Notebook ti·ªÅn x·ª≠ l√Ω ho·∫∑c ƒë∆∞·ªùng d·∫´n.\"\n",
    "    )\n",
    "\n",
    "# 5) Ki·ªÉm so√°t ng·∫´u nhi√™n ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 6) ƒê·ªçc d·ªØ li·ªáu b·∫£ng ƒë√£ ti·ªÅn x·ª≠ l√Ω v√† metadata nh√£n\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "icd_vocab        = meta[\"icd_vocab\"]              # danh s√°ch ICD-block (vocab)\n",
    "lab_vocab_items  = meta[\"lab_vocab_items\"]        # danh s√°ch itemid x√©t nghi·ªám (vocab)\n",
    "itemid_to_label  = {int(k): v for k, v in meta[\"itemid_to_label\"].items()}\n",
    "\n",
    "n_icd = len(icd_vocab)\n",
    "n_lab = len(lab_vocab_items)\n",
    "\n",
    "print(f\"S·ªë m·∫´u: {len(df)} | n_icd: {n_icd} | n_lab: {n_lab}\")\n",
    "\n",
    "# 7) ƒê·∫£m b·∫£o 2 c·ªôt nh√£n l√† m·∫£ng float32 ƒë·ªÉ d√πng v·ªõi BinaryCrossentropy\n",
    "df[\"y_icd\"] = df[\"y_icd\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "df[\"y_lab\"] = df[\"y_lab\"].apply(lambda a: np.asarray(a, dtype=np.float32))\n",
    "\n",
    "# 7.1) [M·ªöI] Chu·∫©n ho√° demographics -> tabular features\n",
    "# - gender -> one-hot (M/F/U) => 3 chi·ªÅu\n",
    "# - age_at_admit -> min-max [0..1] b·∫±ng c√°ch chia 120 => 1 chi·ªÅu\n",
    "# T·ªïng s·ªë ƒë·∫∑c tr∆∞ng tabular: TAB_DIM = 4\n",
    "def _normalize_gender(g):\n",
    "    g = (str(g).upper().strip()[:1] if pd.notna(g) else \"U\")\n",
    "    return g if g in (\"M\", \"F\", \"U\") else \"U\"\n",
    "\n",
    "df[\"gender\"] = df[\"gender\"].apply(_normalize_gender)\n",
    "df[\"age_at_admit\"] = df[\"age_at_admit\"].astype(\"float32\")\n",
    "\n",
    "# one-hot cho gender\n",
    "gender_to_idx = {\"M\": 0, \"F\": 1, \"U\": 2}\n",
    "idx = df[\"gender\"].map(gender_to_idx).fillna(2).astype(int).values\n",
    "gender_onehot = np.eye(3, dtype=np.float32)[idx]   # shape [N, 3]\n",
    "\n",
    "# age chu·∫©n ho√° (0..120) -> 0..1; thi·∫øu th√¨ 0\n",
    "age_norm = (df[\"age_at_admit\"].fillna(0.0).clip(0, 120) / 120.0).astype(\"float32\").values.reshape(-1, 1)\n",
    "\n",
    "# gh√©p th√†nh vector tabular 4 chi·ªÅu\n",
    "tab_feats = np.concatenate([age_norm, gender_onehot], axis=1).astype(\"float32\")  # [N, 4]\n",
    "TAB_DIM = tab_feats.shape[1]\n",
    "\n",
    "# 8) Chia train/validation theo subject_id (tr√°nh leakage ng∆∞·ªùi b·ªánh)\n",
    "groups = df[\"subject_id\"].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=VAL_RATIO, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=groups))\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# c·∫Øt t∆∞∆°ng ·ª©ng tab_feats\n",
    "train_tab = tab_feats[train_idx]\n",
    "val_tab   = tab_feats[val_idx]\n",
    "\n",
    "print(\"S·ªë m·∫´u train:\", len(train_df))\n",
    "print(\"S·ªë m·∫´u val  :\", len(val_df))\n",
    "print(\"TAB_DIM (age_norm + gender_onehot):\", TAB_DIM)\n",
    "\n",
    "# 9) Chu·∫©n b·ªã tokenizer c·ªßa BioBERT\n",
    "try:\n",
    "    tokenizer = tok  # t√°i d√πng tokenizer ƒë√£ n·∫°p ·ªü cell tr∆∞·ªõc (n·∫øu c√≥)\n",
    "except NameError:\n",
    "    MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 10) H√†m m√£ h√≥a 1 l√¥ vƒÉn b·∫£n sang token ID & attention mask (ƒë·∫ßu v√†o m√¥ h√¨nh)\n",
    "def encode_text_batch(text_list, max_length):\n",
    "    enc = tokenizer(\n",
    "        list(text_list),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    enc[\"input_ids\"] = np.asarray(enc[\"input_ids\"], dtype=np.int32)\n",
    "    enc[\"attention_mask\"] = np.asarray(enc[\"attention_mask\"], dtype=np.int32)\n",
    "    return enc\n",
    "\n",
    "# 11) T·∫°o tf.data.Dataset theo ki·ªÉu generator (th√™m 'tab_feats' v√†o X)\n",
    "def make_tfds_from_df(frame, tab_array, batch_size, max_length):\n",
    "    \"\"\"\n",
    "    T·ª´ DataFrame g·ªìm c·ªôt 'text', 'y_icd', 'y_lab' v√† m·∫£ng tab_array [N, TAB_DIM] t·∫°o Dataset:\n",
    "      - X = {\n",
    "          \"input_ids\": [B, L],\n",
    "          \"attention_mask\": [B, L],\n",
    "          \"tab_feats\": [B, TAB_DIM],         # <<-- m·ªõi\n",
    "        }\n",
    "      - y = {\n",
    "          \"icd\": [B, n_icd],\n",
    "          \"lab\": [B, n_lab],\n",
    "        }\n",
    "    \"\"\"\n",
    "    texts = frame[\"text\"].fillna(\"\").tolist()\n",
    "    y_icd = np.stack(frame[\"y_icd\"].to_list()).astype(np.float32)\n",
    "    y_lab = np.stack(frame[\"y_lab\"].to_list()).astype(np.float32)\n",
    "\n",
    "    def gen():\n",
    "        N = len(texts)\n",
    "        start = 0\n",
    "        while start < N:\n",
    "            end = min(start + batch_size, N)\n",
    "            enc = encode_text_batch(texts[start:end], max_length=max_length)\n",
    "            for i in range(end - start):\n",
    "                yield (\n",
    "                    {\n",
    "                        \"input_ids\": enc[\"input_ids\"][i],\n",
    "                        \"attention_mask\": enc[\"attention_mask\"][i],\n",
    "                        \"tab_feats\": tab_array[start + i],   # <<-- m·ªõi\n",
    "                    },\n",
    "                    {\n",
    "                        \"icd\": y_icd[start + i],\n",
    "                        \"lab\": y_lab[start + i],\n",
    "                    }\n",
    "                )\n",
    "            start = end\n",
    "\n",
    "    output_signature = (\n",
    "        {\n",
    "            \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "            \"tab_feats\": tf.TensorSpec(shape=(TAB_DIM,), dtype=tf.float32),  # <<-- m·ªõi\n",
    "        },\n",
    "        {\n",
    "            \"icd\": tf.TensorSpec(shape=(n_icd,), dtype=tf.float32),\n",
    "            \"lab\": tf.TensorSpec(shape=(n_lab,), dtype=tf.float32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "\n",
    "    ds = ds.padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(\n",
    "            {\n",
    "                \"input_ids\": [MAX_LENGTH],\n",
    "                \"attention_mask\": [MAX_LENGTH],\n",
    "                \"tab_feats\": [TAB_DIM],          # c·ªë ƒë·ªãnh, kh√¥ng c·∫ßn padding th√™m\n",
    "            },\n",
    "            {\n",
    "                \"icd\": [n_icd],\n",
    "                \"lab\": [n_lab],\n",
    "            },\n",
    "        ),\n",
    "        padding_values=(\n",
    "            {\n",
    "                \"input_ids\": tf.constant(0, tf.int32),\n",
    "                \"attention_mask\": tf.constant(0, tf.int32),\n",
    "                \"tab_feats\": tf.constant(0.0, tf.float32),\n",
    "            },\n",
    "            {\n",
    "                \"icd\": tf.constant(0.0, tf.float32),\n",
    "                \"lab\": tf.constant(0.0, tf.float32),\n",
    "            },\n",
    "        ),\n",
    "        drop_remainder=False\n",
    "    )\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 12) T·∫°o train_ds v√† val_ds (ƒë√£ k√®m tab_feats)\n",
    "train_ds = make_tfds_from_df(train_df, train_tab, BATCH_SIZE, MAX_LENGTH).shuffle(1024)\n",
    "val_ds   = make_tfds_from_df(val_df,   val_tab,   BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "# 13) Ki·ªÉm tra nhanh m·ªôt batch\n",
    "for batch in train_ds.take(1):\n",
    "    X, Y = batch\n",
    "    print(\"K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (X):\", {k: v.shape for k, v in X.items()})\n",
    "    print(\"K√≠ch th∆∞·ªõc nh√£n (Y):   \", {k: v.shape for k, v in Y.items()})\n",
    "    # g·ª£i √Ω: text g·ªëc v·∫´n xem t·ª´ train_df\n",
    "    print(\"V√≠ d·ª• text g·ªëc (r√∫t g·ªçn):\", train_df.iloc[0][\"text\"][:120].replace(\"\\n\", \" \"))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6b456",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 2 ‚Äî X·ª≠ l√Ω trong BioBERT (t·∫°o vector ng·ªØ nghƒ©a)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "ƒê∆∞a batch input_ids + attention_mask v√†o BioBERT.\n",
    "\n",
    "L·∫•y ra vector bi·ªÉu di·ªÖn c√¢u (d√πng token ƒë·∫∑c bi·ªát [CLS]).\n",
    "\n",
    "Ki·ªÉm tra k√≠ch th∆∞·ªõc v√† gi√° tr·ªã ƒëi·ªÉn h√¨nh ƒë·ªÉ x√°c nh·∫≠n ‚Äúd·ªãch vƒÉn b·∫£n ‚Üí ng√¥n ng·ªØ s·ªë‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ceb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K√≠ch th∆∞·ªõc vector CLS: (8, 768)\n",
      "8 s·ªë ƒë·∫ßu c·ªßa CLS[0]: [0.13459999859333038, -0.023900000378489494, -0.17829999327659607, -0.04659999907016754, -0.436599999666214, -0.028200000524520874, 0.3319000005722046, 0.2379000037908554]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# B∆Ø·ªöC 2 ‚Äî X·ª¨ L√ù TRONG BIOBERT (C·∫¨P NH·∫¨T)\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "DROPOUT = 0.1  # dropout nh·∫π cho CLS\n",
    "\n",
    "# 2.1 N·∫°p backbone BioBERT (TF). N·∫øu ƒë√£ c√≥ bi·∫øn `bert` t·ª´ tr∆∞·ªõc th√¨ t√°i d√πng.\n",
    "try:\n",
    "    _ = bert  # ƒë√£ c√≥ s·∫µn\n",
    "except NameError:\n",
    "    bert = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "# 2.2 Inputs c·ªßa encoder kh·ªõp v·ªõi tf.data: CH·ªà g·ªìm input_ids & attention_mask\n",
    "input_ids      = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# 2.3 Forward qua BioBERT, l·∫•y CLS\n",
    "bert_outputs = bert(input_ids, attention_mask=attention_mask, training=False)\n",
    "cls_vec = bert_outputs.last_hidden_state[:, 0, :]  # [B, H]\n",
    "cls_vec = tf.keras.layers.Dropout(DROPOUT, name=\"cls_dropout\")(cls_vec)\n",
    "\n",
    "# 2.4 ƒê√≥ng g√≥i encoder (text-only). Fusion v·ªõi tab_feats s·∫Ω l√†m ·ªü b∆∞·ªõc sau.\n",
    "encoder = tf.keras.Model(\n",
    "    inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "    outputs={\"cls\": cls_vec},\n",
    "    name=\"biobert_encoder_cls\"\n",
    ")\n",
    "\n",
    "# 2.5 Ki·ªÉm tra nhanh 1 batch\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    # CH·ªà truy·ªÅn 2 kh√≥a m√† encoder mong ƒë·ª£i, kh√¥ng ƒë∆∞a 'tab_feats'\n",
    "    enc_inp = {\n",
    "        \"input_ids\": X_batch[\"input_ids\"],\n",
    "        \"attention_mask\": X_batch[\"attention_mask\"],\n",
    "    }\n",
    "    out = encoder(enc_inp, training=False)\n",
    "    cls_batch = out[\"cls\"].numpy()\n",
    "    print(\"K√≠ch th∆∞·ªõc vector CLS:\", cls_batch.shape)\n",
    "    print(\"8 s·ªë ƒë·∫ßu c·ªßa CLS[0]:\", np.round(cls_batch[0][:8], 4).tolist())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791aa579",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c ma tr·∫≠n k√≠ch th∆∞·ªõc [BATCH_SIZE, HIDDEN] (th∆∞·ªùng HIDDEN = 768).\n",
    "\n",
    "M·ªói h√†ng l√† ‚Äúd·∫•u v√¢n tay s·ªë h·ªçc‚Äù c·ªßa 1 vƒÉn b·∫£n ‚Äî ch·ª©a ng·ªØ c·∫£nh y khoa ƒë√£ ƒë∆∞·ª£c BioBERT ‚Äúƒë·ªçc hi·ªÉu‚Äù.\n",
    "\n",
    "ƒê√¢y ch√≠nh l√† ƒë·∫ßu v√†o cho ‚Äúph·∫ßn ra quy·∫øt ƒë·ªãnh‚Äù ·ªü B∆∞·ªõc 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b908",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 3 ‚Äî Classifier head (Ph·∫ßn ra quy·∫øt ƒë·ªãnh)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "G·∫Øn 2 ‚Äúƒë·∫ßu ra quy·∫øt ƒë·ªãnh‚Äù (Dense) l√™n vector CLS t·ª´ BioBERT:\n",
    "\n",
    "ƒê·∫ßu ICD: d·ª± ƒëo√°n ƒëa nh√£n c√°c ICD-block.\n",
    "\n",
    "ƒê·∫ßu Lab: d·ª± ƒëo√°n ƒëa nh√£n c√°c x√©t nghi·ªám s·ªõm.\n",
    "\n",
    "M·ªói ƒë·∫ßu tr·∫£ v·ªÅ logits (s·ªë th·ª±c √¢m/d∆∞∆°ng). Khi qua sigmoid s·∫Ω th√†nh x√°c su·∫•t 0‚Äì1 cho t·ª´ng nh√£n.\n",
    "\n",
    "Ki·ªÉm tra k√≠ch th∆∞·ªõc ƒë·∫ßu ra v√† xem th·ª≠ top-k d·ª± ƒëo√°n tr√™n m·ªôt batch (ch∆∞a hu·∫•n luy·ªán, ch·ªâ ƒë·ªÉ minh h·ªça d√≤ng ch·∫£y d·ªØ li·ªáu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"biobert_multitask_heads\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                        Param #     Connected to                        \n",
      "========================================================================================================================\n",
      " input_ids (InputLayer)             [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " attention_mask (InputLayer)        [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " tf_bert_model_1 (TFBertModel)      TFBaseModelOutputWithPoolingAndCr   108310272   ['input_ids[0][0]',                 \n",
      "                                    ossAttentions(last_hidden_state=(                'attention_mask[0][0]']            \n",
      "                                    None, None, 768),                                                                   \n",
      "                                     pooler_output=(None, 768),                                                         \n",
      "                                     past_key_values=None, hidden_sta                                                   \n",
      "                                    tes=None, attentions=None, cross_                                                   \n",
      "                                    attentions=None)                                                                    \n",
      "                                                                                                                        \n",
      " tf.__operators__.getitem_1 (Slici  (None, 768)                         0           ['tf_bert_model_1[0][0]']           \n",
      " ngOpLambda)                                                                                                            \n",
      "                                                                                                                        \n",
      " cls_dropout (Dropout)              (None, 768)                         0           ['tf.__operators__.getitem_1[0][0]']\n",
      "                                                                                                                        \n",
      " icd_logits (Dense)                 (None, 200)                         153800      ['cls_dropout[0][0]']               \n",
      "                                                                                                                        \n",
      " lab_logits (Dense)                 (None, 50)                          38450       ['cls_dropout[0][0]']               \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 108502522 (413.90 MB)\n",
      "Trainable params: 108502522 (413.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "________________________________________________________________________________________________________________________\n",
      "K√≠ch th∆∞·ªõc icd_logits: (8, 200)\n",
      "K√≠ch th∆∞·ªõc lab_logits: (8, 50)\n",
      "Top-5 ICD indices: [53, 72, 14, 9, 79]\n",
      "Top-5 ICD probs  : [0.7955999970436096, 0.7822999954223633, 0.7800999879837036, 0.7788000106811523, 0.7644000053405762]\n",
      "Top-5 Lab indices: [11, 44, 37, 28, 7]\n",
      "Top-5 Lab probs  : [0.8439000248908997, 0.8066999912261963, 0.7979999780654907, 0.6883999705314636, 0.6830000281333923]\n",
      "Top-5 ICD codes: ['790', '278', 'I48', 'Y92', '412']\n",
      "Top-5 Lab itemids: [50902, 51274, 51116, 51498, 51279]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# B∆Ø·ªöC 3 ‚Äî CLASSIFIER HEAD (C·∫¨P NH·∫¨T: fusion CLS + tab_feats)\n",
    "# =========================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 3.1 L·∫•y ƒë·∫ßu ra CLS t·ª´ encoder (B∆∞·ªõc 2)\n",
    "cls_output = encoder.outputs[0]  # [batch, hidden_size], vd 768\n",
    "\n",
    "# 3.2 Khai b√°o th√™m input tabular ƒë·ªÉ kh·ªõp v·ªõi tf.data (B∆∞·ªõc 1 ƒë√£ t·∫°o 'tab_feats')\n",
    "tab_input = tf.keras.Input(shape=(TAB_DIM,), dtype=tf.float32, name=\"tab_feats\")  # [batch, TAB_DIM]\n",
    "\n",
    "# 3.3 Fusion: gh√©p CLS + tab_feats (c√≥ th·ªÉ th√™m MLP nh·ªè ƒë·ªÉ ·ªïn ƒë·ªãnh)\n",
    "fused = tf.keras.layers.Concatenate(name=\"fuse_cls_tab\")([cls_output, tab_input])  # [batch, 768+TAB_DIM]\n",
    "fused = tf.keras.layers.Dropout(0.1, name=\"fuse_dropout\")(fused)\n",
    "fused = tf.keras.layers.Dense(512, activation=\"relu\", name=\"fuse_dense\")(fused)\n",
    "\n",
    "# 3.4 Hai nh√°nh logits cho t√°c v·ª• ƒëa nh√£n (from_logits=True ·ªü compile)\n",
    "icd_logits = tf.keras.layers.Dense(n_icd, name=\"icd_logits\")(fused)  # [batch, n_icd]\n",
    "lab_logits = tf.keras.layers.Dense(n_lab, name=\"lab_logits\")(fused)  # [batch, n_lab]\n",
    "\n",
    "# 3.5 M√¥ h√¨nh ƒëa nhi·ªám ho√†n ch·ªânh (inputs = text + tabular)\n",
    "multitask_model = tf.keras.Model(\n",
    "    inputs={\"input_ids\": encoder.inputs[\"input_ids\"],\n",
    "            \"attention_mask\": encoder.inputs[\"attention_mask\"],\n",
    "            \"tab_feats\": tab_input},\n",
    "    outputs={\"icd\": icd_logits, \"lab\": lab_logits},\n",
    "    name=\"biobert_multitask_fusion\"\n",
    ")\n",
    "\n",
    "# 3.6 Ki·ªÉm tra ki·∫øn tr√∫c\n",
    "multitask_model.summary(line_length=120)\n",
    "\n",
    "# 3.7 Ch·∫°y th·ª≠ m·ªôt batch (X_batch ƒë√£ c√≥ 'input_ids', 'attention_mask', 'tab_feats')\n",
    "for X_batch, Y_batch in train_ds.take(1):\n",
    "    logits = multitask_model.predict(X_batch, verbose=0)\n",
    "    icd_log = logits[\"icd\"]\n",
    "    lab_log = logits[\"lab\"]\n",
    "    print(\"K√≠ch th∆∞·ªõc icd_logits:\", icd_log.shape)\n",
    "    print(\"K√≠ch th∆∞·ªõc lab_logits:\", lab_log.shape)\n",
    "\n",
    "    # ƒê·ªïi sang x√°c su·∫•t ƒë·ªÉ quan s√°t\n",
    "    probs_icd = tf.sigmoid(icd_log).numpy()\n",
    "    probs_lab = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "    k = 5\n",
    "    top_icd_idx = probs_icd[0].argsort()[-k:][::-1]\n",
    "    top_lab_idx = probs_lab[0].argsort()[-k:][::-1]\n",
    "    print(\"Top-5 ICD indices:\", top_icd_idx.tolist())\n",
    "    print(\"Top-5 ICD probs  :\", np.round(probs_icd[0][top_icd_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 Lab indices:\", top_lab_idx.tolist())\n",
    "    print(\"Top-5 Lab probs  :\", np.round(probs_lab[0][top_lab_idx], 4).tolist())\n",
    "\n",
    "    print(\"Top-5 ICD codes:\", [icd_vocab[i] for i in top_icd_idx])\n",
    "    print(\"Top-5 Lab itemids:\", [lab_vocab_items[j] for j in top_lab_idx])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98f94a",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "B·∫°n s·∫Ω th·∫•y hai ma tr·∫≠n: icd_logits c√≥ k√≠ch th∆∞·ªõc [BATCH_SIZE, n_icd], lab_logits c√≥ k√≠ch th∆∞·ªõc [BATCH_SIZE, n_lab].\n",
    "\n",
    "Sau khi √°p sigmoid, b·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c hai ma tr·∫≠n x√°c su·∫•t probs_icd, probs_lab c√πng k√≠ch th∆∞·ªõc, m·ªói c·ªôt l√† ‚Äúm·ªôt √¥‚Äù x√°c su·∫•t m√† b·∫°n m√¥ t·∫£ (v√≠ d·ª• 0.85 cho I21, 0.92 cho Troponin‚Ä¶).\n",
    "\n",
    "ƒê√¢y ch√≠nh l√† ‚Äúph·∫ßn ra quy·∫øt ƒë·ªãnh‚Äù tr∆∞·ªõc khi so s√°nh v·ªõi nh√£n th·∫≠t ·ªü B∆∞·ªõc 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e088b",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 4 ‚Äî So s√°nh v·ªõi nh√£n th·∫≠t (Ground truth)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "L·∫•y d·ª± ƒëo√°n t·ª´ B∆∞·ªõc 3 (logits ‚Üí x√°c su·∫•t).\n",
    "\n",
    "So s√°nh v·ªõi nh√£n th·∫≠t y_icd, y_lab cho m·ªôt m·∫´u trong validation:\n",
    "\n",
    "Xem nh·ªØng nh√£n n√†o m√¥ h√¨nh d·ª± ƒëo√°n (theo ng∆∞·ª°ng 0.5) so v·ªõi nh√£n th·∫≠t.\n",
    "\n",
    "Xem Top-k (v√≠ d·ª• k=5) nh√£n c√≥ x√°c su·∫•t cao nh·∫•t v√† ƒë·ªëi chi·∫øu nh√£n th·∫≠t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfca6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== So s√°nh theo NG∆Ø·ª†NG 0.5 (m·∫´u idx=0) ===\n",
      "ICD: th·∫≠t=0 | d·ª± ƒëo√°n=90 | ƒë√∫ng=0\n",
      "LAB: th·∫≠t=0 | d·ª± ƒëo√°n=27 | ƒë√∫ng=0\n",
      "\n",
      "=== So s√°nh theo TOP-K ===\n",
      "Top-5 ICD (index ‚Üí m√£ ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\n",
      "  idx=  53 | ICD=790   | p=0.796 | nh√£n_th·∫≠t=0\n",
      "  idx=  72 | ICD=278   | p=0.782 | nh√£n_th·∫≠t=0\n",
      "  idx=  14 | ICD=I48   | p=0.780 | nh√£n_th·∫≠t=0\n",
      "  idx=   9 | ICD=Y92   | p=0.779 | nh√£n_th·∫≠t=0\n",
      "  idx=  79 | ICD=412   | p=0.764 | nh√£n_th·∫≠t=0\n",
      "\n",
      "Top-5 Lab (index ‚Üí itemid ‚Üí t√™n ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\n",
      "  idx=  11 | itemid=50902 | Chloride | p=0.844 | nh√£n_th·∫≠t=0\n",
      "  idx=  44 | itemid=51274 | PT | p=0.807 | nh√£n_th·∫≠t=0\n",
      "  idx=  37 | itemid=51116 | Lymphocytes | p=0.798 | nh√£n_th·∫≠t=0\n",
      "  idx=  28 | itemid=51498 | Specific Gravity | p=0.688 | nh√£n_th·∫≠t=0\n",
      "  idx=   7 | itemid=51279 | Red Blood Cells | p=0.683 | nh√£n_th·∫≠t=0\n",
      "\n",
      "ICD ‚Äî nh√£n_th·∫≠t (m√£): [] ...\n",
      "ICD ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (m√£): ['401', 'Z87', 'E87', 'Y92', 'V15', 'K21', 'I48', 'V45', 'N17', 'G47', 'Z86', '530', '427', 'N18', 'Z20', 'V10', 'Z85', 'F10', '585', 'E66'] ...\n",
      "ICD ‚Äî giao_nhau_ƒë√∫ng (m√£): [] ...\n",
      "\n",
      "LAB ‚Äî nh√£n_th·∫≠t (itemid, t√™n): [] ...\n",
      "LAB ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (itemid, t√™n): [(51250, 'MCV'), (51265, 'Platelet Count'), (51279, 'Red Blood Cells'), (51301, 'White Blood Cells'), (50882, 'Bicarbonate'), (50902, 'Chloride'), (50911, 'Creatine Kinase, MB Isoenzyme'), (50912, 'Creatinine'), (50960, 'Magnesium'), (51006, 'Urea Nitrogen')] ...\n",
      "LAB ‚Äî giao_nhau_ƒë√∫ng (itemid, t√™n): [] ...\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# B∆Ø·ªöC 4 ‚Äî SO S√ÅNH V·ªöI NH√ÉN TH·∫¨T (C·∫¨P NH·∫¨T)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) L·∫•y m·ªôt batch t·ª´ validation ƒë·ªÉ so s√°nh\n",
    "#    X_batch gi·ªù g·ªìm: input_ids, attention_mask, tab_feats\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "\n",
    "# 2) Ch·∫°y m√¥ h√¨nh ƒëa nhi·ªám (fusion t·ª´ B∆∞·ªõc 3) ƒë·ªÉ l·∫•y logits\n",
    "logits = multitask_model.predict(X_batch, verbose=0)\n",
    "icd_log = logits[\"icd\"]   # [BATCH_SIZE, n_icd]\n",
    "lab_log = logits[\"lab\"]   # [BATCH_SIZE, n_lab]\n",
    "\n",
    "# 3) ƒê·ªïi logits -> x√°c su·∫•t (0..1) b·∫±ng sigmoid ƒë·ªÉ d·ªÖ \"ƒë·ªçc\"\n",
    "probs_icd = tf.sigmoid(icd_log).numpy()\n",
    "probs_lab = tf.sigmoid(lab_log).numpy()\n",
    "\n",
    "# 4) Ch·ªçn m·ªôt m·∫´u trong batch ƒë·ªÉ nh√¨n chi ti·∫øt (v√≠ d·ª• m·∫´u ƒë·∫ßu ti√™n)\n",
    "idx = 0\n",
    "y_icd_true = Y_batch[\"icd\"][idx].numpy()   # vector multi-hot th·∫≠t [n_icd]\n",
    "y_lab_true = Y_batch[\"lab\"][idx].numpy()   # vector multi-hot th·∫≠t [n_lab]\n",
    "\n",
    "# 5) Chi·∫øn l∆∞·ª£c A: So s√°nh theo NG∆Ø·ª†NG 0.5 (m·∫∑c ƒë·ªãnh)\n",
    "THR_ICD = 0.5\n",
    "THR_LAB = 0.5\n",
    "\n",
    "y_icd_pred_bin = (probs_icd[idx] >= THR_ICD).astype(int)\n",
    "y_lab_pred_bin = (probs_lab[idx] >= THR_LAB).astype(int)\n",
    "\n",
    "true_icd_idx = np.where(y_icd_true == 1)[0]\n",
    "pred_icd_idx = np.where(y_icd_pred_bin == 1)[0]\n",
    "hit_icd_idx  = np.intersect1d(true_icd_idx, pred_icd_idx)\n",
    "\n",
    "true_lab_idx = np.where(y_lab_true == 1)[0]\n",
    "pred_lab_idx = np.where(y_lab_pred_bin == 1)[0]\n",
    "hit_lab_idx  = np.intersect1d(true_lab_idx, pred_lab_idx)\n",
    "\n",
    "print(\"=== So s√°nh theo NG∆Ø·ª†NG 0.5 (m·∫´u idx=0) ===\")\n",
    "print(f\"ICD: th·∫≠t={len(true_icd_idx)} | d·ª± ƒëo√°n={len(pred_icd_idx)} | ƒë√∫ng={len(hit_icd_idx)}\")\n",
    "print(f\"LAB: th·∫≠t={len(true_lab_idx)} | d·ª± ƒëo√°n={len(pred_lab_idx)} | ƒë√∫ng={len(hit_lab_idx)}\")\n",
    "\n",
    "# 6) Chi·∫øn l∆∞·ª£c B: So s√°nh theo TOP-K (v√≠ d·ª• k=5)\n",
    "K_ICD = 5\n",
    "K_LAB = 5\n",
    "\n",
    "top_icd_idx = probs_icd[idx].argsort()[-K_ICD:][::-1]\n",
    "top_lab_idx = probs_lab[idx].argsort()[-K_LAB:][::-1]\n",
    "\n",
    "print(\"\\n=== So s√°nh theo TOP-K ===\")\n",
    "print(\"Top-5 ICD (index ‚Üí m√£ ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\")\n",
    "for j in top_icd_idx:\n",
    "    code = icd_vocab[j]\n",
    "    p = float(probs_icd[idx][j])\n",
    "    is_true = int(y_icd_true[j])\n",
    "    print(f\"  idx={j:4d} | ICD={code:5s} | p={p:0.3f} | nh√£n_th·∫≠t={is_true}\")\n",
    "\n",
    "print(\"\\nTop-5 Lab (index ‚Üí itemid ‚Üí t√™n ‚Üí x√°c su·∫•t ‚Üí c√≥ ph·∫£i nh√£n th·∫≠t?):\")\n",
    "for j in top_lab_idx:\n",
    "    itemid = int(lab_vocab_items[j])\n",
    "    name = itemid_to_label.get(itemid, str(itemid))\n",
    "    p = float(probs_lab[idx][j])\n",
    "    is_true = int(y_lab_true[j])\n",
    "    print(f\"  idx={j:4d} | itemid={itemid} | {name} | p={p:0.3f} | nh√£n_th·∫≠t={is_true}\")\n",
    "\n",
    "# 7) (T√πy ch·ªçn) In danh s√°ch nh√£n ƒë√∫ng theo ng∆∞·ª°ng ƒë·ªÉ h√¨nh dung\n",
    "def map_icd_indices(indices):\n",
    "    return [icd_vocab[i] for i in indices]\n",
    "\n",
    "def map_lab_indices(indices):\n",
    "    return [(int(lab_vocab_items[i]),\n",
    "             itemid_to_label.get(int(lab_vocab_items[i]), str(lab_vocab_items[i])))\n",
    "            for i in indices]\n",
    "\n",
    "print(\"\\nICD ‚Äî nh√£n_th·∫≠t (m√£):\", map_icd_indices(true_icd_idx)[:20], \"...\")\n",
    "print(\"ICD ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (m√£):\", map_icd_indices(pred_icd_idx)[:20], \"...\")\n",
    "print(\"ICD ‚Äî giao_nhau_ƒë√∫ng (m√£):\", map_icd_indices(hit_icd_idx)[:20], \"...\")\n",
    "\n",
    "print(\"\\nLAB ‚Äî nh√£n_th·∫≠t (itemid, t√™n):\", map_lab_indices(true_lab_idx)[:10], \"...\")\n",
    "print(\"LAB ‚Äî d·ª±_ƒëo√°n_theo_ng∆∞·ª°ng (itemid, t√™n):\", map_lab_indices(pred_lab_idx)[:10], \"...\")\n",
    "print(\"LAB ‚Äî giao_nhau_ƒë√∫ng (itemid, t√™n):\", map_lab_indices(hit_lab_idx)[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28058",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "B·∫°n th·∫•y r√µ c√°ch ƒë·ªëi chi·∫øu:\n",
    "\n",
    "‚ÄúD·ª± ƒëo√°n theo ng∆∞·ª°ng‚Äù (‚â• 0.5) vs. nh√£n th·∫≠t ‚Üí ƒë·∫øm ƒë√∫ng/sai.\n",
    "\n",
    "‚ÄúD·ª± ƒëo√°n Top-k‚Äù ‚Üí xem 5 nh√£n cao nh·∫•t v√† ƒë√°nh d·∫•u nh√£n n√†o tr√πng nh√£n th·∫≠t.\n",
    "\n",
    "ƒê√¢y l√† b∆∞·ªõc kh√¥ng t√≠nh loss (ch∆∞a hu·∫•n luy·ªán), ch·ªâ so s√°nh ƒë·ªÉ hi·ªÉu d√≤ng ch·∫£y d·ª± ƒëo√°n ‚Üî nh√£n.\n",
    "(Loss s·∫Ω thi·∫øt l·∫≠p ·ªü B∆∞·ªõc 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c195b65",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 5 ‚Äî T√≠nh sai s·ªë (Loss)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "Khai b√°o h√†m m·∫•t m√°t cho 2 nh√°nh ƒë·∫ßu ra (ICD v√† Lab) ƒë√∫ng v·ªõi thi·∫øt k·∫ø ƒëa nh√£n.\n",
    "\n",
    "Bi√™n d·ªãch (compile) m√¥ h√¨nh v·ªõi optimizer/metrics, ch∆∞a hu·∫•n luy·ªán.\n",
    "\n",
    "T√≠nh th·ª≠ loss & AUPRC tr√™n 1 batch ƒë·ªÉ th·∫•y m√¥ h√¨nh ƒëang ·ªü m·ª©c ‚Äúng·∫´u nhi√™n‚Äù tr∆∞·ªõc khi h·ªçc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ba5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone BioBERT trainable: False\n",
      "K·∫øt qu·∫£ tr√™n 1 batch (tr∆∞·ªõc khi train):\n",
      "  loss: 1.4856\n",
      "  icd_logits_loss: 0.7190\n",
      "  lab_logits_loss: 0.7666\n",
      "  icd_logits_AUPRC: 0.0261\n",
      "  lab_logits_AUPRC: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# B∆Ø·ªöC 5 ‚Äî T√çNH SAI S·ªê (LOSS) & COMPILE (C·∫¨P NH·∫¨T)\n",
    "# ======================================\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) Loss ƒëa nh√£n cho 2 nh√°nh (logits)\n",
    "losses = {\n",
    "    \"icd\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    \"lab\": tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "}\n",
    "\n",
    "# 2) Tr·ªçng s·ªë loss (tu·ª≥ ch·ªânh n·∫øu mu·ªën ∆∞u ti√™n 1 nh√°nh)\n",
    "loss_weights = {\"icd\": 1.0, \"lab\": 1.0}\n",
    "\n",
    "# 3) Metrics: AUPRC (ph√π h·ª£p m·∫•t c√¢n b·∫±ng nh√£n)\n",
    "metrics = {\n",
    "    \"icd\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "    \"lab\": [tf.keras.metrics.AUC(curve=\"PR\", multi_label=True, name=\"AUPRC\")],\n",
    "}\n",
    "\n",
    "# 4) Optimizer cho fine-tune\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "# üîí 4.1 ƒê√≥ng bƒÉng backbone BioBERT (khi fusion c√≥ th√™m tab_feats)\n",
    "#     - Th·ª≠ ƒë√≥ng bƒÉng tr·ª±c ti·∫øp bi·∫øn `bert` n·∫øu c√≥\n",
    "try:\n",
    "    bert.trainable = False\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "#     - Ph√≤ng tr∆∞·ªùng h·ª£p backbone ch·ªâ xu·∫•t hi·ªán l·ªìng trong model:\n",
    "for sub in multitask_model.submodules:\n",
    "    if \"bert\" in sub.name.lower():\n",
    "        sub.trainable = False\n",
    "\n",
    "# (Tu·ª≥ ch·ªçn) in ra x√°c nh·∫≠n tr·∫°ng th√°i trainable c·ªßa c√°c module ch·ª©a 'bert'\n",
    "print(\"C√°c module ch·ª©a 'bert' sau khi ƒë√≥ng bƒÉng:\")\n",
    "for sub in multitask_model.submodules:\n",
    "    if \"bert\" in sub.name.lower():\n",
    "        print(f\"  - {sub.name}: trainable={sub.trainable}\")\n",
    "\n",
    "# 5) Compile m√¥ h√¨nh (inputs g·ªìm: input_ids, attention_mask, tab_feats)\n",
    "multitask_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# 6) ƒê√°nh gi√° nhanh tr√™n 1 batch validation (ƒë·∫ßu v√†o ƒë√£ c√≥ c·∫£ tab_feats)\n",
    "X_batch, Y_batch = next(iter(val_ds))\n",
    "results = multitask_model.evaluate(\n",
    "    X_batch, Y_batch,\n",
    "    verbose=0,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "print(\"K·∫øt qu·∫£ tr√™n 1 batch (tr∆∞·ªõc khi train):\")\n",
    "for k, v in results.items():\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f2e1c",
   "metadata": {},
   "source": [
    "B∆Ø·ªöC 6 ‚Äî C·∫≠p nh·∫≠t (Training)\n",
    "\n",
    "M·ª•c ti√™u\n",
    "\n",
    "Hu·∫•n luy·ªán to√†n b·ªô m√¥ h√¨nh: BioBERT + 2 ƒë·∫ßu ra (ICD, Lab) b·∫±ng Adam.\n",
    "\n",
    "D√πng EarlyStopping ƒë·ªÉ d·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán.\n",
    "\n",
    "L∆∞u checkpoint t·ªët nh·∫•t theo ch·ªâ s·ªë th·∫©m ƒë·ªãnh (validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86192080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "     90/Unknown - 120s 1s/step - loss: 1.3712 - icd_logits_loss: 0.6643 - lab_logits_loss: 0.7069 - icd_logits_AUPRC: 0.0459 - lab_logits_AUPRC: 0.0076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 15:42:01.032424: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-09-08 15:42:18.976215: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_icd_logits_AUPRC improved from -inf to 0.03958, saving model to checkpoints_biobert_multitask/best\n",
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/best/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 171s 2s/step - loss: 1.3712 - icd_logits_loss: 0.6643 - lab_logits_loss: 0.7069 - icd_logits_AUPRC: 0.0459 - lab_logits_AUPRC: 0.0076 - val_loss: 1.2425 - val_icd_logits_loss: 0.6034 - val_lab_logits_loss: 0.6391 - val_icd_logits_AUPRC: 0.0396 - val_lab_logits_AUPRC: 0.0000e+00\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - ETA: 0s - loss: 1.1564 - icd_logits_loss: 0.5652 - lab_logits_loss: 0.5911 - icd_logits_AUPRC: 0.0465 - lab_logits_AUPRC: 0.0053\n",
      "Epoch 2: val_icd_logits_AUPRC did not improve from 0.03958\n",
      "90/90 [==============================] - 130s 1s/step - loss: 1.1564 - icd_logits_loss: 0.5652 - lab_logits_loss: 0.5911 - icd_logits_AUPRC: 0.0465 - lab_logits_AUPRC: 0.0053 - val_loss: 1.0482 - val_icd_logits_loss: 0.5147 - val_lab_logits_loss: 0.5335 - val_icd_logits_AUPRC: 0.0389 - val_lab_logits_AUPRC: 0.0000e+00\n",
      "14/14 [==============================] - 18s 1s/step - loss: 1.2425 - icd_logits_loss: 0.6034 - lab_logits_loss: 0.6391 - icd_logits_AUPRC: 0.0396 - lab_logits_AUPRC: 0.0000e+00\n",
      "\n",
      "ƒê√°nh gi√° cu·ªëi c√πng tr√™n validation:\n",
      "  loss: 1.2425\n",
      "  icd_logits_loss: 0.6034\n",
      "  lab_logits_loss: 0.6391\n",
      "  icd_logits_AUPRC: 0.0396\n",
      "  lab_logits_AUPRC: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 15:45:19.949599: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints_biobert_multitask/final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒê√£ l∆∞u m√¥ h√¨nh hi·ªán t·∫°i t·∫°i: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/notebooks/checkpoints_biobert_multitask/final\n",
      "Checkpoint t·ªët nh·∫•t (theo val_icd_logits_AUPRC) t·∫°i: /Users/lehoangkhang/TaÃÄi lieÃ£ÃÇu/revita-sympdiag/notebooks/checkpoints_biobert_multitask/best\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# B∆Ø·ªöC 6 ‚Äî HU·∫§N LUY·ªÜN (TRAINING) [C·∫¨P NH·∫¨T]\n",
    "# ==================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "EPOCHS = 3\n",
    "OUT_DIR = Path(\"checkpoints_biobert_multitask\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# EarlyStopping theo AUPRC c·ªßa nh√°nh ICD (ƒë√∫ng t√™n metric sau fusion: icd_AUPRC)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    patience=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ModelCheckpoint l∆∞u SavedModel t·ªët nh·∫•t theo val_icd_AUPRC\n",
    "ckpt_path = OUT_DIR / \"best\"\n",
    "ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(ckpt_path),\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,  # l∆∞u SavedModel\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# (khuy·∫øn ngh·ªã) gi·∫£m LR khi metric kh√¥ng c·∫£i thi·ªán\n",
    "rlrop = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_icd_AUPRC\",\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# (tu·ª≥ ch·ªçn) log l·ªãch s·ª≠ hu·∫•n luy·ªán\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(OUT_DIR / \"train_log.csv\", append=False)\n",
    "\n",
    "# Train\n",
    "history = multitask_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop, ckpt_cb, rlrop, csv_logger],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate cu·ªëi\n",
    "final_eval = multitask_model.evaluate(val_ds, return_dict=True, verbose=1)\n",
    "print(\"\\nƒê√°nh gi√° cu·ªëi c√πng tr√™n validation:\")\n",
    "for k, v in final_eval.items():\n",
    "    try:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# L∆∞u b·∫£n final\n",
    "final_path = OUT_DIR / \"final\"\n",
    "multitask_model.save(final_path)\n",
    "print(f\"\\nƒê√£ l∆∞u m√¥ h√¨nh hi·ªán t·∫°i t·∫°i: {final_path.resolve()}\")\n",
    "print(f\"Checkpoint t·ªët nh·∫•t (theo val_icd_AUPRC) t·∫°i: {ckpt_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f250e",
   "metadata": {},
   "source": [
    "√ù nghƒ©a c·ªßa k·∫øt qu·∫£\n",
    "\n",
    "loss, icd_loss, lab_loss tr√™n validation s·∫Ω gi·∫£m d·∫ßn qua epoch; AUPRC s·∫Ω tƒÉng d·∫ßn n·∫øu d·ªØ li·ªáu/ti·ªÅn x·ª≠ l√Ω ·ªïn.\n",
    "\n",
    "Sau khi hu·∫•n luy·ªán, m√¥ h√¨nh b·∫Øt ƒë·∫ßu d·ª± ƒëo√°n h·ª£p l√Ω h∆°n (√≠t ‚Äútr√†n nh√£n‚Äù ‚â• 0.5, top-k s√°t th·ª±c t·∫ø h∆°n)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
