"""
Script ƒë·ªÉ t·∫°o file train_unified.parquet t·ª´ c√°c file ngu·ªìn.
K·∫øt h·ª£p discharge notes, demographics, v√† ICD codes ƒë√£ unified.
"""

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import re
from pathlib import Path

# ƒê∆∞·ªùng d·∫´n
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
OUTPUT_DIR = DATA_DIR / "proc"

# ƒê∆∞·ªùng d·∫´n c√°c file ngu·ªìn
DISCHARGE_FILE = DATA_DIR / "mimic-iv-note" / "2.2" / "note" / "discharge.csv.gz"
PATIENTS_FILE = DATA_DIR / "mimiciv" / "3.1" / "hosp" / "patients.csv.gz"
ADMISSIONS_FILE = DATA_DIR / "mimiciv" / "3.1" / "hosp" / "admissions.csv.gz"
DIAGNOSES_FILE = DATA_DIR / "proc" / "diagnoses_icd_unified.csv.gz"

# File output
OUTPUT_FILE = OUTPUT_DIR / "train_unified.parquet"

# C·∫•u h√¨nh
MAX_CHARS = 8000  # Gi·ªõi h·∫°n ƒë·ªô d√†i text
TEXT_FROM_SERVICE_ONLY = True  # Ch·ªâ l·∫•y ph·∫ßn t·ª´ "Service:" tr·ªü ƒëi

def keep_from_service(text: str) -> str:
    """L·∫•y ph·∫ßn text t·ª´ 'Service:' tr·ªü ƒëi"""
    if not isinstance(text, str):
        return ""
    match = re.search(r'\bService\s*:', text, flags=re.I)
    if match is not None:
        return text[match.start():]
    return text

def create_train_unified():
    """T·∫°o file train_unified.parquet"""
    
    print("=" * 60)
    print("T·∫†O FILE train_unified.parquet")
    print("=" * 60)
    
    # Ki·ªÉm tra c√°c file ngu·ªìn
    required_files = {
        "discharge": DISCHARGE_FILE,
        "patients": PATIENTS_FILE,
        "admissions": ADMISSIONS_FILE,
        "diagnoses": DIAGNOSES_FILE
    }
    
    for name, path in required_files.items():
        if not path.exists():
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {path}")
            return
        print(f"‚úÖ {name}: {path.name}")
    
    # B∆∞·ªõc 1: ƒê·ªçc v√† chu·∫©n b·ªã demographics
    print("\nüìñ B∆∞·ªõc 1: ƒê·ªçc demographics...")
    print("   ƒêang ƒë·ªçc patients...")
    patients = pd.read_csv(PATIENTS_FILE, compression='gzip', 
                          usecols=['subject_id', 'gender', 'anchor_age', 'anchor_year'])
    
    print("   ƒêang ƒë·ªçc admissions...")
    admissions = pd.read_csv(ADMISSIONS_FILE, compression='gzip',
                            usecols=['subject_id', 'hadm_id', 'admittime'],
                            parse_dates=['admittime'])
    
    # Merge v√† t√≠nh age_at_admit
    print("   T√≠nh to√°n age_at_admit...")
    adm_pat = admissions.merge(patients, on='subject_id', how='left')
    adm_pat['age_at_admit'] = (
        adm_pat['anchor_age'] + 
        (adm_pat['admittime'].dt.year - adm_pat['anchor_year'])
    ).clip(lower=0, upper=120)
    
    demographics = adm_pat[['subject_id', 'hadm_id', 'gender', 'age_at_admit']].copy()
    print(f"   ƒê√£ t·∫°o demographics: {len(demographics):,} d√≤ng")
    
    # B∆∞·ªõc 2: ƒê·ªçc v√† chu·∫©n b·ªã ICD codes
    print("\nüìñ B∆∞·ªõc 2: ƒê·ªçc ICD codes...")
    print("   ƒêang ƒë·ªçc diagnoses_icd_unified...")
    
    # T·∫°o mapping hadm_id -> list icd_full
    hadm2codes = {}
    chunk_size = 200_000
    
    for chunk in pd.read_csv(DIAGNOSES_FILE, compression='gzip',
                            usecols=['hadm_id', 'icd_code', 'icd_version'],
                            chunksize=chunk_size):
        # Lo·∫°i b·ªè duplicate
        chunk = chunk.drop_duplicates(['hadm_id', 'icd_code', 'icd_version'])
        
        # T·∫°o icd_full: version-code
        chunk['icd_full'] = (
            chunk['icd_version'].astype(str) + "-" + 
            chunk['icd_code'].astype(str)
        )
        
        # G·ªôp theo hadm_id
        for hadm_id, group in chunk.groupby('hadm_id'):
            hadm_id_int = int(hadm_id)
            if hadm_id_int not in hadm2codes:
                hadm2codes[hadm_id_int] = []
            hadm2codes[hadm_id_int].extend(group['icd_full'].tolist())
    
    # Lo·∫°i b·ªè duplicate trong m·ªói hadm_id v√† s·∫Øp x·∫øp
    for hadm_id in hadm2codes:
        hadm2codes[hadm_id] = sorted(set(hadm2codes[hadm_id]))
    
    print(f"   ƒê√£ t·∫°o mapping cho {len(hadm2codes):,} hadm_id")
    
    # B∆∞·ªõc 3: ƒê·ªçc discharge notes v√† merge
    print("\nüìñ B∆∞·ªõc 3: ƒê·ªçc discharge notes v√† merge...")
    
    # T√¨m c·ªôt text
    header = pd.read_csv(DISCHARGE_FILE, compression='gzip', nrows=0).columns
    text_col = None
    for col in header:
        if col.lower() in ['text', 'note_text']:
            text_col = col
            break
    
    if text_col is None:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt text trong {DISCHARGE_FILE}")
        return
    
    print(f"   S·ª≠ d·ª•ng c·ªôt: {text_col}")
    
    # X√≥a file output n·∫øu ƒë√£ t·ªìn t·∫°i
    if OUTPUT_FILE.exists():
        OUTPUT_FILE.unlink()
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # X·ª≠ l√Ω theo chunks
    writer = None
    batch = []
    batch_size = 10_000
    total_processed = 0
    total_written = 0
    
    print("   ƒêang x·ª≠ l√Ω discharge notes...")
    
    for chunk_num, chunk in enumerate(pd.read_csv(
        DISCHARGE_FILE, 
        compression='gzip',
        usecols=['subject_id', 'hadm_id', text_col],
        chunksize=100_000,
        low_memory=True
    )):
        total_processed += len(chunk)
        
        # Merge v·ªõi demographics
        chunk = chunk.merge(demographics, on=['subject_id', 'hadm_id'], how='left')
        
        # L·ªçc c√°c d√≤ng c√≥ text h·ª£p l·ªá
        txt = chunk[text_col].astype(str)
        mask = ~(txt.isna() | txt.str.lower().isin(['nan', 'none', '']))
        chunk = chunk[mask].copy()
        
        if chunk.empty:
            continue
        
        # T·∫°o text_clean
        if TEXT_FROM_SERVICE_ONLY:
            chunk['text_clean'] = txt[mask].map(keep_from_service)
        else:
            chunk['text_clean'] = txt[mask].str.slice(0, MAX_CHARS)
        
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        chunk['text_clean'] = chunk['text_clean'].str.slice(0, MAX_CHARS)
        
        # G·∫Øn ICD codes
        chunk['icd_codes'] = chunk['hadm_id'].map(
            lambda h: ';'.join(hadm2codes.get(int(h), []))
        )
        
        # Ch·ªâ gi·ªØ c√°c d√≤ng c√≥ c·∫£ text_clean v√† icd_codes
        chunk = chunk[
            (chunk['text_clean'].str.len() > 0) & 
            (chunk['icd_codes'].str.len() > 0)
        ].copy()
        
        if chunk.empty:
            continue
        
        # Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt
        output_cols = ['subject_id', 'hadm_id', 'gender', 'age_at_admit', 
                      'icd_codes', 'text_clean']
        chunk = chunk[output_cols].copy()
        
        batch.append(chunk)
        
        # Ghi batch khi ƒë·ªß l·ªõn
        if sum(len(b) for b in batch) >= batch_size:
            combined = pd.concat(batch, ignore_index=True)
            
            try:
                table = pa.Table.from_pandas(combined)
                if writer is None:
                    writer = pq.ParquetWriter(OUTPUT_FILE, table.schema, compression='snappy')
                writer.write_table(table)
                total_written += len(combined)
                batch.clear()
                
                if (chunk_num + 1) % 10 == 0:
                    print(f"   ƒê√£ x·ª≠ l√Ω {total_processed:,} d√≤ng, ƒë√£ ghi {total_written:,} d√≤ng...")
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói khi ghi parquet: {e}")
                # Fallback: ghi CSV
                csv_file = OUTPUT_DIR / "train_unified.csv"
                if not csv_file.exists():
                    combined.to_csv(csv_file, index=False, mode='w', header=True)
                else:
                    combined.to_csv(csv_file, index=False, mode='a', header=False)
                total_written += len(combined)
                batch.clear()
    
    # Ghi ph·∫ßn c√≤n l·∫°i
    if batch:
        combined = pd.concat(batch, ignore_index=True)
        try:
            table = pa.Table.from_pandas(combined)
            if writer is None:
                writer = pq.ParquetWriter(OUTPUT_FILE, table.schema, compression='snappy')
            writer.write_table(table)
            total_written += len(combined)
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi ghi parquet: {e}")
            csv_file = OUTPUT_DIR / "train_unified.csv"
            if not csv_file.exists():
                combined.to_csv(csv_file, index=False, mode='w', header=True)
            else:
                combined.to_csv(csv_file, index=False, mode='a', header=False)
            total_written += len(combined)
    
    # ƒê√≥ng writer
    if writer is not None:
        writer.close()
    
    print("\n" + "=" * 60)
    print("‚ú® HO√ÄN TH√ÄNH!")
    print("=" * 60)
    print(f"\nüìä Th·ªëng k√™:")
    print(f"   T·ªïng s·ªë d√≤ng ƒë√£ x·ª≠ l√Ω: {total_processed:,}")
    print(f"   S·ªë d√≤ng ƒë√£ ghi: {total_written:,}")
    print(f"   File output: {OUTPUT_FILE}")
    
    if OUTPUT_FILE.exists():
        file_size = OUTPUT_FILE.stat().st_size / 1024 / 1024
        print(f"   K√≠ch th∆∞·ªõc file: {file_size:.2f} MB")
        
        # Ki·ªÉm tra file
        print("\nüìã Ki·ªÉm tra file output:")
        df_sample = pd.read_parquet(OUTPUT_FILE).head(5)
        print(f"   S·ªë c·ªôt: {len(df_sample.columns)}")
        print(f"   C√°c c·ªôt: {list(df_sample.columns)}")
        print("\n   M·∫´u d·ªØ li·ªáu:")
        print(df_sample.to_string())

if __name__ == "__main__":
    create_train_unified()

